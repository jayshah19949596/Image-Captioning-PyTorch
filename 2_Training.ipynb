{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "\n",
    "#### CNN-RNN Architecture Overview\n",
    "1. The model used is a CNN-RNN architecthure\n",
    "2. The CNN works as the enocder and the RNN works as the Decoder\n",
    "3. The CNN-Enocder tries to encode the image into feature-encodings\n",
    "4. The RNN-Decoder tries to decode the feature-encodings from CNN-Enocder into captions\n",
    "5. The CNN feature-enocdings goes as the input at first time to the RNN-Decoder\n",
    "6. You can see this in the below image: \n",
    "![Image Captioning CNN-RNN model](images/encoder-decoder.png)\n",
    "\n",
    "#### CNN-Enocder ResNet50\n",
    "1. The CNN-Enocder used is ResNet50 model\n",
    "2. The ResNet50 used was a pre-trained model on ImageNet and the weights were loaded from this pre-trained weights\n",
    "3. CNN-Enocder layers were not trainable\n",
    "4. The ResNet50 model consists of 50 layers \n",
    "5. The building block of ResNet50 is residual block\n",
    "6. Below is the diagram of residual block:\n",
    "![Image Captioning RESIDUAL BLOCK model](images/residual-block.png)\n",
    "7. Instead of hoping each stack of layers directly fits a desired underlying mapping, we explicitly let these layers fit a residual mapping. \n",
    "8. The original mapping is recast into F(x)+x. \n",
    "9. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. \n",
    "10. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers.\n",
    "11. To know more about Residual Networks, refer the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf).\n",
    "\n",
    "\n",
    "\n",
    "#### RNN-Decoder\n",
    "1] The RNN-Decoder:  \n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t|\n",
    "|:---------------------:|:---------------------------------------------:|\n",
    "| Input         \t\t| Captions encoded into numbers     \t\t    | \n",
    "| Embedding Layer     \t| 128 Neurons                \t                |\n",
    "| LSTM Layer\t\t\t| 100 Neurons \t\t\t\t\t\t\t\t\t|\n",
    "| Output Layer      \t| 414113 Neurons\t                            |  \n",
    "2] EMBEDDING LAYER:\n",
    "  - When you're dealing with words in text, you end up with tens of thousands of classes to predict, one for each word. \n",
    "  - Trying to one-hot encode these words is massively inefficient, you'll have one element set to 1 and the other values set to 0.\n",
    "  - The matrix multiplication going into the first hidden layer will have almost all of the resulting values be zero. This a huge waste of computation.\n",
    "![Image Captioning Embedding_Explainataion_1](images/embedding_explainataion_1.png)\n",
    "  - To solve this problem and greatly increase the efficiency of our networks, we use what are called embeddings.\n",
    "  - Embeddings are just a fully connected layer like you've seen before.\n",
    "  - We call this layer the embedding layer and the weights are embedding weights.\n",
    "  - We skip the multiplication into the embedding layer by instead directly grabbing the hidden layer values from the weight matrix.\n",
    "  - We can do this because the multiplication of a one-hot encoded vector with a matrix returns the row of the matrix corresponding the index of the \"on\" input unit.\n",
    "  - Instead of doing the matrix multiplication, we use the weight matrix as a lookup table.\n",
    "![Image Captioning Embedding_Explainataion_2](images/embedding_lookup.png)\n",
    "\n",
    "3] LSTM Layer:\n",
    "  - LSTM layer comprises of LSTM Nodes\n",
    "  - Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies\n",
    "  - LSTMs are explicitly designed to avoid the long-term dependency problem.\n",
    "  - Below is LSTM Node Diagram and the computations that happend in LSTM node\n",
    "  ![Image Captioning LSTM Node](images/lstm_node.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Chosen and Why\n",
    "\n",
    "1. `transform_train` is a part of pre-processing step\n",
    "2. The original image is resized to 256\n",
    "3. The Resized image gets randomly cropped to 224x224\n",
    "4. The Cropped Image has probability of 50% of horizontally flip\n",
    "5. The Image is Converted to Torch Tensor. This is necessary so that image (H x W x C) is converted to (C x H x W) which can be acccepted by the Model\n",
    "6. Normalize the Torch Tensor image with mean and standard deviation.\n",
    "7. The transform_train in code : `transform_train = transforms.Compose([transforms.Resize(256), transforms.RandomCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), (0.229, 0.224, 0.225))])`\n",
    "8. I left the transform at it's provided value\n",
    "9. I think it's a good transform because every epoch the model will see a different image with the same captions\n",
    "10. This will help the model to generalize by looking different and more data\n",
    "11. The images in the dataset is of heights and widths and the model can not accept images of varying length and hence all are resized and cropped to 224x224 hieght and width\n",
    "12. If using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Trainable Parameters Chosen and Why\n",
    "\n",
    "1. Hyper-parameters selected:\n",
    "   - number of epochs = 3\n",
    "   - batch size = 10\n",
    "   - embedding nodes = 128\n",
    "   - lstm nodes = 100\n",
    "   - optimizer = adam\n",
    "2. Trainable weights were only of RNN-Decoder and The Last layer of CNN-Encoder\n",
    "3. `params = list(decoder.parameters()) + list(encoder.embed.parameters())`\n",
    "4. The ResNet50 is already a pretrained model on ImageNet and hence is a good candidate for extracting rich features... By not training the whole model of ResNet50 save training time and precious GPU memory... Because training ResNet50 which is a humongous network will be really costly\n",
    "5. The last layer of CNN Encoder is trainable so that the features selected are based on this specific dataset\n",
    "6. By this stratergy we pick the better features and save Gpu memory and Training time\n",
    "7. RNN-Decoder weights are trainable because it is necessary for them to learn the context of images and captions.\n",
    "\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "- Adam optimizer is selected\n",
    "- Adam is chosen because it is computational effficient\n",
    "- Faster convergence\n",
    "- Much better than other variants of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/share/applications/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /usr/share/applications/anaconda3/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to /home/jai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2643/414113 [00:00<00:31, 13249.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:29<00:00, 14013.36it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 3          # batch size\n",
    "vocab_threshold = 5       # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 128          # dimensionality of image and word embeddings\n",
    "hidden_size = 100        # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/138038], Loss: 8.5686, Perplexity: 5263.7438\n",
      "Epoch [1/3], Step [200/138038], Loss: 6.6477, Perplexity: 771.03467\n",
      "Epoch [1/3], Step [300/138038], Loss: 5.2150, Perplexity: 184.01379\n",
      "Epoch [1/3], Step [400/138038], Loss: 5.6655, Perplexity: 288.7222\n",
      "Epoch [1/3], Step [500/138038], Loss: 5.0063, Perplexity: 149.3552\n",
      "Epoch [1/3], Step [600/138038], Loss: 4.8283, Perplexity: 125.0007\n",
      "Epoch [1/3], Step [700/138038], Loss: 5.7179, Perplexity: 304.2780\n",
      "Epoch [1/3], Step [800/138038], Loss: 4.6182, Perplexity: 101.3149\n",
      "Epoch [1/3], Step [900/138038], Loss: 5.3391, Perplexity: 208.3182\n",
      "Epoch [1/3], Step [1000/138038], Loss: 4.6895, Perplexity: 108.7992\n",
      "Epoch [1/3], Step [1100/138038], Loss: 4.2879, Perplexity: 72.81400\n",
      "Epoch [1/3], Step [1200/138038], Loss: 5.0057, Perplexity: 149.2591\n",
      "Epoch [1/3], Step [1300/138038], Loss: 4.7591, Perplexity: 116.6428\n",
      "Epoch [1/3], Step [1400/138038], Loss: 4.3322, Perplexity: 76.11296\n",
      "Epoch [1/3], Step [1500/138038], Loss: 4.2645, Perplexity: 71.13197\n",
      "Epoch [1/3], Step [1600/138038], Loss: 3.7914, Perplexity: 44.31807\n",
      "Epoch [1/3], Step [1700/138038], Loss: 4.3335, Perplexity: 76.20942\n",
      "Epoch [1/3], Step [1800/138038], Loss: 5.0795, Perplexity: 160.6870\n",
      "Epoch [1/3], Step [1900/138038], Loss: 4.1035, Perplexity: 60.54943\n",
      "Epoch [1/3], Step [2000/138038], Loss: 4.1933, Perplexity: 66.24319\n",
      "Epoch [1/3], Step [2100/138038], Loss: 4.4778, Perplexity: 88.03647\n",
      "Epoch [1/3], Step [2200/138038], Loss: 4.7663, Perplexity: 117.4875\n",
      "Epoch [1/3], Step [2300/138038], Loss: 4.2634, Perplexity: 71.04957\n",
      "Epoch [1/3], Step [2400/138038], Loss: 4.6478, Perplexity: 104.3546\n",
      "Epoch [1/3], Step [2500/138038], Loss: 4.7273, Perplexity: 112.9873\n",
      "Epoch [1/3], Step [2600/138038], Loss: 3.6430, Perplexity: 38.20780\n",
      "Epoch [1/3], Step [2700/138038], Loss: 4.0304, Perplexity: 56.28518\n",
      "Epoch [1/3], Step [2800/138038], Loss: 4.2915, Perplexity: 73.07413\n",
      "Epoch [1/3], Step [2900/138038], Loss: 4.5201, Perplexity: 91.84167\n",
      "Epoch [1/3], Step [3000/138038], Loss: 4.0253, Perplexity: 55.99482\n",
      "Epoch [1/3], Step [3100/138038], Loss: 3.4271, Perplexity: 30.78717\n",
      "Epoch [1/3], Step [3200/138038], Loss: 4.6893, Perplexity: 108.7788\n",
      "Epoch [1/3], Step [3300/138038], Loss: 4.5316, Perplexity: 92.90620\n",
      "Epoch [1/3], Step [3400/138038], Loss: 3.8531, Perplexity: 47.13685\n",
      "Epoch [1/3], Step [3500/138038], Loss: 4.1031, Perplexity: 60.52857\n",
      "Epoch [1/3], Step [3600/138038], Loss: 4.7302, Perplexity: 113.3174\n",
      "Epoch [1/3], Step [3700/138038], Loss: 4.2262, Perplexity: 68.45579\n",
      "Epoch [1/3], Step [3800/138038], Loss: 3.9054, Perplexity: 49.67025\n",
      "Epoch [1/3], Step [3900/138038], Loss: 4.3571, Perplexity: 78.03091\n",
      "Epoch [1/3], Step [4000/138038], Loss: 4.8715, Perplexity: 130.5207\n",
      "Epoch [1/3], Step [4100/138038], Loss: 3.7904, Perplexity: 44.27214\n",
      "Epoch [1/3], Step [4200/138038], Loss: 3.9395, Perplexity: 51.39523\n",
      "Epoch [1/3], Step [4300/138038], Loss: 3.5283, Perplexity: 34.06624\n",
      "Epoch [1/3], Step [4400/138038], Loss: 3.7686, Perplexity: 43.31907\n",
      "Epoch [1/3], Step [4500/138038], Loss: 4.2929, Perplexity: 73.18160\n",
      "Epoch [1/3], Step [4600/138038], Loss: 4.3682, Perplexity: 78.89954\n",
      "Epoch [1/3], Step [4700/138038], Loss: 4.1082, Perplexity: 60.83587\n",
      "Epoch [1/3], Step [4800/138038], Loss: 4.0143, Perplexity: 55.38668\n",
      "Epoch [1/3], Step [4900/138038], Loss: 5.0026, Perplexity: 148.8041\n",
      "Epoch [1/3], Step [5000/138038], Loss: 3.6634, Perplexity: 38.99397\n",
      "Epoch [1/3], Step [5100/138038], Loss: 4.1526, Perplexity: 63.60002\n",
      "Epoch [1/3], Step [5200/138038], Loss: 4.4663, Perplexity: 87.03290\n",
      "Epoch [1/3], Step [5300/138038], Loss: 3.4614, Perplexity: 31.86089\n",
      "Epoch [1/3], Step [5400/138038], Loss: 4.6810, Perplexity: 107.8809\n",
      "Epoch [1/3], Step [5500/138038], Loss: 3.2358, Perplexity: 25.42562\n",
      "Epoch [1/3], Step [5600/138038], Loss: 3.9363, Perplexity: 51.22888\n",
      "Epoch [1/3], Step [5700/138038], Loss: 4.0796, Perplexity: 59.12403\n",
      "Epoch [1/3], Step [5800/138038], Loss: 4.4915, Perplexity: 89.25956\n",
      "Epoch [1/3], Step [5900/138038], Loss: 5.0142, Perplexity: 150.5372\n",
      "Epoch [1/3], Step [6000/138038], Loss: 3.6693, Perplexity: 39.22338\n",
      "Epoch [1/3], Step [6100/138038], Loss: 4.5510, Perplexity: 94.72344\n",
      "Epoch [1/3], Step [6200/138038], Loss: 4.6756, Perplexity: 107.3022\n",
      "Epoch [1/3], Step [6300/138038], Loss: 4.1592, Perplexity: 64.02332\n",
      "Epoch [1/3], Step [6400/138038], Loss: 4.5718, Perplexity: 96.71517\n",
      "Epoch [1/3], Step [6500/138038], Loss: 3.9506, Perplexity: 51.96553\n",
      "Epoch [1/3], Step [6600/138038], Loss: 3.7131, Perplexity: 40.98172\n",
      "Epoch [1/3], Step [6700/138038], Loss: 3.5287, Perplexity: 34.08029\n",
      "Epoch [1/3], Step [6800/138038], Loss: 4.9682, Perplexity: 143.7673\n",
      "Epoch [1/3], Step [6900/138038], Loss: 3.3071, Perplexity: 27.30512\n",
      "Epoch [1/3], Step [7000/138038], Loss: 4.1090, Perplexity: 60.88375\n",
      "Epoch [1/3], Step [7100/138038], Loss: 3.8742, Perplexity: 48.14203\n",
      "Epoch [1/3], Step [7200/138038], Loss: 4.4833, Perplexity: 88.52506\n",
      "Epoch [1/3], Step [7300/138038], Loss: 3.5740, Perplexity: 35.65836\n",
      "Epoch [1/3], Step [7400/138038], Loss: 4.4213, Perplexity: 83.20450\n",
      "Epoch [1/3], Step [7500/138038], Loss: 4.0409, Perplexity: 56.87591\n",
      "Epoch [1/3], Step [7600/138038], Loss: 3.6756, Perplexity: 39.47085\n",
      "Epoch [1/3], Step [7700/138038], Loss: 3.6774, Perplexity: 39.54291\n",
      "Epoch [1/3], Step [7800/138038], Loss: 4.3229, Perplexity: 75.40333\n",
      "Epoch [1/3], Step [7900/138038], Loss: 3.5526, Perplexity: 34.90481\n",
      "Epoch [1/3], Step [8000/138038], Loss: 3.6582, Perplexity: 38.79247\n",
      "Epoch [1/3], Step [8100/138038], Loss: 3.4990, Perplexity: 33.08138\n",
      "Epoch [1/3], Step [8200/138038], Loss: 3.6650, Perplexity: 39.05576\n",
      "Epoch [1/3], Step [8300/138038], Loss: 3.3553, Perplexity: 28.65420\n",
      "Epoch [1/3], Step [8400/138038], Loss: 4.4428, Perplexity: 85.01617\n",
      "Epoch [1/3], Step [8500/138038], Loss: 3.4133, Perplexity: 30.36590\n",
      "Epoch [1/3], Step [8600/138038], Loss: 3.7057, Perplexity: 40.67697\n",
      "Epoch [1/3], Step [8700/138038], Loss: 3.3457, Perplexity: 28.38127\n",
      "Epoch [1/3], Step [8800/138038], Loss: 3.9567, Perplexity: 52.28471\n",
      "Epoch [1/3], Step [8900/138038], Loss: 3.6599, Perplexity: 38.85704\n",
      "Epoch [1/3], Step [9000/138038], Loss: 3.5944, Perplexity: 36.39321\n",
      "Epoch [1/3], Step [9100/138038], Loss: 4.6915, Perplexity: 109.0142\n",
      "Epoch [1/3], Step [9200/138038], Loss: 4.2755, Perplexity: 71.91281\n",
      "Epoch [1/3], Step [9300/138038], Loss: 3.8184, Perplexity: 45.52959\n",
      "Epoch [1/3], Step [9400/138038], Loss: 4.5635, Perplexity: 95.92290\n",
      "Epoch [1/3], Step [9500/138038], Loss: 3.2483, Perplexity: 25.74565\n",
      "Epoch [1/3], Step [9600/138038], Loss: 3.1643, Perplexity: 23.67335\n",
      "Epoch [1/3], Step [9700/138038], Loss: 4.1090, Perplexity: 60.88360\n",
      "Epoch [1/3], Step [9800/138038], Loss: 3.8467, Perplexity: 46.83952\n",
      "Epoch [1/3], Step [9900/138038], Loss: 3.4447, Perplexity: 31.33411\n",
      "Epoch [1/3], Step [10000/138038], Loss: 3.1844, Perplexity: 24.1521\n",
      "Epoch [1/3], Step [10100/138038], Loss: 3.7873, Perplexity: 44.13490\n",
      "Epoch [1/3], Step [10200/138038], Loss: 4.3675, Perplexity: 78.84884\n",
      "Epoch [1/3], Step [10300/138038], Loss: 3.7851, Perplexity: 44.03799\n",
      "Epoch [1/3], Step [10400/138038], Loss: 4.1997, Perplexity: 66.66649\n",
      "Epoch [1/3], Step [10500/138038], Loss: 4.4816, Perplexity: 88.37911\n",
      "Epoch [1/3], Step [10600/138038], Loss: 4.2950, Perplexity: 73.33217\n",
      "Epoch [1/3], Step [10700/138038], Loss: 4.2402, Perplexity: 69.42399\n",
      "Epoch [1/3], Step [10800/138038], Loss: 3.7090, Perplexity: 40.81404\n",
      "Epoch [1/3], Step [10900/138038], Loss: 3.4992, Perplexity: 33.09062\n",
      "Epoch [1/3], Step [11000/138038], Loss: 3.7315, Perplexity: 41.74281\n",
      "Epoch [1/3], Step [11100/138038], Loss: 4.6928, Perplexity: 109.1626\n",
      "Epoch [1/3], Step [11200/138038], Loss: 4.4935, Perplexity: 89.43449\n",
      "Epoch [1/3], Step [11300/138038], Loss: 3.0983, Perplexity: 22.15972\n",
      "Epoch [1/3], Step [11400/138038], Loss: 3.5196, Perplexity: 33.76986\n",
      "Epoch [1/3], Step [11500/138038], Loss: 3.8836, Perplexity: 48.59970\n",
      "Epoch [1/3], Step [11600/138038], Loss: 3.5754, Perplexity: 35.70782\n",
      "Epoch [1/3], Step [11700/138038], Loss: 4.2156, Perplexity: 67.73427\n",
      "Epoch [1/3], Step [11800/138038], Loss: 3.2037, Perplexity: 24.62458\n",
      "Epoch [1/3], Step [11900/138038], Loss: 3.8800, Perplexity: 48.42556\n",
      "Epoch [1/3], Step [12000/138038], Loss: 3.2003, Perplexity: 24.54068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [12100/138038], Loss: 4.0432, Perplexity: 57.00975\n",
      "Epoch [1/3], Step [12200/138038], Loss: 3.3443, Perplexity: 28.34206\n",
      "Epoch [1/3], Step [12300/138038], Loss: 4.5519, Perplexity: 94.81028\n",
      "Epoch [1/3], Step [12400/138038], Loss: 3.3265, Perplexity: 27.84173\n",
      "Epoch [1/3], Step [12500/138038], Loss: 3.6667, Perplexity: 39.12309\n",
      "Epoch [1/3], Step [12600/138038], Loss: 4.5171, Perplexity: 91.56865\n",
      "Epoch [1/3], Step [12700/138038], Loss: 3.5817, Perplexity: 35.93408\n",
      "Epoch [1/3], Step [12800/138038], Loss: 3.6377, Perplexity: 38.00561\n",
      "Epoch [1/3], Step [12900/138038], Loss: 4.4117, Perplexity: 82.41141\n",
      "Epoch [1/3], Step [13000/138038], Loss: 3.4575, Perplexity: 31.73709\n",
      "Epoch [1/3], Step [13100/138038], Loss: 4.0240, Perplexity: 55.92453\n",
      "Epoch [1/3], Step [13200/138038], Loss: 3.5956, Perplexity: 36.43741\n",
      "Epoch [1/3], Step [13300/138038], Loss: 4.3064, Perplexity: 74.16936\n",
      "Epoch [1/3], Step [13400/138038], Loss: 3.7802, Perplexity: 43.82321\n",
      "Epoch [1/3], Step [13500/138038], Loss: 4.7157, Perplexity: 111.6887\n",
      "Epoch [1/3], Step [13600/138038], Loss: 3.0089, Perplexity: 20.26440\n",
      "Epoch [1/3], Step [13700/138038], Loss: 4.4925, Perplexity: 89.34812\n",
      "Epoch [1/3], Step [13800/138038], Loss: 3.6182, Perplexity: 37.26982\n",
      "Epoch [1/3], Step [13900/138038], Loss: 3.3723, Perplexity: 29.14505\n",
      "Epoch [1/3], Step [14000/138038], Loss: 4.1161, Perplexity: 61.31815\n",
      "Epoch [1/3], Step [14100/138038], Loss: 3.6566, Perplexity: 38.73124\n",
      "Epoch [1/3], Step [14200/138038], Loss: 3.7416, Perplexity: 42.16401\n",
      "Epoch [1/3], Step [14300/138038], Loss: 3.2943, Perplexity: 26.95803\n",
      "Epoch [1/3], Step [14400/138038], Loss: 4.0049, Perplexity: 54.86657\n",
      "Epoch [1/3], Step [14500/138038], Loss: 3.6705, Perplexity: 39.26995\n",
      "Epoch [1/3], Step [14600/138038], Loss: 3.5250, Perplexity: 33.95518\n",
      "Epoch [1/3], Step [14700/138038], Loss: 2.8528, Perplexity: 17.33545\n",
      "Epoch [1/3], Step [14800/138038], Loss: 3.6174, Perplexity: 37.24117\n",
      "Epoch [1/3], Step [14900/138038], Loss: 3.7800, Perplexity: 43.81756\n",
      "Epoch [1/3], Step [15000/138038], Loss: 3.4288, Perplexity: 30.84048\n",
      "Epoch [1/3], Step [15100/138038], Loss: 3.1568, Perplexity: 23.49526\n",
      "Epoch [1/3], Step [15200/138038], Loss: 3.0813, Perplexity: 21.78625\n",
      "Epoch [1/3], Step [15300/138038], Loss: 3.7041, Perplexity: 40.61397\n",
      "Epoch [1/3], Step [15400/138038], Loss: 3.1430, Perplexity: 23.17282\n",
      "Epoch [1/3], Step [15500/138038], Loss: 4.3362, Perplexity: 76.41336\n",
      "Epoch [1/3], Step [15600/138038], Loss: 2.9751, Perplexity: 19.59091\n",
      "Epoch [1/3], Step [15700/138038], Loss: 2.9257, Perplexity: 18.64683\n",
      "Epoch [1/3], Step [15800/138038], Loss: 3.8444, Perplexity: 46.73030\n",
      "Epoch [1/3], Step [15900/138038], Loss: 3.6654, Perplexity: 39.0727\n",
      "Epoch [1/3], Step [16000/138038], Loss: 3.0279, Perplexity: 20.65346\n",
      "Epoch [1/3], Step [16100/138038], Loss: 3.8198, Perplexity: 45.59728\n",
      "Epoch [1/3], Step [16200/138038], Loss: 4.5007, Perplexity: 90.08268\n",
      "Epoch [1/3], Step [16300/138038], Loss: 4.4652, Perplexity: 86.93958\n",
      "Epoch [1/3], Step [16400/138038], Loss: 3.4250, Perplexity: 30.72180\n",
      "Epoch [1/3], Step [16500/138038], Loss: 4.0279, Perplexity: 56.14234\n",
      "Epoch [1/3], Step [16600/138038], Loss: 3.7872, Perplexity: 44.13266\n",
      "Epoch [1/3], Step [16700/138038], Loss: 3.0217, Perplexity: 20.52704\n",
      "Epoch [1/3], Step [16800/138038], Loss: 3.1682, Perplexity: 23.76364\n",
      "Epoch [1/3], Step [16900/138038], Loss: 2.6302, Perplexity: 13.87673\n",
      "Epoch [1/3], Step [17000/138038], Loss: 3.0946, Perplexity: 22.07931\n",
      "Epoch [1/3], Step [17100/138038], Loss: 2.7374, Perplexity: 15.44711\n",
      "Epoch [1/3], Step [17200/138038], Loss: 4.3385, Perplexity: 76.59325\n",
      "Epoch [1/3], Step [17300/138038], Loss: 2.5526, Perplexity: 12.84020\n",
      "Epoch [1/3], Step [17400/138038], Loss: 3.9645, Perplexity: 52.69353\n",
      "Epoch [1/3], Step [17500/138038], Loss: 3.9707, Perplexity: 53.01983\n",
      "Epoch [1/3], Step [17600/138038], Loss: 3.6654, Perplexity: 39.07376\n",
      "Epoch [1/3], Step [17700/138038], Loss: 3.8898, Perplexity: 48.90309\n",
      "Epoch [1/3], Step [17800/138038], Loss: 3.2164, Perplexity: 24.93746\n",
      "Epoch [1/3], Step [17900/138038], Loss: 3.6713, Perplexity: 39.30366\n",
      "Epoch [1/3], Step [18000/138038], Loss: 3.3896, Perplexity: 29.65538\n",
      "Epoch [1/3], Step [18100/138038], Loss: 3.8837, Perplexity: 48.60240\n",
      "Epoch [1/3], Step [18200/138038], Loss: 4.5296, Perplexity: 92.72489\n",
      "Epoch [1/3], Step [18300/138038], Loss: 2.9203, Perplexity: 18.54628\n",
      "Epoch [1/3], Step [18400/138038], Loss: 2.7902, Perplexity: 16.28463\n",
      "Epoch [1/3], Step [18500/138038], Loss: 3.9185, Perplexity: 50.32533\n",
      "Epoch [1/3], Step [18600/138038], Loss: 2.7138, Perplexity: 15.08721\n",
      "Epoch [1/3], Step [18700/138038], Loss: 3.7701, Perplexity: 43.38578\n",
      "Epoch [1/3], Step [18800/138038], Loss: 3.9017, Perplexity: 49.48733\n",
      "Epoch [1/3], Step [18900/138038], Loss: 3.9333, Perplexity: 51.07355\n",
      "Epoch [1/3], Step [19000/138038], Loss: 3.2052, Perplexity: 24.66159\n",
      "Epoch [1/3], Step [19100/138038], Loss: 4.7309, Perplexity: 113.3994\n",
      "Epoch [1/3], Step [19200/138038], Loss: 3.2385, Perplexity: 25.49500\n",
      "Epoch [1/3], Step [19300/138038], Loss: 4.5948, Perplexity: 98.97070\n",
      "Epoch [1/3], Step [19400/138038], Loss: 4.7129, Perplexity: 111.3801\n",
      "Epoch [1/3], Step [19500/138038], Loss: 3.7340, Perplexity: 41.84826\n",
      "Epoch [1/3], Step [19600/138038], Loss: 3.5272, Perplexity: 34.02940\n",
      "Epoch [1/3], Step [19700/138038], Loss: 3.2657, Perplexity: 26.19925\n",
      "Epoch [1/3], Step [19800/138038], Loss: 3.2952, Perplexity: 26.98196\n",
      "Epoch [1/3], Step [19900/138038], Loss: 3.3207, Perplexity: 27.67858\n",
      "Epoch [1/3], Step [20000/138038], Loss: 3.6883, Perplexity: 39.97656\n",
      "Epoch [1/3], Step [20100/138038], Loss: 4.2334, Perplexity: 68.94985\n",
      "Epoch [1/3], Step [20200/138038], Loss: 3.0940, Perplexity: 22.06435\n",
      "Epoch [1/3], Step [20300/138038], Loss: 3.5130, Perplexity: 33.54835\n",
      "Epoch [1/3], Step [20400/138038], Loss: 3.3518, Perplexity: 28.55473\n",
      "Epoch [1/3], Step [20500/138038], Loss: 3.3631, Perplexity: 28.87840\n",
      "Epoch [1/3], Step [20600/138038], Loss: 3.5652, Perplexity: 35.34597\n",
      "Epoch [1/3], Step [20700/138038], Loss: 2.9813, Perplexity: 19.71282\n",
      "Epoch [1/3], Step [20800/138038], Loss: 3.5941, Perplexity: 36.38413\n",
      "Epoch [1/3], Step [20900/138038], Loss: 4.9260, Perplexity: 137.8315\n",
      "Epoch [1/3], Step [21000/138038], Loss: 3.5857, Perplexity: 36.08029\n",
      "Epoch [1/3], Step [21100/138038], Loss: 3.4975, Perplexity: 33.03382\n",
      "Epoch [1/3], Step [21200/138038], Loss: 3.6336, Perplexity: 37.84811\n",
      "Epoch [1/3], Step [21300/138038], Loss: 3.9731, Perplexity: 53.14764\n",
      "Epoch [1/3], Step [21400/138038], Loss: 2.5115, Perplexity: 12.32366\n",
      "Epoch [1/3], Step [21500/138038], Loss: 4.2514, Perplexity: 70.20298\n",
      "Epoch [1/3], Step [21600/138038], Loss: 4.4044, Perplexity: 81.81241\n",
      "Epoch [1/3], Step [21700/138038], Loss: 4.6396, Perplexity: 103.4989\n",
      "Epoch [1/3], Step [21800/138038], Loss: 3.1394, Perplexity: 23.0903\n",
      "Epoch [1/3], Step [21900/138038], Loss: 2.6712, Perplexity: 14.45693\n",
      "Epoch [1/3], Step [22000/138038], Loss: 3.1291, Perplexity: 22.85430\n",
      "Epoch [1/3], Step [22100/138038], Loss: 3.8800, Perplexity: 48.42540\n",
      "Epoch [1/3], Step [22200/138038], Loss: 3.1509, Perplexity: 23.35829\n",
      "Epoch [1/3], Step [22300/138038], Loss: 3.5231, Perplexity: 33.88973\n",
      "Epoch [1/3], Step [22400/138038], Loss: 2.7162, Perplexity: 15.12215\n",
      "Epoch [1/3], Step [22500/138038], Loss: 4.3828, Perplexity: 80.06016\n",
      "Epoch [1/3], Step [22600/138038], Loss: 3.9006, Perplexity: 49.43061\n",
      "Epoch [1/3], Step [22700/138038], Loss: 3.7370, Perplexity: 41.97043\n",
      "Epoch [1/3], Step [22800/138038], Loss: 2.5991, Perplexity: 13.45175\n",
      "Epoch [1/3], Step [22900/138038], Loss: 3.8879, Perplexity: 48.81047\n",
      "Epoch [1/3], Step [23000/138038], Loss: 3.0190, Perplexity: 20.47150\n",
      "Epoch [1/3], Step [23100/138038], Loss: 3.2295, Perplexity: 25.26797\n",
      "Epoch [1/3], Step [23200/138038], Loss: 2.8601, Perplexity: 17.4626\n",
      "Epoch [1/3], Step [23300/138038], Loss: 4.0269, Perplexity: 56.08790\n",
      "Epoch [1/3], Step [23400/138038], Loss: 4.0722, Perplexity: 58.68717\n",
      "Epoch [1/3], Step [23500/138038], Loss: 3.1921, Perplexity: 24.33997\n",
      "Epoch [1/3], Step [23600/138038], Loss: 2.9803, Perplexity: 19.69388\n",
      "Epoch [1/3], Step [23700/138038], Loss: 3.2219, Perplexity: 25.07641\n",
      "Epoch [1/3], Step [23800/138038], Loss: 3.9629, Perplexity: 52.61113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [23900/138038], Loss: 3.2845, Perplexity: 26.69646\n",
      "Epoch [1/3], Step [24000/138038], Loss: 3.5984, Perplexity: 36.54088\n",
      "Epoch [1/3], Step [24100/138038], Loss: 4.9037, Perplexity: 134.7816\n",
      "Epoch [1/3], Step [24200/138038], Loss: 3.0545, Perplexity: 21.209812\n",
      "Epoch [1/3], Step [24300/138038], Loss: 2.8637, Perplexity: 17.52710\n",
      "Epoch [1/3], Step [24400/138038], Loss: 2.9109, Perplexity: 18.37307\n",
      "Epoch [1/3], Step [24500/138038], Loss: 3.8301, Perplexity: 46.06902\n",
      "Epoch [1/3], Step [24600/138038], Loss: 4.4488, Perplexity: 85.52164\n",
      "Epoch [1/3], Step [24700/138038], Loss: 3.2617, Perplexity: 26.09405\n",
      "Epoch [1/3], Step [24800/138038], Loss: 3.5144, Perplexity: 33.59662\n",
      "Epoch [1/3], Step [24900/138038], Loss: 3.8400, Perplexity: 46.52477\n",
      "Epoch [1/3], Step [25000/138038], Loss: 3.5025, Perplexity: 33.19805\n",
      "Epoch [1/3], Step [25100/138038], Loss: 4.9707, Perplexity: 144.1274\n",
      "Epoch [1/3], Step [25200/138038], Loss: 4.2295, Perplexity: 68.68360\n",
      "Epoch [1/3], Step [25300/138038], Loss: 4.0605, Perplexity: 58.00261\n",
      "Epoch [1/3], Step [25400/138038], Loss: 3.6639, Perplexity: 39.01139\n",
      "Epoch [1/3], Step [25500/138038], Loss: 3.5699, Perplexity: 35.51396\n",
      "Epoch [1/3], Step [25600/138038], Loss: 4.0065, Perplexity: 54.95527\n",
      "Epoch [1/3], Step [25700/138038], Loss: 2.9388, Perplexity: 18.89325\n",
      "Epoch [1/3], Step [25800/138038], Loss: 3.1932, Perplexity: 24.36621\n",
      "Epoch [1/3], Step [25900/138038], Loss: 4.0895, Perplexity: 59.70891\n",
      "Epoch [1/3], Step [26000/138038], Loss: 4.2843, Perplexity: 72.54914\n",
      "Epoch [1/3], Step [26100/138038], Loss: 2.9760, Perplexity: 19.60997\n",
      "Epoch [1/3], Step [26200/138038], Loss: 3.8438, Perplexity: 46.70278\n",
      "Epoch [1/3], Step [26300/138038], Loss: 3.4006, Perplexity: 29.98296\n",
      "Epoch [1/3], Step [26400/138038], Loss: 3.6030, Perplexity: 36.70788\n",
      "Epoch [1/3], Step [26500/138038], Loss: 3.3450, Perplexity: 28.36105\n",
      "Epoch [1/3], Step [26600/138038], Loss: 3.4182, Perplexity: 30.51408\n",
      "Epoch [1/3], Step [26700/138038], Loss: 3.6068, Perplexity: 36.84789\n",
      "Epoch [1/3], Step [26800/138038], Loss: 3.3441, Perplexity: 28.33585\n",
      "Epoch [1/3], Step [26900/138038], Loss: 3.4117, Perplexity: 30.31829\n",
      "Epoch [1/3], Step [27000/138038], Loss: 2.9909, Perplexity: 19.90360\n",
      "Epoch [1/3], Step [27100/138038], Loss: 3.0274, Perplexity: 20.64261\n",
      "Epoch [1/3], Step [27200/138038], Loss: 3.2926, Perplexity: 26.91250\n",
      "Epoch [1/3], Step [27300/138038], Loss: 3.5566, Perplexity: 35.04494\n",
      "Epoch [1/3], Step [27400/138038], Loss: 3.4435, Perplexity: 31.29751\n",
      "Epoch [1/3], Step [27500/138038], Loss: 3.4431, Perplexity: 31.28357\n",
      "Epoch [1/3], Step [27600/138038], Loss: 2.6991, Perplexity: 14.86701\n",
      "Epoch [1/3], Step [27700/138038], Loss: 3.4941, Perplexity: 32.91909\n",
      "Epoch [1/3], Step [27800/138038], Loss: 3.1490, Perplexity: 23.31290\n",
      "Epoch [1/3], Step [27900/138038], Loss: 3.4148, Perplexity: 30.41202\n",
      "Epoch [1/3], Step [28000/138038], Loss: 3.6199, Perplexity: 37.33525\n",
      "Epoch [1/3], Step [28100/138038], Loss: 2.7854, Perplexity: 16.20591\n",
      "Epoch [1/3], Step [28200/138038], Loss: 2.9645, Perplexity: 19.38416\n",
      "Epoch [1/3], Step [28300/138038], Loss: 3.5473, Perplexity: 34.71810\n",
      "Epoch [1/3], Step [28400/138038], Loss: 2.9764, Perplexity: 19.61711\n",
      "Epoch [1/3], Step [28500/138038], Loss: 3.1880, Perplexity: 24.24026\n",
      "Epoch [1/3], Step [28600/138038], Loss: 3.2321, Perplexity: 25.3325\n",
      "Epoch [1/3], Step [28700/138038], Loss: 2.5837, Perplexity: 13.2466\n",
      "Epoch [1/3], Step [28800/138038], Loss: 3.6764, Perplexity: 39.50494\n",
      "Epoch [1/3], Step [28900/138038], Loss: 2.4242, Perplexity: 11.29285\n",
      "Epoch [1/3], Step [29000/138038], Loss: 3.5620, Perplexity: 35.23458\n",
      "Epoch [1/3], Step [29100/138038], Loss: 3.9383, Perplexity: 51.33028\n",
      "Epoch [1/3], Step [29200/138038], Loss: 2.5100, Perplexity: 12.30437\n",
      "Epoch [1/3], Step [29300/138038], Loss: 2.4647, Perplexity: 11.75954\n",
      "Epoch [1/3], Step [29400/138038], Loss: 4.2932, Perplexity: 73.19816\n",
      "Epoch [1/3], Step [29500/138038], Loss: 3.3891, Perplexity: 29.6394\n",
      "Epoch [1/3], Step [29600/138038], Loss: 3.4354, Perplexity: 31.04272\n",
      "Epoch [1/3], Step [29700/138038], Loss: 2.9933, Perplexity: 19.9509\n",
      "Epoch [1/3], Step [29800/138038], Loss: 3.4742, Perplexity: 32.27094\n",
      "Epoch [1/3], Step [29900/138038], Loss: 2.6661, Perplexity: 14.38421\n",
      "Epoch [1/3], Step [30000/138038], Loss: 3.3849, Perplexity: 29.51388\n",
      "Epoch [1/3], Step [30100/138038], Loss: 3.5753, Perplexity: 35.70435\n",
      "Epoch [1/3], Step [30200/138038], Loss: 4.2515, Perplexity: 70.20719\n",
      "Epoch [1/3], Step [30300/138038], Loss: 2.1665, Perplexity: 8.727654\n",
      "Epoch [1/3], Step [30400/138038], Loss: 2.9828, Perplexity: 19.74336\n",
      "Epoch [1/3], Step [30500/138038], Loss: 3.1695, Perplexity: 23.79688\n",
      "Epoch [1/3], Step [30600/138038], Loss: 3.3983, Perplexity: 29.91458\n",
      "Epoch [1/3], Step [30700/138038], Loss: 3.0080, Perplexity: 20.2479\n",
      "Epoch [1/3], Step [30800/138038], Loss: 3.1499, Perplexity: 23.33430\n",
      "Epoch [1/3], Step [30900/138038], Loss: 3.2119, Perplexity: 24.82625\n",
      "Epoch [1/3], Step [31000/138038], Loss: 4.1702, Perplexity: 64.72897\n",
      "Epoch [1/3], Step [31100/138038], Loss: 3.1667, Perplexity: 23.72858\n",
      "Epoch [1/3], Step [31200/138038], Loss: 5.1718, Perplexity: 176.2378\n",
      "Epoch [1/3], Step [31300/138038], Loss: 4.9256, Perplexity: 137.7659\n",
      "Epoch [1/3], Step [31400/138038], Loss: 3.6850, Perplexity: 39.844725\n",
      "Epoch [1/3], Step [31500/138038], Loss: 3.3382, Perplexity: 28.16815\n",
      "Epoch [1/3], Step [31600/138038], Loss: 2.4876, Perplexity: 12.03265\n",
      "Epoch [1/3], Step [31700/138038], Loss: 3.6617, Perplexity: 38.92722\n",
      "Epoch [1/3], Step [31800/138038], Loss: 3.3486, Perplexity: 28.46371\n",
      "Epoch [1/3], Step [31900/138038], Loss: 2.7097, Perplexity: 15.02443\n",
      "Epoch [1/3], Step [32000/138038], Loss: 3.8885, Perplexity: 48.83735\n",
      "Epoch [1/3], Step [32100/138038], Loss: 3.0562, Perplexity: 21.24698\n",
      "Epoch [1/3], Step [32200/138038], Loss: 2.9409, Perplexity: 18.93300\n",
      "Epoch [1/3], Step [32300/138038], Loss: 3.4185, Perplexity: 30.52418\n",
      "Epoch [1/3], Step [32400/138038], Loss: 3.0645, Perplexity: 21.42409\n",
      "Epoch [1/3], Step [32500/138038], Loss: 3.9540, Perplexity: 52.14598\n",
      "Epoch [1/3], Step [32600/138038], Loss: 4.4937, Perplexity: 89.4535\n",
      "Epoch [1/3], Step [32700/138038], Loss: 2.6007, Perplexity: 13.47345\n",
      "Epoch [1/3], Step [32800/138038], Loss: 4.1817, Perplexity: 65.47935\n",
      "Epoch [1/3], Step [32900/138038], Loss: 4.4711, Perplexity: 87.45614\n",
      "Epoch [1/3], Step [33000/138038], Loss: 3.6547, Perplexity: 38.65720\n",
      "Epoch [1/3], Step [33100/138038], Loss: 3.4928, Perplexity: 32.87692\n",
      "Epoch [1/3], Step [33200/138038], Loss: 3.0981, Perplexity: 22.15592\n",
      "Epoch [1/3], Step [33300/138038], Loss: 3.2820, Perplexity: 26.62888\n",
      "Epoch [1/3], Step [33400/138038], Loss: 2.9658, Perplexity: 19.40953\n",
      "Epoch [1/3], Step [33500/138038], Loss: 4.4093, Perplexity: 82.21212\n",
      "Epoch [1/3], Step [33600/138038], Loss: 3.4570, Perplexity: 31.72317\n",
      "Epoch [1/3], Step [33700/138038], Loss: 3.0944, Perplexity: 22.07422\n",
      "Epoch [1/3], Step [33800/138038], Loss: 3.2932, Perplexity: 26.92948\n",
      "Epoch [1/3], Step [33900/138038], Loss: 3.5467, Perplexity: 34.69920\n",
      "Epoch [1/3], Step [34000/138038], Loss: 4.5848, Perplexity: 97.98549\n",
      "Epoch [1/3], Step [34100/138038], Loss: 3.6899, Perplexity: 40.04190\n",
      "Epoch [1/3], Step [34200/138038], Loss: 2.9992, Perplexity: 20.06853\n",
      "Epoch [1/3], Step [34300/138038], Loss: 3.0847, Perplexity: 21.86019\n",
      "Epoch [1/3], Step [34400/138038], Loss: 2.8773, Perplexity: 17.76544\n",
      "Epoch [1/3], Step [34500/138038], Loss: 4.0946, Perplexity: 60.01331\n",
      "Epoch [1/3], Step [34600/138038], Loss: 3.0146, Perplexity: 20.38026\n",
      "Epoch [1/3], Step [34700/138038], Loss: 3.1439, Perplexity: 23.19321\n",
      "Epoch [1/3], Step [34800/138038], Loss: 2.8802, Perplexity: 17.81722\n",
      "Epoch [1/3], Step [34900/138038], Loss: 3.1482, Perplexity: 23.29356\n",
      "Epoch [1/3], Step [35000/138038], Loss: 2.3475, Perplexity: 10.45998\n",
      "Epoch [1/3], Step [35100/138038], Loss: 3.5139, Perplexity: 33.58026\n",
      "Epoch [1/3], Step [35200/138038], Loss: 2.9383, Perplexity: 18.8844\n",
      "Epoch [1/3], Step [35300/138038], Loss: 3.4663, Perplexity: 32.01848\n",
      "Epoch [1/3], Step [35400/138038], Loss: 2.9100, Perplexity: 18.3569\n",
      "Epoch [1/3], Step [35500/138038], Loss: 3.4008, Perplexity: 29.98696\n",
      "Epoch [1/3], Step [35600/138038], Loss: 2.8142, Perplexity: 16.67913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [35700/138038], Loss: 2.8109, Perplexity: 16.62566\n",
      "Epoch [1/3], Step [35800/138038], Loss: 2.8033, Perplexity: 16.49872\n",
      "Epoch [1/3], Step [35900/138038], Loss: 3.7450, Perplexity: 42.30819\n",
      "Epoch [1/3], Step [36000/138038], Loss: 2.8795, Perplexity: 17.80564\n",
      "Epoch [1/3], Step [36100/138038], Loss: 3.2155, Perplexity: 24.91511\n",
      "Epoch [1/3], Step [36200/138038], Loss: 3.3663, Perplexity: 28.96993\n",
      "Epoch [1/3], Step [36300/138038], Loss: 3.8657, Perplexity: 47.7359\n",
      "Epoch [1/3], Step [36400/138038], Loss: 3.0360, Perplexity: 20.82201\n",
      "Epoch [1/3], Step [36500/138038], Loss: 3.1364, Perplexity: 23.02061\n",
      "Epoch [1/3], Step [36600/138038], Loss: 4.2066, Perplexity: 67.12641\n",
      "Epoch [1/3], Step [36700/138038], Loss: 3.6267, Perplexity: 37.58870\n",
      "Epoch [1/3], Step [36800/138038], Loss: 3.7455, Perplexity: 42.33155\n",
      "Epoch [1/3], Step [36900/138038], Loss: 4.1058, Perplexity: 60.69061\n",
      "Epoch [1/3], Step [37000/138038], Loss: 4.7067, Perplexity: 110.6813\n",
      "Epoch [1/3], Step [37100/138038], Loss: 2.9007, Perplexity: 18.18608\n",
      "Epoch [1/3], Step [37200/138038], Loss: 2.8796, Perplexity: 17.80795\n",
      "Epoch [1/3], Step [37300/138038], Loss: 3.4881, Perplexity: 32.72417\n",
      "Epoch [1/3], Step [37400/138038], Loss: 3.9985, Perplexity: 54.51810\n",
      "Epoch [1/3], Step [37500/138038], Loss: 3.2262, Perplexity: 25.18486\n",
      "Epoch [1/3], Step [37600/138038], Loss: 3.8135, Perplexity: 45.30801\n",
      "Epoch [1/3], Step [37700/138038], Loss: 2.9911, Perplexity: 19.90784\n",
      "Epoch [1/3], Step [37800/138038], Loss: 3.4892, Perplexity: 32.76042\n",
      "Epoch [1/3], Step [37900/138038], Loss: 2.8743, Perplexity: 17.71229\n",
      "Epoch [1/3], Step [38000/138038], Loss: 3.0223, Perplexity: 20.53818\n",
      "Epoch [1/3], Step [38100/138038], Loss: 4.1106, Perplexity: 60.98226\n",
      "Epoch [1/3], Step [38200/138038], Loss: 3.5231, Perplexity: 33.89082\n",
      "Epoch [1/3], Step [38300/138038], Loss: 3.9767, Perplexity: 53.34032\n",
      "Epoch [1/3], Step [38400/138038], Loss: 2.5618, Perplexity: 12.95880\n",
      "Epoch [1/3], Step [38500/138038], Loss: 3.5172, Perplexity: 33.68985\n",
      "Epoch [1/3], Step [38600/138038], Loss: 2.3440, Perplexity: 10.4232\n",
      "Epoch [1/3], Step [38700/138038], Loss: 3.4066, Perplexity: 30.16232\n",
      "Epoch [1/3], Step [38800/138038], Loss: 3.1752, Perplexity: 23.93079\n",
      "Epoch [1/3], Step [38900/138038], Loss: 3.4762, Perplexity: 32.33585\n",
      "Epoch [1/3], Step [39000/138038], Loss: 3.6540, Perplexity: 38.63068\n",
      "Epoch [1/3], Step [39100/138038], Loss: 3.6895, Perplexity: 40.02417\n",
      "Epoch [1/3], Step [39200/138038], Loss: 4.1275, Perplexity: 62.02232\n",
      "Epoch [1/3], Step [39300/138038], Loss: 2.8902, Perplexity: 17.99788\n",
      "Epoch [1/3], Step [39400/138038], Loss: 3.1107, Perplexity: 22.43727\n",
      "Epoch [1/3], Step [39500/138038], Loss: 4.1368, Perplexity: 62.60463\n",
      "Epoch [1/3], Step [39600/138038], Loss: 3.8550, Perplexity: 47.22869\n",
      "Epoch [1/3], Step [39700/138038], Loss: 3.9050, Perplexity: 49.65165\n",
      "Epoch [1/3], Step [39800/138038], Loss: 3.1471, Perplexity: 23.26757\n",
      "Epoch [1/3], Step [39900/138038], Loss: 2.9047, Perplexity: 18.2602\n",
      "Epoch [1/3], Step [40000/138038], Loss: 2.8157, Perplexity: 16.70518\n",
      "Epoch [1/3], Step [40100/138038], Loss: 4.4149, Perplexity: 82.67628\n",
      "Epoch [1/3], Step [40200/138038], Loss: 3.2697, Perplexity: 26.3046\n",
      "Epoch [1/3], Step [40300/138038], Loss: 3.5071, Perplexity: 33.35241\n",
      "Epoch [1/3], Step [40400/138038], Loss: 2.8138, Perplexity: 16.67291\n",
      "Epoch [1/3], Step [40500/138038], Loss: 3.2720, Perplexity: 26.36398\n",
      "Epoch [1/3], Step [40600/138038], Loss: 2.7950, Perplexity: 16.36193\n",
      "Epoch [1/3], Step [40700/138038], Loss: 2.7594, Perplexity: 15.79018\n",
      "Epoch [1/3], Step [40800/138038], Loss: 3.8271, Perplexity: 45.93003\n",
      "Epoch [1/3], Step [40900/138038], Loss: 3.3010, Perplexity: 27.1402\n",
      "Epoch [1/3], Step [41000/138038], Loss: 3.4871, Perplexity: 32.69114\n",
      "Epoch [1/3], Step [41100/138038], Loss: 3.2972, Perplexity: 27.03634\n",
      "Epoch [1/3], Step [41200/138038], Loss: 3.7002, Perplexity: 40.45398\n",
      "Epoch [1/3], Step [41300/138038], Loss: 3.5493, Perplexity: 34.78784\n",
      "Epoch [1/3], Step [41400/138038], Loss: 2.8728, Perplexity: 17.68635\n",
      "Epoch [1/3], Step [41500/138038], Loss: 4.1230, Perplexity: 61.74289\n",
      "Epoch [1/3], Step [41600/138038], Loss: 3.2834, Perplexity: 26.66522\n",
      "Epoch [1/3], Step [41700/138038], Loss: 3.9515, Perplexity: 52.0133\n",
      "Epoch [1/3], Step [41800/138038], Loss: 3.3715, Perplexity: 29.12173\n",
      "Epoch [1/3], Step [41900/138038], Loss: 3.5715, Perplexity: 35.56821\n",
      "Epoch [1/3], Step [42000/138038], Loss: 2.9395, Perplexity: 18.90573\n",
      "Epoch [1/3], Step [42100/138038], Loss: 2.3892, Perplexity: 10.90428\n",
      "Epoch [1/3], Step [42200/138038], Loss: 4.1061, Perplexity: 60.7122\n",
      "Epoch [1/3], Step [42300/138038], Loss: 2.8203, Perplexity: 16.78134\n",
      "Epoch [1/3], Step [42400/138038], Loss: 2.1754, Perplexity: 8.805814\n",
      "Epoch [1/3], Step [42500/138038], Loss: 3.0517, Perplexity: 21.15220\n",
      "Epoch [1/3], Step [42600/138038], Loss: 3.5045, Perplexity: 33.26340\n",
      "Epoch [1/3], Step [42700/138038], Loss: 3.7574, Perplexity: 42.83869\n",
      "Epoch [1/3], Step [42800/138038], Loss: 3.7219, Perplexity: 41.34196\n",
      "Epoch [1/3], Step [42900/138038], Loss: 3.1131, Perplexity: 22.48965\n",
      "Epoch [1/3], Step [43000/138038], Loss: 4.1480, Perplexity: 63.30845\n",
      "Epoch [1/3], Step [43100/138038], Loss: 3.6123, Perplexity: 37.04982\n",
      "Epoch [1/3], Step [43200/138038], Loss: 3.8554, Perplexity: 47.24983\n",
      "Epoch [1/3], Step [43300/138038], Loss: 4.3836, Perplexity: 80.12586\n",
      "Epoch [1/3], Step [43400/138038], Loss: 2.5723, Perplexity: 13.09599\n",
      "Epoch [1/3], Step [43500/138038], Loss: 3.2367, Perplexity: 25.44928\n",
      "Epoch [1/3], Step [43600/138038], Loss: 3.0786, Perplexity: 21.72831\n",
      "Epoch [1/3], Step [43700/138038], Loss: 2.8315, Perplexity: 16.97067\n",
      "Epoch [1/3], Step [43800/138038], Loss: 3.2757, Perplexity: 26.4608\n",
      "Epoch [1/3], Step [43900/138038], Loss: 3.0530, Perplexity: 21.1786\n",
      "Epoch [1/3], Step [44000/138038], Loss: 2.6771, Perplexity: 14.54286\n",
      "Epoch [1/3], Step [44100/138038], Loss: 4.1536, Perplexity: 63.66046\n",
      "Epoch [1/3], Step [44200/138038], Loss: 3.5705, Perplexity: 35.53600\n",
      "Epoch [1/3], Step [44300/138038], Loss: 3.2519, Perplexity: 25.84035\n",
      "Epoch [1/3], Step [44400/138038], Loss: 3.4955, Perplexity: 32.96543\n",
      "Epoch [1/3], Step [44500/138038], Loss: 2.8678, Perplexity: 17.59793\n",
      "Epoch [1/3], Step [44600/138038], Loss: 3.6359, Perplexity: 37.93511\n",
      "Epoch [1/3], Step [44700/138038], Loss: 3.4447, Perplexity: 31.33382\n",
      "Epoch [1/3], Step [44800/138038], Loss: 3.2601, Perplexity: 26.05180\n",
      "Epoch [1/3], Step [44900/138038], Loss: 3.8778, Perplexity: 48.31866\n",
      "Epoch [1/3], Step [45000/138038], Loss: 4.6260, Perplexity: 102.1033\n",
      "Epoch [1/3], Step [45100/138038], Loss: 2.8071, Perplexity: 16.56233\n",
      "Epoch [1/3], Step [45200/138038], Loss: 3.8849, Perplexity: 48.66110\n",
      "Epoch [1/3], Step [45300/138038], Loss: 3.7952, Perplexity: 44.48772\n",
      "Epoch [1/3], Step [45400/138038], Loss: 3.6348, Perplexity: 37.89573\n",
      "Epoch [1/3], Step [45500/138038], Loss: 2.9009, Perplexity: 18.1910\n",
      "Epoch [1/3], Step [45600/138038], Loss: 2.9021, Perplexity: 18.21271\n",
      "Epoch [1/3], Step [45700/138038], Loss: 3.8998, Perplexity: 49.3909\n",
      "Epoch [1/3], Step [45800/138038], Loss: 2.9905, Perplexity: 19.89569\n",
      "Epoch [1/3], Step [45900/138038], Loss: 3.7623, Perplexity: 43.04943\n",
      "Epoch [1/3], Step [46000/138038], Loss: 2.8025, Perplexity: 16.4866\n",
      "Epoch [1/3], Step [46100/138038], Loss: 3.6776, Perplexity: 39.55298\n",
      "Epoch [1/3], Step [46200/138038], Loss: 3.2343, Perplexity: 25.38888\n",
      "Epoch [1/3], Step [46300/138038], Loss: 2.4008, Perplexity: 11.03252\n",
      "Epoch [1/3], Step [46400/138038], Loss: 2.1753, Perplexity: 8.805025\n",
      "Epoch [1/3], Step [46500/138038], Loss: 2.5266, Perplexity: 12.5106\n",
      "Epoch [1/3], Step [46600/138038], Loss: 2.9750, Perplexity: 19.58919\n",
      "Epoch [1/3], Step [46700/138038], Loss: 4.3249, Perplexity: 75.55616\n",
      "Epoch [1/3], Step [46800/138038], Loss: 3.1115, Perplexity: 22.4552\n",
      "Epoch [1/3], Step [46900/138038], Loss: 2.4434, Perplexity: 11.51248\n",
      "Epoch [1/3], Step [47000/138038], Loss: 4.0444, Perplexity: 57.07643\n",
      "Epoch [1/3], Step [47100/138038], Loss: 3.2879, Perplexity: 26.7866\n",
      "Epoch [1/3], Step [47200/138038], Loss: 2.6836, Perplexity: 14.6377\n",
      "Epoch [1/3], Step [47300/138038], Loss: 3.3121, Perplexity: 27.44141\n",
      "Epoch [1/3], Step [47400/138038], Loss: 3.2565, Perplexity: 25.95934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [47500/138038], Loss: 2.8126, Perplexity: 16.65302\n",
      "Epoch [1/3], Step [47600/138038], Loss: 2.7977, Perplexity: 16.40741\n",
      "Epoch [1/3], Step [47700/138038], Loss: 2.4577, Perplexity: 11.67811\n",
      "Epoch [1/3], Step [47800/138038], Loss: 3.1066, Perplexity: 22.3441\n",
      "Epoch [1/3], Step [47900/138038], Loss: 2.8816, Perplexity: 17.84365\n",
      "Epoch [1/3], Step [48000/138038], Loss: 2.7723, Perplexity: 15.99600\n",
      "Epoch [1/3], Step [48100/138038], Loss: 2.6535, Perplexity: 14.20391\n",
      "Epoch [1/3], Step [48200/138038], Loss: 3.4741, Perplexity: 32.26932\n",
      "Epoch [1/3], Step [48300/138038], Loss: 2.4853, Perplexity: 12.00468\n",
      "Epoch [1/3], Step [48400/138038], Loss: 2.1443, Perplexity: 8.536191\n",
      "Epoch [1/3], Step [48500/138038], Loss: 4.8910, Perplexity: 133.0816\n",
      "Epoch [1/3], Step [48600/138038], Loss: 2.9137, Perplexity: 18.42464\n",
      "Epoch [1/3], Step [48700/138038], Loss: 4.4796, Perplexity: 88.20241\n",
      "Epoch [1/3], Step [48800/138038], Loss: 3.1966, Perplexity: 24.44820\n",
      "Epoch [1/3], Step [48900/138038], Loss: 2.9574, Perplexity: 19.24850\n",
      "Epoch [1/3], Step [49000/138038], Loss: 2.7784, Perplexity: 16.09318\n",
      "Epoch [1/3], Step [49100/138038], Loss: 2.0013, Perplexity: 7.398835\n",
      "Epoch [1/3], Step [49200/138038], Loss: 4.5617, Perplexity: 95.74188\n",
      "Epoch [1/3], Step [49300/138038], Loss: 4.5242, Perplexity: 92.22674\n",
      "Epoch [1/3], Step [49400/138038], Loss: 3.5946, Perplexity: 36.4028\n",
      "Epoch [1/3], Step [49500/138038], Loss: 3.4645, Perplexity: 31.96124\n",
      "Epoch [1/3], Step [49600/138038], Loss: 3.2769, Perplexity: 26.49399\n",
      "Epoch [1/3], Step [49700/138038], Loss: 2.6512, Perplexity: 14.17148\n",
      "Epoch [1/3], Step [49800/138038], Loss: 3.6502, Perplexity: 38.48388\n",
      "Epoch [1/3], Step [49900/138038], Loss: 3.4319, Perplexity: 30.93558\n",
      "Epoch [1/3], Step [50000/138038], Loss: 3.7747, Perplexity: 43.58241\n",
      "Epoch [1/3], Step [50100/138038], Loss: 3.4557, Perplexity: 31.6790\n",
      "Epoch [1/3], Step [50200/138038], Loss: 3.5118, Perplexity: 33.50757\n",
      "Epoch [1/3], Step [50300/138038], Loss: 4.6565, Perplexity: 105.2644\n",
      "Epoch [1/3], Step [50400/138038], Loss: 3.7168, Perplexity: 41.13161\n",
      "Epoch [1/3], Step [50500/138038], Loss: 3.6439, Perplexity: 38.24188\n",
      "Epoch [1/3], Step [50600/138038], Loss: 2.5830, Perplexity: 13.23720\n",
      "Epoch [1/3], Step [50700/138038], Loss: 3.1925, Perplexity: 24.3488\n",
      "Epoch [1/3], Step [50800/138038], Loss: 2.9890, Perplexity: 19.86597\n",
      "Epoch [1/3], Step [50900/138038], Loss: 3.2479, Perplexity: 25.73518\n",
      "Epoch [1/3], Step [51000/138038], Loss: 3.0863, Perplexity: 21.89705\n",
      "Epoch [1/3], Step [51100/138038], Loss: 3.6157, Perplexity: 37.17812\n",
      "Epoch [1/3], Step [51200/138038], Loss: 3.8539, Perplexity: 47.17525\n",
      "Epoch [1/3], Step [51300/138038], Loss: 3.0273, Perplexity: 20.64067\n",
      "Epoch [1/3], Step [51400/138038], Loss: 2.9862, Perplexity: 19.81024\n",
      "Epoch [1/3], Step [51500/138038], Loss: 3.7340, Perplexity: 41.84745\n",
      "Epoch [1/3], Step [51600/138038], Loss: 2.9662, Perplexity: 19.41883\n",
      "Epoch [1/3], Step [51700/138038], Loss: 2.9372, Perplexity: 18.86267\n",
      "Epoch [1/3], Step [51800/138038], Loss: 4.6536, Perplexity: 104.9630\n",
      "Epoch [1/3], Step [51900/138038], Loss: 3.4233, Perplexity: 30.66932\n",
      "Epoch [1/3], Step [52000/138038], Loss: 3.3879, Perplexity: 29.60372\n",
      "Epoch [1/3], Step [52100/138038], Loss: 2.6799, Perplexity: 14.5832\n",
      "Epoch [1/3], Step [52200/138038], Loss: 3.4335, Perplexity: 30.98581\n",
      "Epoch [1/3], Step [52300/138038], Loss: 2.3993, Perplexity: 11.01548\n",
      "Epoch [1/3], Step [52400/138038], Loss: 2.6640, Perplexity: 14.35378\n",
      "Epoch [1/3], Step [52500/138038], Loss: 2.4711, Perplexity: 11.8357\n",
      "Epoch [1/3], Step [52600/138038], Loss: 4.1756, Perplexity: 65.08093\n",
      "Epoch [1/3], Step [52700/138038], Loss: 3.3258, Perplexity: 27.8214\n",
      "Epoch [1/3], Step [52800/138038], Loss: 3.6687, Perplexity: 39.20039\n",
      "Epoch [1/3], Step [52900/138038], Loss: 3.4471, Perplexity: 31.40967\n",
      "Epoch [1/3], Step [53000/138038], Loss: 2.9064, Perplexity: 18.29063\n",
      "Epoch [1/3], Step [53100/138038], Loss: 3.7916, Perplexity: 44.3278\n",
      "Epoch [1/3], Step [53200/138038], Loss: 3.2992, Perplexity: 27.08963\n",
      "Epoch [1/3], Step [53300/138038], Loss: 3.7252, Perplexity: 41.47758\n",
      "Epoch [1/3], Step [53400/138038], Loss: 3.1295, Perplexity: 22.86243\n",
      "Epoch [1/3], Step [53500/138038], Loss: 2.8528, Perplexity: 17.3357\n",
      "Epoch [1/3], Step [53600/138038], Loss: 2.4827, Perplexity: 11.97361\n",
      "Epoch [1/3], Step [53700/138038], Loss: 2.0870, Perplexity: 8.060797\n",
      "Epoch [1/3], Step [53800/138038], Loss: 3.9324, Perplexity: 51.0274\n",
      "Epoch [1/3], Step [53900/138038], Loss: 3.1338, Perplexity: 22.96147\n",
      "Epoch [1/3], Step [54000/138038], Loss: 3.1154, Perplexity: 22.54293\n",
      "Epoch [1/3], Step [54100/138038], Loss: 3.2799, Perplexity: 26.5733\n",
      "Epoch [1/3], Step [54200/138038], Loss: 2.9198, Perplexity: 18.5381\n",
      "Epoch [1/3], Step [54300/138038], Loss: 4.7948, Perplexity: 120.8801\n",
      "Epoch [1/3], Step [54400/138038], Loss: 3.0190, Perplexity: 20.47109\n",
      "Epoch [1/3], Step [54500/138038], Loss: 4.1586, Perplexity: 63.98482\n",
      "Epoch [1/3], Step [54600/138038], Loss: 2.9690, Perplexity: 19.47310\n",
      "Epoch [1/3], Step [54700/138038], Loss: 3.0204, Perplexity: 20.5003\n",
      "Epoch [1/3], Step [54800/138038], Loss: 2.6275, Perplexity: 13.83923\n",
      "Epoch [1/3], Step [54900/138038], Loss: 3.4079, Perplexity: 30.2002\n",
      "Epoch [1/3], Step [55000/138038], Loss: 4.1460, Perplexity: 63.17913\n",
      "Epoch [1/3], Step [55100/138038], Loss: 3.5183, Perplexity: 33.72690\n",
      "Epoch [1/3], Step [55200/138038], Loss: 3.3095, Perplexity: 27.3714\n",
      "Epoch [1/3], Step [55300/138038], Loss: 3.5328, Perplexity: 34.21853\n",
      "Epoch [1/3], Step [55400/138038], Loss: 3.3611, Perplexity: 28.82234\n",
      "Epoch [1/3], Step [55500/138038], Loss: 2.7185, Perplexity: 15.15740\n",
      "Epoch [1/3], Step [55600/138038], Loss: 2.0069, Perplexity: 7.44032\n",
      "Epoch [1/3], Step [55700/138038], Loss: 3.7535, Perplexity: 42.67215\n",
      "Epoch [1/3], Step [55800/138038], Loss: 2.8869, Perplexity: 17.9372\n",
      "Epoch [1/3], Step [55900/138038], Loss: 2.6055, Perplexity: 13.53857\n",
      "Epoch [1/3], Step [56000/138038], Loss: 2.6037, Perplexity: 13.51385\n",
      "Epoch [1/3], Step [56100/138038], Loss: 2.0126, Perplexity: 7.482688\n",
      "Epoch [1/3], Step [56200/138038], Loss: 3.0871, Perplexity: 21.91431\n",
      "Epoch [1/3], Step [56300/138038], Loss: 2.8834, Perplexity: 17.8749\n",
      "Epoch [1/3], Step [56400/138038], Loss: 2.8368, Perplexity: 17.0619\n",
      "Epoch [1/3], Step [56500/138038], Loss: 2.9272, Perplexity: 18.67441\n",
      "Epoch [1/3], Step [56600/138038], Loss: 3.9248, Perplexity: 50.64300\n",
      "Epoch [1/3], Step [56700/138038], Loss: 3.3374, Perplexity: 28.14695\n",
      "Epoch [1/3], Step [56800/138038], Loss: 3.1371, Perplexity: 23.03680\n",
      "Epoch [1/3], Step [56900/138038], Loss: 4.3664, Perplexity: 78.76283\n",
      "Epoch [1/3], Step [57000/138038], Loss: 3.1653, Perplexity: 23.69624\n",
      "Epoch [1/3], Step [57100/138038], Loss: 3.3997, Perplexity: 29.95579\n",
      "Epoch [1/3], Step [57200/138038], Loss: 2.8616, Perplexity: 17.4888\n",
      "Epoch [1/3], Step [57300/138038], Loss: 2.5766, Perplexity: 13.15280\n",
      "Epoch [1/3], Step [57400/138038], Loss: 2.6124, Perplexity: 13.63204\n",
      "Epoch [1/3], Step [57500/138038], Loss: 2.5915, Perplexity: 13.3500\n",
      "Epoch [1/3], Step [57600/138038], Loss: 3.4935, Perplexity: 32.90033\n",
      "Epoch [1/3], Step [57700/138038], Loss: 2.5139, Perplexity: 12.35259\n",
      "Epoch [1/3], Step [57800/138038], Loss: 4.2617, Perplexity: 70.93173\n",
      "Epoch [1/3], Step [57900/138038], Loss: 3.4130, Perplexity: 30.35580\n",
      "Epoch [1/3], Step [58000/138038], Loss: 4.1632, Perplexity: 64.27540\n",
      "Epoch [1/3], Step [58100/138038], Loss: 3.6224, Perplexity: 37.42551\n",
      "Epoch [1/3], Step [58200/138038], Loss: 3.6854, Perplexity: 39.8606\n",
      "Epoch [1/3], Step [58300/138038], Loss: 3.6543, Perplexity: 38.6411\n",
      "Epoch [1/3], Step [58400/138038], Loss: 3.9387, Perplexity: 51.35315\n",
      "Epoch [1/3], Step [58500/138038], Loss: 2.4406, Perplexity: 11.47971\n",
      "Epoch [1/3], Step [58600/138038], Loss: 3.3834, Perplexity: 29.46947\n",
      "Epoch [1/3], Step [58700/138038], Loss: 2.6752, Perplexity: 14.51471\n",
      "Epoch [1/3], Step [58800/138038], Loss: 2.3734, Perplexity: 10.73422\n",
      "Epoch [1/3], Step [58900/138038], Loss: 3.9526, Perplexity: 52.07049\n",
      "Epoch [1/3], Step [59000/138038], Loss: 3.8562, Perplexity: 47.28649\n",
      "Epoch [1/3], Step [59100/138038], Loss: 3.9922, Perplexity: 54.17369\n",
      "Epoch [1/3], Step [59200/138038], Loss: 3.2242, Perplexity: 25.1332\n",
      "Epoch [1/3], Step [59300/138038], Loss: 3.6010, Perplexity: 36.63538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [59400/138038], Loss: 3.2827, Perplexity: 26.64792\n",
      "Epoch [1/3], Step [59500/138038], Loss: 2.3942, Perplexity: 10.95994\n",
      "Epoch [1/3], Step [59600/138038], Loss: 3.5814, Perplexity: 35.92399\n",
      "Epoch [1/3], Step [59700/138038], Loss: 3.4234, Perplexity: 30.67414\n",
      "Epoch [1/3], Step [59800/138038], Loss: 3.2238, Perplexity: 25.12228\n",
      "Epoch [1/3], Step [59900/138038], Loss: 1.9141, Perplexity: 6.78082\n",
      "Epoch [1/3], Step [60000/138038], Loss: 3.2716, Perplexity: 26.35354\n",
      "Epoch [1/3], Step [60100/138038], Loss: 2.8215, Perplexity: 16.80139\n",
      "Epoch [1/3], Step [60200/138038], Loss: 3.4999, Perplexity: 33.11183\n",
      "Epoch [1/3], Step [60300/138038], Loss: 2.9420, Perplexity: 18.95326\n",
      "Epoch [1/3], Step [60400/138038], Loss: 3.4727, Perplexity: 32.22225\n",
      "Epoch [1/3], Step [60500/138038], Loss: 3.3759, Perplexity: 29.2505\n",
      "Epoch [1/3], Step [60600/138038], Loss: 2.7917, Perplexity: 16.3089\n",
      "Epoch [1/3], Step [60700/138038], Loss: 3.0878, Perplexity: 21.9280\n",
      "Epoch [1/3], Step [60800/138038], Loss: 1.9910, Perplexity: 7.322645\n",
      "Epoch [1/3], Step [60900/138038], Loss: 1.8953, Perplexity: 6.654766\n",
      "Epoch [1/3], Step [61000/138038], Loss: 2.7472, Perplexity: 15.5993\n",
      "Epoch [1/3], Step [61100/138038], Loss: 3.3917, Perplexity: 29.71605\n",
      "Epoch [1/3], Step [61200/138038], Loss: 4.1459, Perplexity: 63.17723\n",
      "Epoch [1/3], Step [61300/138038], Loss: 2.4912, Perplexity: 12.07611\n",
      "Epoch [1/3], Step [61400/138038], Loss: 4.1166, Perplexity: 61.3495\n",
      "Epoch [1/3], Step [61500/138038], Loss: 3.8862, Perplexity: 48.72691\n",
      "Epoch [1/3], Step [61600/138038], Loss: 2.5546, Perplexity: 12.86644\n",
      "Epoch [1/3], Step [61700/138038], Loss: 3.0628, Perplexity: 21.3882\n",
      "Epoch [1/3], Step [61800/138038], Loss: 2.8525, Perplexity: 17.33098\n",
      "Epoch [1/3], Step [61900/138038], Loss: 4.1540, Perplexity: 63.68845\n",
      "Epoch [1/3], Step [62000/138038], Loss: 3.3530, Perplexity: 28.58734\n",
      "Epoch [1/3], Step [62100/138038], Loss: 2.7645, Perplexity: 15.8710\n",
      "Epoch [1/3], Step [62200/138038], Loss: 2.5818, Perplexity: 13.2207\n",
      "Epoch [1/3], Step [62300/138038], Loss: 2.1515, Perplexity: 8.597747\n",
      "Epoch [1/3], Step [62400/138038], Loss: 2.3190, Perplexity: 10.1653\n",
      "Epoch [1/3], Step [62500/138038], Loss: 3.0020, Perplexity: 20.12652\n",
      "Epoch [1/3], Step [62600/138038], Loss: 2.1904, Perplexity: 8.93903\n",
      "Epoch [1/3], Step [62700/138038], Loss: 2.7051, Perplexity: 14.95631\n",
      "Epoch [1/3], Step [62800/138038], Loss: 3.9946, Perplexity: 54.30200\n",
      "Epoch [1/3], Step [62900/138038], Loss: 3.4953, Perplexity: 32.95876\n",
      "Epoch [1/3], Step [63000/138038], Loss: 3.5783, Perplexity: 35.8122\n",
      "Epoch [1/3], Step [63100/138038], Loss: 2.8993, Perplexity: 18.16194\n",
      "Epoch [1/3], Step [63200/138038], Loss: 2.6216, Perplexity: 13.75717\n",
      "Epoch [1/3], Step [63300/138038], Loss: 2.4114, Perplexity: 11.1496\n",
      "Epoch [1/3], Step [63400/138038], Loss: 2.6374, Perplexity: 13.97699\n",
      "Epoch [1/3], Step [63500/138038], Loss: 2.1329, Perplexity: 8.43894\n",
      "Epoch [1/3], Step [63600/138038], Loss: 2.2286, Perplexity: 9.287081\n",
      "Epoch [1/3], Step [63700/138038], Loss: 2.1181, Perplexity: 8.314995\n",
      "Epoch [1/3], Step [63800/138038], Loss: 2.7566, Perplexity: 15.74628\n",
      "Epoch [1/3], Step [63900/138038], Loss: 2.6805, Perplexity: 14.5929\n",
      "Epoch [1/3], Step [64000/138038], Loss: 2.3689, Perplexity: 10.68515\n",
      "Epoch [1/3], Step [64100/138038], Loss: 2.2274, Perplexity: 9.275902\n",
      "Epoch [1/3], Step [64200/138038], Loss: 3.0334, Perplexity: 20.76781\n",
      "Epoch [1/3], Step [64300/138038], Loss: 2.9685, Perplexity: 19.46253\n",
      "Epoch [1/3], Step [64400/138038], Loss: 2.2738, Perplexity: 9.716130\n",
      "Epoch [1/3], Step [64500/138038], Loss: 2.8634, Perplexity: 17.5207\n",
      "Epoch [1/3], Step [64600/138038], Loss: 3.2499, Perplexity: 25.78868\n",
      "Epoch [1/3], Step [64700/138038], Loss: 3.9347, Perplexity: 51.14934\n",
      "Epoch [1/3], Step [64800/138038], Loss: 3.5056, Perplexity: 33.30110\n",
      "Epoch [1/3], Step [64900/138038], Loss: 2.4283, Perplexity: 11.33912\n",
      "Epoch [1/3], Step [65000/138038], Loss: 2.5234, Perplexity: 12.47142\n",
      "Epoch [1/3], Step [65100/138038], Loss: 3.5281, Perplexity: 34.06027\n",
      "Epoch [1/3], Step [65200/138038], Loss: 3.2295, Perplexity: 25.26772\n",
      "Epoch [1/3], Step [65300/138038], Loss: 3.2544, Perplexity: 25.90511\n",
      "Epoch [1/3], Step [65400/138038], Loss: 3.2976, Perplexity: 27.04818\n",
      "Epoch [1/3], Step [65500/138038], Loss: 3.3491, Perplexity: 28.47655\n",
      "Epoch [1/3], Step [65600/138038], Loss: 3.2554, Perplexity: 25.92960\n",
      "Epoch [1/3], Step [65700/138038], Loss: 3.7213, Perplexity: 41.31693\n",
      "Epoch [1/3], Step [65800/138038], Loss: 2.2310, Perplexity: 9.308700\n",
      "Epoch [1/3], Step [65900/138038], Loss: 3.3122, Perplexity: 27.44562\n",
      "Epoch [1/3], Step [66000/138038], Loss: 4.1152, Perplexity: 61.26267\n",
      "Epoch [1/3], Step [66100/138038], Loss: 2.7852, Perplexity: 16.2024\n",
      "Epoch [1/3], Step [66200/138038], Loss: 3.1222, Perplexity: 22.69616\n",
      "Epoch [1/3], Step [66300/138038], Loss: 3.3216, Perplexity: 27.70419\n",
      "Epoch [1/3], Step [66400/138038], Loss: 3.2999, Perplexity: 27.11090\n",
      "Epoch [1/3], Step [66500/138038], Loss: 3.4126, Perplexity: 30.34536\n",
      "Epoch [1/3], Step [66600/138038], Loss: 3.4984, Perplexity: 33.0610\n",
      "Epoch [1/3], Step [66700/138038], Loss: 2.7018, Perplexity: 14.90605\n",
      "Epoch [1/3], Step [66800/138038], Loss: 3.3960, Perplexity: 29.8458\n",
      "Epoch [1/3], Step [66900/138038], Loss: 2.5416, Perplexity: 12.69947\n",
      "Epoch [1/3], Step [67000/138038], Loss: 3.5619, Perplexity: 35.23171\n",
      "Epoch [1/3], Step [67100/138038], Loss: 2.7569, Perplexity: 15.75154\n",
      "Epoch [1/3], Step [67200/138038], Loss: 2.7302, Perplexity: 15.33646\n",
      "Epoch [1/3], Step [67300/138038], Loss: 2.8843, Perplexity: 17.89119\n",
      "Epoch [1/3], Step [67400/138038], Loss: 2.2477, Perplexity: 9.466448\n",
      "Epoch [1/3], Step [67500/138038], Loss: 3.1761, Perplexity: 23.95330\n",
      "Epoch [1/3], Step [67600/138038], Loss: 3.1013, Perplexity: 22.22758\n",
      "Epoch [1/3], Step [67700/138038], Loss: 2.6738, Perplexity: 14.49472\n",
      "Epoch [1/3], Step [67800/138038], Loss: 2.5770, Perplexity: 13.15715\n",
      "Epoch [1/3], Step [67900/138038], Loss: 3.4885, Perplexity: 32.7358\n",
      "Epoch [1/3], Step [68000/138038], Loss: 2.7864, Perplexity: 16.22248\n",
      "Epoch [1/3], Step [68100/138038], Loss: 2.6742, Perplexity: 14.50011\n",
      "Epoch [1/3], Step [68200/138038], Loss: 3.3597, Perplexity: 28.77962\n",
      "Epoch [1/3], Step [68300/138038], Loss: 3.4796, Perplexity: 32.44775\n",
      "Epoch [1/3], Step [68400/138038], Loss: 3.5560, Perplexity: 35.0227\n",
      "Epoch [1/3], Step [68500/138038], Loss: 3.1286, Perplexity: 22.84141\n",
      "Epoch [1/3], Step [68600/138038], Loss: 2.5442, Perplexity: 12.73363\n",
      "Epoch [1/3], Step [68700/138038], Loss: 4.6293, Perplexity: 102.4462\n",
      "Epoch [1/3], Step [68800/138038], Loss: 3.2724, Perplexity: 26.37464\n",
      "Epoch [1/3], Step [68900/138038], Loss: 3.3815, Perplexity: 29.4150\n",
      "Epoch [1/3], Step [69000/138038], Loss: 3.3449, Perplexity: 28.3564\n",
      "Epoch [1/3], Step [69100/138038], Loss: 2.3675, Perplexity: 10.67046\n",
      "Epoch [1/3], Step [69200/138038], Loss: 2.7023, Perplexity: 14.91408\n",
      "Epoch [1/3], Step [69300/138038], Loss: 2.6404, Perplexity: 14.01938\n",
      "Epoch [1/3], Step [69400/138038], Loss: 3.9734, Perplexity: 53.16285\n",
      "Epoch [1/3], Step [69500/138038], Loss: 2.0051, Perplexity: 7.42701\n",
      "Epoch [1/3], Step [69600/138038], Loss: 2.8597, Perplexity: 17.4555\n",
      "Epoch [1/3], Step [69700/138038], Loss: 3.6321, Perplexity: 37.7920\n",
      "Epoch [1/3], Step [69800/138038], Loss: 3.3808, Perplexity: 29.3954\n",
      "Epoch [1/3], Step [69900/138038], Loss: 2.5351, Perplexity: 12.6182\n",
      "Epoch [1/3], Step [70000/138038], Loss: 2.0473, Perplexity: 7.74685\n",
      "Epoch [1/3], Step [70100/138038], Loss: 2.6017, Perplexity: 13.48630\n",
      "Epoch [1/3], Step [70200/138038], Loss: 3.4925, Perplexity: 32.86659\n",
      "Epoch [1/3], Step [70300/138038], Loss: 2.2951, Perplexity: 9.925179\n",
      "Epoch [1/3], Step [70400/138038], Loss: 1.9534, Perplexity: 7.05291\n",
      "Epoch [1/3], Step [70500/138038], Loss: 3.0057, Perplexity: 20.19952\n",
      "Epoch [1/3], Step [70600/138038], Loss: 2.8316, Perplexity: 16.97200\n",
      "Epoch [1/3], Step [70700/138038], Loss: 2.9795, Perplexity: 19.6780\n",
      "Epoch [1/3], Step [70800/138038], Loss: 3.3063, Perplexity: 27.28459\n",
      "Epoch [1/3], Step [70900/138038], Loss: 3.5480, Perplexity: 34.74216\n",
      "Epoch [1/3], Step [71000/138038], Loss: 3.0875, Perplexity: 21.9221\n",
      "Epoch [1/3], Step [71100/138038], Loss: 2.5205, Perplexity: 12.4344\n",
      "Epoch [1/3], Step [71200/138038], Loss: 3.5695, Perplexity: 35.4996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [71300/138038], Loss: 2.1870, Perplexity: 8.908310\n",
      "Epoch [1/3], Step [71400/138038], Loss: 2.6071, Perplexity: 13.55998\n",
      "Epoch [1/3], Step [71500/138038], Loss: 3.7703, Perplexity: 43.39218\n",
      "Epoch [1/3], Step [71600/138038], Loss: 3.2505, Perplexity: 25.8021\n",
      "Epoch [1/3], Step [71700/138038], Loss: 3.2722, Perplexity: 26.36881\n",
      "Epoch [1/3], Step [71800/138038], Loss: 2.7749, Perplexity: 16.03660\n",
      "Epoch [1/3], Step [71900/138038], Loss: 3.3764, Perplexity: 29.26395\n",
      "Epoch [1/3], Step [72000/138038], Loss: 3.9761, Perplexity: 53.3094\n",
      "Epoch [1/3], Step [72100/138038], Loss: 2.5661, Perplexity: 13.01533\n",
      "Epoch [1/3], Step [72200/138038], Loss: 2.7054, Perplexity: 14.96029\n",
      "Epoch [1/3], Step [72300/138038], Loss: 3.3413, Perplexity: 28.25563\n",
      "Epoch [1/3], Step [72400/138038], Loss: 3.3236, Perplexity: 27.75965\n",
      "Epoch [1/3], Step [72500/138038], Loss: 3.4092, Perplexity: 30.2416\n",
      "Epoch [1/3], Step [72600/138038], Loss: 3.4655, Perplexity: 31.9924\n",
      "Epoch [1/3], Step [72700/138038], Loss: 2.8071, Perplexity: 16.56168\n",
      "Epoch [1/3], Step [72800/138038], Loss: 2.5988, Perplexity: 13.44760\n",
      "Epoch [1/3], Step [72900/138038], Loss: 3.0021, Perplexity: 20.12764\n",
      "Epoch [1/3], Step [73000/138038], Loss: 2.8140, Perplexity: 16.67642\n",
      "Epoch [1/3], Step [73100/138038], Loss: 3.6058, Perplexity: 36.81067\n",
      "Epoch [1/3], Step [73200/138038], Loss: 3.5545, Perplexity: 34.97208\n",
      "Epoch [1/3], Step [73300/138038], Loss: 3.2684, Perplexity: 26.26859\n",
      "Epoch [1/3], Step [73400/138038], Loss: 2.2070, Perplexity: 9.088778\n",
      "Epoch [1/3], Step [73500/138038], Loss: 2.1067, Perplexity: 8.221028\n",
      "Epoch [1/3], Step [73600/138038], Loss: 2.6054, Perplexity: 13.53625\n",
      "Epoch [1/3], Step [73700/138038], Loss: 2.3323, Perplexity: 10.30163\n",
      "Epoch [1/3], Step [73800/138038], Loss: 3.5646, Perplexity: 35.32418\n",
      "Epoch [1/3], Step [73900/138038], Loss: 3.2493, Perplexity: 25.77333\n",
      "Epoch [1/3], Step [74000/138038], Loss: 2.6662, Perplexity: 14.38544\n",
      "Epoch [1/3], Step [74100/138038], Loss: 2.1129, Perplexity: 8.272097\n",
      "Epoch [1/3], Step [74200/138038], Loss: 3.0147, Perplexity: 20.38391\n",
      "Epoch [1/3], Step [74300/138038], Loss: 2.1625, Perplexity: 8.692400\n",
      "Epoch [1/3], Step [74400/138038], Loss: 2.0193, Perplexity: 7.533073\n",
      "Epoch [1/3], Step [74500/138038], Loss: 3.1856, Perplexity: 24.1818\n",
      "Epoch [1/3], Step [74600/138038], Loss: 2.4325, Perplexity: 11.38703\n",
      "Epoch [1/3], Step [74700/138038], Loss: 3.2897, Perplexity: 26.83596\n",
      "Epoch [1/3], Step [74800/138038], Loss: 2.6135, Perplexity: 13.64721\n",
      "Epoch [1/3], Step [74900/138038], Loss: 3.4894, Perplexity: 32.76640\n",
      "Epoch [1/3], Step [75000/138038], Loss: 2.8846, Perplexity: 17.8972\n",
      "Epoch [1/3], Step [75100/138038], Loss: 2.0612, Perplexity: 7.85505\n",
      "Epoch [1/3], Step [75200/138038], Loss: 2.2855, Perplexity: 9.830447\n",
      "Epoch [1/3], Step [75300/138038], Loss: 2.7853, Perplexity: 16.20465\n",
      "Epoch [1/3], Step [75400/138038], Loss: 3.6287, Perplexity: 37.66327\n",
      "Epoch [1/3], Step [75500/138038], Loss: 2.4472, Perplexity: 11.5562\n",
      "Epoch [1/3], Step [75600/138038], Loss: 3.6054, Perplexity: 36.79615\n",
      "Epoch [1/3], Step [75700/138038], Loss: 3.2269, Perplexity: 25.20156\n",
      "Epoch [1/3], Step [75800/138038], Loss: 4.0552, Perplexity: 57.69690\n",
      "Epoch [1/3], Step [75900/138038], Loss: 2.3291, Perplexity: 10.26908\n",
      "Epoch [1/3], Step [76000/138038], Loss: 2.4234, Perplexity: 11.2840\n",
      "Epoch [1/3], Step [76100/138038], Loss: 2.2034, Perplexity: 9.05589\n",
      "Epoch [1/3], Step [76200/138038], Loss: 2.5846, Perplexity: 13.25851\n",
      "Epoch [1/3], Step [76300/138038], Loss: 3.5872, Perplexity: 36.13378\n",
      "Epoch [1/3], Step [76400/138038], Loss: 2.0886, Perplexity: 8.073825\n",
      "Epoch [1/3], Step [76500/138038], Loss: 3.0107, Perplexity: 20.3022\n",
      "Epoch [1/3], Step [76600/138038], Loss: 3.6825, Perplexity: 39.7476\n",
      "Epoch [1/3], Step [76700/138038], Loss: 3.3973, Perplexity: 29.8845\n",
      "Epoch [1/3], Step [76800/138038], Loss: 3.2985, Perplexity: 27.07096\n",
      "Epoch [1/3], Step [76900/138038], Loss: 2.7166, Perplexity: 15.12907\n",
      "Epoch [1/3], Step [77000/138038], Loss: 3.3778, Perplexity: 29.3066\n",
      "Epoch [1/3], Step [77100/138038], Loss: 2.8286, Perplexity: 16.92224\n",
      "Epoch [1/3], Step [77200/138038], Loss: 2.5682, Perplexity: 13.0429\n",
      "Epoch [1/3], Step [77300/138038], Loss: 2.2995, Perplexity: 9.969728\n",
      "Epoch [1/3], Step [77400/138038], Loss: 2.5960, Perplexity: 13.41042\n",
      "Epoch [1/3], Step [77500/138038], Loss: 3.7707, Perplexity: 43.41007\n",
      "Epoch [1/3], Step [77600/138038], Loss: 2.4675, Perplexity: 11.79277\n",
      "Epoch [1/3], Step [77700/138038], Loss: 2.9240, Perplexity: 18.61548\n",
      "Epoch [1/3], Step [77800/138038], Loss: 2.5448, Perplexity: 12.7412\n",
      "Epoch [1/3], Step [77900/138038], Loss: 2.5177, Perplexity: 12.40026\n",
      "Epoch [1/3], Step [78000/138038], Loss: 2.3813, Perplexity: 10.8185\n",
      "Epoch [1/3], Step [78100/138038], Loss: 2.1194, Perplexity: 8.326149\n",
      "Epoch [1/3], Step [78200/138038], Loss: 2.5963, Perplexity: 13.4142\n",
      "Epoch [1/3], Step [78300/138038], Loss: 3.5699, Perplexity: 35.51134\n",
      "Epoch [1/3], Step [78400/138038], Loss: 2.9234, Perplexity: 18.60484\n",
      "Epoch [1/3], Step [78500/138038], Loss: 2.7404, Perplexity: 15.49373\n",
      "Epoch [1/3], Step [78600/138038], Loss: 2.8943, Perplexity: 18.07165\n",
      "Epoch [1/3], Step [78700/138038], Loss: 4.3271, Perplexity: 75.72642\n",
      "Epoch [1/3], Step [78800/138038], Loss: 2.8949, Perplexity: 18.08254\n",
      "Epoch [1/3], Step [78900/138038], Loss: 3.0018, Perplexity: 20.12126\n",
      "Epoch [1/3], Step [79000/138038], Loss: 3.0749, Perplexity: 21.64753\n",
      "Epoch [1/3], Step [79100/138038], Loss: 2.6616, Perplexity: 14.31971\n",
      "Epoch [1/3], Step [79200/138038], Loss: 2.0979, Perplexity: 8.14924\n",
      "Epoch [1/3], Step [79300/138038], Loss: 3.4567, Perplexity: 31.71292\n",
      "Epoch [1/3], Step [79400/138038], Loss: 2.8310, Perplexity: 16.9621\n",
      "Epoch [1/3], Step [79500/138038], Loss: 3.3239, Perplexity: 27.7691\n",
      "Epoch [1/3], Step [79600/138038], Loss: 2.4927, Perplexity: 12.09417\n",
      "Epoch [1/3], Step [79700/138038], Loss: 3.2411, Perplexity: 25.56120\n",
      "Epoch [1/3], Step [79800/138038], Loss: 2.3000, Perplexity: 9.974751\n",
      "Epoch [1/3], Step [79900/138038], Loss: 2.8116, Perplexity: 16.63596\n",
      "Epoch [1/3], Step [80000/138038], Loss: 2.8563, Perplexity: 17.3972\n",
      "Epoch [1/3], Step [80100/138038], Loss: 3.4385, Perplexity: 31.1395\n",
      "Epoch [1/3], Step [80200/138038], Loss: 3.0581, Perplexity: 21.2877\n",
      "Epoch [1/3], Step [80300/138038], Loss: 3.9534, Perplexity: 52.1100\n",
      "Epoch [1/3], Step [80400/138038], Loss: 3.3166, Perplexity: 27.5655\n",
      "Epoch [1/3], Step [80500/138038], Loss: 2.5525, Perplexity: 12.8386\n",
      "Epoch [1/3], Step [80600/138038], Loss: 2.0690, Perplexity: 7.91716\n",
      "Epoch [1/3], Step [80700/138038], Loss: 2.8984, Perplexity: 18.14549\n",
      "Epoch [1/3], Step [80800/138038], Loss: 2.3293, Perplexity: 10.27116\n",
      "Epoch [1/3], Step [80900/138038], Loss: 3.1199, Perplexity: 22.64428\n",
      "Epoch [1/3], Step [81000/138038], Loss: 2.3741, Perplexity: 10.7415\n",
      "Epoch [1/3], Step [81100/138038], Loss: 3.0092, Perplexity: 20.2710\n",
      "Epoch [1/3], Step [81200/138038], Loss: 3.9531, Perplexity: 52.09659\n",
      "Epoch [1/3], Step [81300/138038], Loss: 3.6072, Perplexity: 36.86223\n",
      "Epoch [1/3], Step [81400/138038], Loss: 2.7360, Perplexity: 15.42529\n",
      "Epoch [1/3], Step [81500/138038], Loss: 2.8453, Perplexity: 17.20603\n",
      "Epoch [1/3], Step [81600/138038], Loss: 3.8791, Perplexity: 48.3815\n",
      "Epoch [1/3], Step [81700/138038], Loss: 2.6654, Perplexity: 14.3739\n",
      "Epoch [1/3], Step [81800/138038], Loss: 2.8937, Perplexity: 18.0593\n",
      "Epoch [1/3], Step [81900/138038], Loss: 2.3313, Perplexity: 10.29162\n",
      "Epoch [1/3], Step [82000/138038], Loss: 3.7254, Perplexity: 41.48802\n",
      "Epoch [1/3], Step [82100/138038], Loss: 3.6183, Perplexity: 37.27384\n",
      "Epoch [1/3], Step [82200/138038], Loss: 3.4467, Perplexity: 31.39617\n",
      "Epoch [1/3], Step [82300/138038], Loss: 3.2733, Perplexity: 26.3978\n",
      "Epoch [1/3], Step [82400/138038], Loss: 2.4055, Perplexity: 11.08418\n",
      "Epoch [1/3], Step [82500/138038], Loss: 2.3694, Perplexity: 10.69152\n",
      "Epoch [1/3], Step [82600/138038], Loss: 3.6018, Perplexity: 36.66545\n",
      "Epoch [1/3], Step [82700/138038], Loss: 3.7627, Perplexity: 43.06254\n",
      "Epoch [1/3], Step [82800/138038], Loss: 2.5778, Perplexity: 13.16832\n",
      "Epoch [1/3], Step [82900/138038], Loss: 2.5988, Perplexity: 13.4471\n",
      "Epoch [1/3], Step [83000/138038], Loss: 3.6924, Perplexity: 40.13929\n",
      "Epoch [1/3], Step [83100/138038], Loss: 4.0234, Perplexity: 55.88916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [83200/138038], Loss: 2.5239, Perplexity: 12.4773\n",
      "Epoch [1/3], Step [83300/138038], Loss: 3.4295, Perplexity: 30.86132\n",
      "Epoch [1/3], Step [83400/138038], Loss: 2.7663, Perplexity: 15.89995\n",
      "Epoch [1/3], Step [83500/138038], Loss: 3.9382, Perplexity: 51.32364\n",
      "Epoch [1/3], Step [83600/138038], Loss: 3.5115, Perplexity: 33.4973\n",
      "Epoch [1/3], Step [83700/138038], Loss: 1.9497, Perplexity: 7.026893\n",
      "Epoch [1/3], Step [83800/138038], Loss: 2.4103, Perplexity: 11.13737\n",
      "Epoch [1/3], Step [83900/138038], Loss: 2.3445, Perplexity: 10.42858\n",
      "Epoch [1/3], Step [84000/138038], Loss: 2.2178, Perplexity: 9.187236\n",
      "Epoch [1/3], Step [84100/138038], Loss: 3.3973, Perplexity: 29.8839\n",
      "Epoch [1/3], Step [84200/138038], Loss: 3.8991, Perplexity: 49.3588\n",
      "Epoch [1/3], Step [84300/138038], Loss: 2.8285, Perplexity: 16.9205\n",
      "Epoch [1/3], Step [84400/138038], Loss: 2.3753, Perplexity: 10.75377\n",
      "Epoch [1/3], Step [84500/138038], Loss: 2.2343, Perplexity: 9.34033\n",
      "Epoch [1/3], Step [84600/138038], Loss: 2.6025, Perplexity: 13.4979\n",
      "Epoch [1/3], Step [84700/138038], Loss: 3.3326, Perplexity: 28.01044\n",
      "Epoch [1/3], Step [84800/138038], Loss: 2.3203, Perplexity: 10.17898\n",
      "Epoch [1/3], Step [84900/138038], Loss: 2.1879, Perplexity: 8.91628\n",
      "Epoch [1/3], Step [85000/138038], Loss: 1.8991, Perplexity: 6.679872\n",
      "Epoch [1/3], Step [85100/138038], Loss: 2.3830, Perplexity: 10.8371\n",
      "Epoch [1/3], Step [85200/138038], Loss: 2.1404, Perplexity: 8.50261\n",
      "Epoch [1/3], Step [85300/138038], Loss: 3.1861, Perplexity: 24.19474\n",
      "Epoch [1/3], Step [85400/138038], Loss: 2.9876, Perplexity: 19.83759\n",
      "Epoch [1/3], Step [85500/138038], Loss: 2.0869, Perplexity: 8.05999\n",
      "Epoch [1/3], Step [85600/138038], Loss: 2.6608, Perplexity: 14.30737\n",
      "Epoch [1/3], Step [85700/138038], Loss: 3.4870, Perplexity: 32.68774\n",
      "Epoch [1/3], Step [85800/138038], Loss: 3.9237, Perplexity: 50.58728\n",
      "Epoch [1/3], Step [85900/138038], Loss: 3.4525, Perplexity: 31.57959\n",
      "Epoch [1/3], Step [86000/138038], Loss: 2.3194, Perplexity: 10.1696\n",
      "Epoch [1/3], Step [86100/138038], Loss: 2.7386, Perplexity: 15.46552\n",
      "Epoch [1/3], Step [86200/138038], Loss: 3.7174, Perplexity: 41.15536\n",
      "Epoch [1/3], Step [86300/138038], Loss: 4.8183, Perplexity: 123.7583\n",
      "Epoch [1/3], Step [86400/138038], Loss: 2.7699, Perplexity: 15.9574\n",
      "Epoch [1/3], Step [86500/138038], Loss: 2.3811, Perplexity: 10.81680\n",
      "Epoch [1/3], Step [86600/138038], Loss: 2.9636, Perplexity: 19.36787\n",
      "Epoch [1/3], Step [86700/138038], Loss: 2.7959, Perplexity: 16.37764\n",
      "Epoch [1/3], Step [86800/138038], Loss: 3.4799, Perplexity: 32.45561\n",
      "Epoch [1/3], Step [86900/138038], Loss: 2.8711, Perplexity: 17.6556\n",
      "Epoch [1/3], Step [87000/138038], Loss: 1.9975, Perplexity: 7.37063\n",
      "Epoch [1/3], Step [87100/138038], Loss: 3.4328, Perplexity: 30.9627\n",
      "Epoch [1/3], Step [87200/138038], Loss: 2.8137, Perplexity: 16.67170\n",
      "Epoch [1/3], Step [87300/138038], Loss: 2.6540, Perplexity: 14.21010\n",
      "Epoch [1/3], Step [87400/138038], Loss: 2.8457, Perplexity: 17.21280\n",
      "Epoch [1/3], Step [87500/138038], Loss: 4.4043, Perplexity: 81.8059\n",
      "Epoch [1/3], Step [87600/138038], Loss: 1.9126, Perplexity: 6.77089\n",
      "Epoch [1/3], Step [87700/138038], Loss: 2.5439, Perplexity: 12.7287\n",
      "Epoch [1/3], Step [87800/138038], Loss: 2.8712, Perplexity: 17.6578\n",
      "Epoch [1/3], Step [87900/138038], Loss: 3.1050, Perplexity: 22.30978\n",
      "Epoch [1/3], Step [88000/138038], Loss: 3.1135, Perplexity: 22.49878\n",
      "Epoch [1/3], Step [88100/138038], Loss: 2.6129, Perplexity: 13.6386\n",
      "Epoch [1/3], Step [88200/138038], Loss: 2.6204, Perplexity: 13.7410\n",
      "Epoch [1/3], Step [88300/138038], Loss: 2.5824, Perplexity: 13.22858\n",
      "Epoch [1/3], Step [88400/138038], Loss: 3.2224, Perplexity: 25.08943\n",
      "Epoch [1/3], Step [88500/138038], Loss: 3.0915, Perplexity: 22.0108\n",
      "Epoch [1/3], Step [88600/138038], Loss: 3.6995, Perplexity: 40.42824\n",
      "Epoch [1/3], Step [88700/138038], Loss: 1.7878, Perplexity: 5.976142\n",
      "Epoch [1/3], Step [88800/138038], Loss: 2.0272, Perplexity: 7.592658\n",
      "Epoch [1/3], Step [88900/138038], Loss: 2.6471, Perplexity: 14.1136\n",
      "Epoch [1/3], Step [89000/138038], Loss: 3.4655, Perplexity: 31.99223\n",
      "Epoch [1/3], Step [89100/138038], Loss: 3.7047, Perplexity: 40.63748\n",
      "Epoch [1/3], Step [89200/138038], Loss: 2.8941, Perplexity: 18.06703\n",
      "Epoch [1/3], Step [89300/138038], Loss: 2.2475, Perplexity: 9.46436\n",
      "Epoch [1/3], Step [89400/138038], Loss: 2.6727, Perplexity: 14.4791\n",
      "Epoch [1/3], Step [89500/138038], Loss: 3.8736, Perplexity: 48.1150\n",
      "Epoch [1/3], Step [89600/138038], Loss: 2.5271, Perplexity: 12.5172\n",
      "Epoch [1/3], Step [89700/138038], Loss: 3.4925, Perplexity: 32.86798\n",
      "Epoch [1/3], Step [89800/138038], Loss: 2.9504, Perplexity: 19.11275\n",
      "Epoch [1/3], Step [89900/138038], Loss: 2.6605, Perplexity: 14.30320\n",
      "Epoch [1/3], Step [90000/138038], Loss: 2.7661, Perplexity: 15.89668\n",
      "Epoch [1/3], Step [90100/138038], Loss: 3.3695, Perplexity: 29.06523\n",
      "Epoch [1/3], Step [90200/138038], Loss: 3.7188, Perplexity: 41.21669\n",
      "Epoch [1/3], Step [90300/138038], Loss: 2.8218, Perplexity: 16.80678\n",
      "Epoch [1/3], Step [90400/138038], Loss: 2.3163, Perplexity: 10.13846\n",
      "Epoch [1/3], Step [90500/138038], Loss: 2.8726, Perplexity: 17.6838\n",
      "Epoch [1/3], Step [90600/138038], Loss: 3.7550, Perplexity: 42.73525\n",
      "Epoch [1/3], Step [90700/138038], Loss: 3.0038, Perplexity: 20.1612\n",
      "Epoch [1/3], Step [90800/138038], Loss: 2.4018, Perplexity: 11.0435\n",
      "Epoch [1/3], Step [90900/138038], Loss: 2.0880, Perplexity: 8.068627\n",
      "Epoch [1/3], Step [91000/138038], Loss: 3.1788, Perplexity: 24.0189\n",
      "Epoch [1/3], Step [91100/138038], Loss: 2.1924, Perplexity: 8.956892\n",
      "Epoch [1/3], Step [91200/138038], Loss: 4.5002, Perplexity: 90.0384\n",
      "Epoch [1/3], Step [91300/138038], Loss: 3.2391, Perplexity: 25.5107\n",
      "Epoch [1/3], Step [91400/138038], Loss: 2.7032, Perplexity: 14.92772\n",
      "Epoch [1/3], Step [91500/138038], Loss: 3.5796, Perplexity: 35.85942\n",
      "Epoch [1/3], Step [91600/138038], Loss: 2.0036, Perplexity: 7.41544\n",
      "Epoch [1/3], Step [91700/138038], Loss: 2.7039, Perplexity: 14.9386\n",
      "Epoch [1/3], Step [91800/138038], Loss: 3.1761, Perplexity: 23.95372\n",
      "Epoch [1/3], Step [91900/138038], Loss: 2.8691, Perplexity: 17.62198\n",
      "Epoch [1/3], Step [92000/138038], Loss: 2.4922, Perplexity: 12.0878\n",
      "Epoch [1/3], Step [92100/138038], Loss: 4.9843, Perplexity: 146.1018\n",
      "Epoch [1/3], Step [92200/138038], Loss: 3.5155, Perplexity: 33.63231\n",
      "Epoch [1/3], Step [92300/138038], Loss: 3.1019, Perplexity: 22.2393\n",
      "Epoch [1/3], Step [92400/138038], Loss: 2.6102, Perplexity: 13.60218\n",
      "Epoch [1/3], Step [92500/138038], Loss: 3.6801, Perplexity: 39.65178\n",
      "Epoch [1/3], Step [92600/138038], Loss: 3.3679, Perplexity: 29.0182\n",
      "Epoch [1/3], Step [92700/138038], Loss: 3.1071, Perplexity: 22.35647\n",
      "Epoch [1/3], Step [92800/138038], Loss: 2.9918, Perplexity: 19.9212\n",
      "Epoch [1/3], Step [92900/138038], Loss: 2.7204, Perplexity: 15.18637\n",
      "Epoch [1/3], Step [93000/138038], Loss: 4.2547, Perplexity: 70.43831\n",
      "Epoch [1/3], Step [93100/138038], Loss: 3.1934, Perplexity: 24.3711\n",
      "Epoch [1/3], Step [93200/138038], Loss: 1.5901, Perplexity: 4.90404\n",
      "Epoch [1/3], Step [93300/138038], Loss: 2.9901, Perplexity: 19.88841\n",
      "Epoch [1/3], Step [93400/138038], Loss: 4.4650, Perplexity: 86.9215\n",
      "Epoch [1/3], Step [93500/138038], Loss: 3.1091, Perplexity: 22.40147\n",
      "Epoch [1/3], Step [93600/138038], Loss: 2.0829, Perplexity: 8.02801\n",
      "Epoch [1/3], Step [93700/138038], Loss: 4.1484, Perplexity: 63.33274\n",
      "Epoch [1/3], Step [93800/138038], Loss: 3.2188, Perplexity: 24.99860\n",
      "Epoch [1/3], Step [93900/138038], Loss: 3.8240, Perplexity: 45.7850\n",
      "Epoch [1/3], Step [94000/138038], Loss: 2.1529, Perplexity: 8.61010\n",
      "Epoch [1/3], Step [94100/138038], Loss: 3.6160, Perplexity: 37.1868\n",
      "Epoch [1/3], Step [94200/138038], Loss: 1.8966, Perplexity: 6.663389\n",
      "Epoch [1/3], Step [94300/138038], Loss: 3.0361, Perplexity: 20.8241\n",
      "Epoch [1/3], Step [94400/138038], Loss: 2.6019, Perplexity: 13.48973\n",
      "Epoch [1/3], Step [94500/138038], Loss: 2.7635, Perplexity: 15.8559\n",
      "Epoch [1/3], Step [94600/138038], Loss: 3.7203, Perplexity: 41.2756\n",
      "Epoch [1/3], Step [94700/138038], Loss: 2.2388, Perplexity: 9.382240\n",
      "Epoch [1/3], Step [94800/138038], Loss: 3.3663, Perplexity: 28.97108\n",
      "Epoch [1/3], Step [94900/138038], Loss: 2.2182, Perplexity: 9.19073\n",
      "Epoch [1/3], Step [95000/138038], Loss: 4.0735, Perplexity: 58.76331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [95100/138038], Loss: 2.7854, Perplexity: 16.2057\n",
      "Epoch [1/3], Step [95200/138038], Loss: 2.1455, Perplexity: 8.54644\n",
      "Epoch [1/3], Step [95300/138038], Loss: 4.0678, Perplexity: 58.4287\n",
      "Epoch [1/3], Step [95400/138038], Loss: 3.1456, Perplexity: 23.23315\n",
      "Epoch [1/3], Step [95500/138038], Loss: 3.0752, Perplexity: 21.6538\n",
      "Epoch [1/3], Step [95600/138038], Loss: 3.0064, Perplexity: 20.2152\n",
      "Epoch [1/3], Step [95700/138038], Loss: 3.0085, Perplexity: 20.2565\n",
      "Epoch [1/3], Step [95800/138038], Loss: 2.5559, Perplexity: 12.88344\n",
      "Epoch [1/3], Step [95900/138038], Loss: 3.3458, Perplexity: 28.3838\n",
      "Epoch [1/3], Step [96000/138038], Loss: 4.2828, Perplexity: 72.44658\n",
      "Epoch [1/3], Step [96100/138038], Loss: 3.3388, Perplexity: 28.18631\n",
      "Epoch [1/3], Step [96200/138038], Loss: 3.3909, Perplexity: 29.69380\n",
      "Epoch [1/3], Step [96300/138038], Loss: 2.6639, Perplexity: 14.35268\n",
      "Epoch [1/3], Step [96400/138038], Loss: 2.8335, Perplexity: 17.00442\n",
      "Epoch [1/3], Step [96500/138038], Loss: 3.4995, Perplexity: 33.09771\n",
      "Epoch [1/3], Step [96600/138038], Loss: 4.0378, Perplexity: 56.70250\n",
      "Epoch [1/3], Step [96700/138038], Loss: 2.0323, Perplexity: 7.63150\n",
      "Epoch [1/3], Step [96800/138038], Loss: 5.1603, Perplexity: 174.2158\n",
      "Epoch [1/3], Step [96900/138038], Loss: 3.3808, Perplexity: 29.39454\n",
      "Epoch [1/3], Step [97000/138038], Loss: 3.9285, Perplexity: 50.83208\n",
      "Epoch [1/3], Step [97100/138038], Loss: 2.4697, Perplexity: 11.81863\n",
      "Epoch [1/3], Step [97200/138038], Loss: 2.4414, Perplexity: 11.48950\n",
      "Epoch [1/3], Step [97300/138038], Loss: 2.8511, Perplexity: 17.3066\n",
      "Epoch [1/3], Step [97400/138038], Loss: 2.8824, Perplexity: 17.85625\n",
      "Epoch [1/3], Step [97500/138038], Loss: 3.2915, Perplexity: 26.88426\n",
      "Epoch [1/3], Step [97600/138038], Loss: 3.0780, Perplexity: 21.71449\n",
      "Epoch [1/3], Step [97700/138038], Loss: 3.2682, Perplexity: 26.2646\n",
      "Epoch [1/3], Step [97800/138038], Loss: 2.3096, Perplexity: 10.07049\n",
      "Epoch [1/3], Step [97900/138038], Loss: 2.7709, Perplexity: 15.9734\n",
      "Epoch [1/3], Step [98000/138038], Loss: 1.8841, Perplexity: 6.580583\n",
      "Epoch [1/3], Step [98100/138038], Loss: 3.7201, Perplexity: 41.2671\n",
      "Epoch [1/3], Step [98200/138038], Loss: 4.2193, Perplexity: 67.98414\n",
      "Epoch [1/3], Step [98300/138038], Loss: 2.0616, Perplexity: 7.858593\n",
      "Epoch [1/3], Step [98400/138038], Loss: 2.7850, Perplexity: 16.19999\n",
      "Epoch [1/3], Step [98500/138038], Loss: 3.5046, Perplexity: 33.2666\n",
      "Epoch [1/3], Step [98600/138038], Loss: 3.9055, Perplexity: 49.67666\n",
      "Epoch [1/3], Step [98700/138038], Loss: 3.5954, Perplexity: 36.43002\n",
      "Epoch [1/3], Step [98800/138038], Loss: 3.6168, Perplexity: 37.21793\n",
      "Epoch [1/3], Step [98900/138038], Loss: 2.4335, Perplexity: 11.3992\n",
      "Epoch [1/3], Step [99000/138038], Loss: 2.2768, Perplexity: 9.745868\n",
      "Epoch [1/3], Step [99100/138038], Loss: 2.5519, Perplexity: 12.8310\n",
      "Epoch [1/3], Step [99200/138038], Loss: 3.5049, Perplexity: 33.27932\n",
      "Epoch [1/3], Step [99300/138038], Loss: 2.5317, Perplexity: 12.5752\n",
      "Epoch [1/3], Step [99400/138038], Loss: 2.9072, Perplexity: 18.3047\n",
      "Epoch [1/3], Step [99500/138038], Loss: 3.2207, Perplexity: 25.04685\n",
      "Epoch [1/3], Step [99600/138038], Loss: 2.7397, Perplexity: 15.4831\n",
      "Epoch [1/3], Step [99700/138038], Loss: 3.1510, Perplexity: 23.3603\n",
      "Epoch [1/3], Step [99800/138038], Loss: 2.9263, Perplexity: 18.6588\n",
      "Epoch [1/3], Step [99900/138038], Loss: 2.0604, Perplexity: 7.848741\n",
      "Epoch [1/3], Step [100000/138038], Loss: 3.0776, Perplexity: 21.7052\n",
      "Epoch [1/3], Step [100100/138038], Loss: 3.2020, Perplexity: 24.5820\n",
      "Epoch [1/3], Step [100200/138038], Loss: 2.6029, Perplexity: 13.50303\n",
      "Epoch [1/3], Step [100300/138038], Loss: 2.7161, Perplexity: 15.12086\n",
      "Epoch [1/3], Step [100400/138038], Loss: 3.1811, Perplexity: 24.07368\n",
      "Epoch [1/3], Step [100500/138038], Loss: 2.7646, Perplexity: 15.87357\n",
      "Epoch [1/3], Step [100600/138038], Loss: 2.8938, Perplexity: 18.06147\n",
      "Epoch [1/3], Step [100700/138038], Loss: 2.6729, Perplexity: 14.48145\n",
      "Epoch [1/3], Step [100800/138038], Loss: 3.3816, Perplexity: 29.41926\n",
      "Epoch [1/3], Step [100900/138038], Loss: 3.3709, Perplexity: 29.1043\n",
      "Epoch [1/3], Step [101000/138038], Loss: 2.9535, Perplexity: 19.1736\n",
      "Epoch [1/3], Step [101100/138038], Loss: 3.5749, Perplexity: 35.68978\n",
      "Epoch [1/3], Step [101200/138038], Loss: 2.3325, Perplexity: 10.30379\n",
      "Epoch [1/3], Step [101300/138038], Loss: 2.9543, Perplexity: 19.1881\n",
      "Epoch [1/3], Step [101400/138038], Loss: 3.0670, Perplexity: 21.4771\n",
      "Epoch [1/3], Step [101500/138038], Loss: 3.1795, Perplexity: 24.03574\n",
      "Epoch [1/3], Step [101600/138038], Loss: 3.1277, Perplexity: 22.82184\n",
      "Epoch [1/3], Step [101700/138038], Loss: 3.1174, Perplexity: 22.58656\n",
      "Epoch [1/3], Step [101800/138038], Loss: 3.4189, Perplexity: 30.53715\n",
      "Epoch [1/3], Step [101900/138038], Loss: 3.0920, Perplexity: 22.02190\n",
      "Epoch [1/3], Step [102000/138038], Loss: 2.7569, Perplexity: 15.75169\n",
      "Epoch [1/3], Step [102100/138038], Loss: 2.9066, Perplexity: 18.29519\n",
      "Epoch [1/3], Step [102200/138038], Loss: 2.8844, Perplexity: 17.8922\n",
      "Epoch [1/3], Step [102300/138038], Loss: 3.6657, Perplexity: 39.08370\n",
      "Epoch [1/3], Step [102400/138038], Loss: 3.2647, Perplexity: 26.1719\n",
      "Epoch [1/3], Step [102500/138038], Loss: 3.5822, Perplexity: 35.9529\n",
      "Epoch [1/3], Step [102600/138038], Loss: 4.5758, Perplexity: 97.10220\n",
      "Epoch [1/3], Step [102700/138038], Loss: 2.8120, Perplexity: 16.6439\n",
      "Epoch [1/3], Step [102800/138038], Loss: 2.9746, Perplexity: 19.58231\n",
      "Epoch [1/3], Step [102900/138038], Loss: 2.4071, Perplexity: 11.10121\n",
      "Epoch [1/3], Step [103000/138038], Loss: 4.1090, Perplexity: 60.88474\n",
      "Epoch [1/3], Step [103100/138038], Loss: 1.5791, Perplexity: 4.85067\n",
      "Epoch [1/3], Step [103200/138038], Loss: 2.3562, Perplexity: 10.55122\n",
      "Epoch [1/3], Step [103300/138038], Loss: 2.3836, Perplexity: 10.8437\n",
      "Epoch [1/3], Step [103400/138038], Loss: 3.5467, Perplexity: 34.6982\n",
      "Epoch [1/3], Step [103500/138038], Loss: 3.8787, Perplexity: 48.36178\n",
      "Epoch [1/3], Step [103600/138038], Loss: 3.6065, Perplexity: 36.83619\n",
      "Epoch [1/3], Step [103700/138038], Loss: 1.5967, Perplexity: 4.936743\n",
      "Epoch [1/3], Step [103800/138038], Loss: 3.2319, Perplexity: 25.3286\n",
      "Epoch [1/3], Step [103900/138038], Loss: 3.0167, Perplexity: 20.42389\n",
      "Epoch [1/3], Step [104000/138038], Loss: 2.1039, Perplexity: 8.197856\n",
      "Epoch [1/3], Step [104100/138038], Loss: 3.2880, Perplexity: 26.7884\n",
      "Epoch [1/3], Step [104200/138038], Loss: 2.1504, Perplexity: 8.588176\n",
      "Epoch [1/3], Step [104300/138038], Loss: 4.4762, Perplexity: 87.89950\n",
      "Epoch [1/3], Step [104400/138038], Loss: 2.8020, Perplexity: 16.4769\n",
      "Epoch [1/3], Step [104500/138038], Loss: 3.7957, Perplexity: 44.50831\n",
      "Epoch [1/3], Step [104600/138038], Loss: 2.7913, Perplexity: 16.30256\n",
      "Epoch [1/3], Step [104700/138038], Loss: 3.5815, Perplexity: 35.92590\n",
      "Epoch [1/3], Step [104800/138038], Loss: 3.4817, Perplexity: 32.5140\n",
      "Epoch [1/3], Step [104900/138038], Loss: 2.2892, Perplexity: 9.86677\n",
      "Epoch [1/3], Step [105000/138038], Loss: 5.3054, Perplexity: 201.4289\n",
      "Epoch [1/3], Step [105100/138038], Loss: 3.8511, Perplexity: 47.04440\n",
      "Epoch [1/3], Step [105200/138038], Loss: 3.7719, Perplexity: 43.46140\n",
      "Epoch [1/3], Step [105300/138038], Loss: 2.5284, Perplexity: 12.5337\n",
      "Epoch [1/3], Step [105400/138038], Loss: 2.8537, Perplexity: 17.35184\n",
      "Epoch [1/3], Step [105500/138038], Loss: 3.0946, Perplexity: 22.0791\n",
      "Epoch [1/3], Step [105600/138038], Loss: 3.6618, Perplexity: 38.9330\n",
      "Epoch [1/3], Step [105700/138038], Loss: 3.1630, Perplexity: 23.64141\n",
      "Epoch [1/3], Step [105800/138038], Loss: 2.8227, Perplexity: 16.82261\n",
      "Epoch [1/3], Step [105900/138038], Loss: 2.0435, Perplexity: 7.717712\n",
      "Epoch [1/3], Step [106000/138038], Loss: 2.5482, Perplexity: 12.7838\n",
      "Epoch [1/3], Step [106100/138038], Loss: 1.8717, Perplexity: 6.49918\n",
      "Epoch [1/3], Step [106200/138038], Loss: 2.9324, Perplexity: 18.77236\n",
      "Epoch [1/3], Step [106300/138038], Loss: 2.6258, Perplexity: 13.8150\n",
      "Epoch [1/3], Step [106400/138038], Loss: 2.9138, Perplexity: 18.42691\n",
      "Epoch [1/3], Step [106500/138038], Loss: 3.0275, Perplexity: 20.6464\n",
      "Epoch [1/3], Step [106600/138038], Loss: 2.5088, Perplexity: 12.2903\n",
      "Epoch [1/3], Step [106700/138038], Loss: 3.5783, Perplexity: 35.81169\n",
      "Epoch [1/3], Step [106800/138038], Loss: 3.7244, Perplexity: 41.44721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [106900/138038], Loss: 2.0406, Perplexity: 7.69545\n",
      "Epoch [1/3], Step [107000/138038], Loss: 3.3565, Perplexity: 28.69005\n",
      "Epoch [1/3], Step [107100/138038], Loss: 2.8187, Perplexity: 16.7546\n",
      "Epoch [1/3], Step [107200/138038], Loss: 2.2195, Perplexity: 9.202889\n",
      "Epoch [1/3], Step [107300/138038], Loss: 5.4174, Perplexity: 225.2998\n",
      "Epoch [1/3], Step [107400/138038], Loss: 3.5884, Perplexity: 36.1764\n",
      "Epoch [1/3], Step [107500/138038], Loss: 2.6924, Perplexity: 14.76679\n",
      "Epoch [1/3], Step [107600/138038], Loss: 3.2419, Perplexity: 25.5817\n",
      "Epoch [1/3], Step [107700/138038], Loss: 2.4961, Perplexity: 12.13521\n",
      "Epoch [1/3], Step [107800/138038], Loss: 1.6575, Perplexity: 5.24610\n",
      "Epoch [1/3], Step [107900/138038], Loss: 3.1528, Perplexity: 23.40152\n",
      "Epoch [1/3], Step [108000/138038], Loss: 3.3610, Perplexity: 28.81903\n",
      "Epoch [1/3], Step [108100/138038], Loss: 3.4402, Perplexity: 31.1930\n",
      "Epoch [1/3], Step [108200/138038], Loss: 1.9766, Perplexity: 7.21847\n",
      "Epoch [1/3], Step [108300/138038], Loss: 2.1242, Perplexity: 8.36655\n",
      "Epoch [1/3], Step [108400/138038], Loss: 1.9845, Perplexity: 7.275422\n",
      "Epoch [1/3], Step [108500/138038], Loss: 4.8275, Perplexity: 124.8976\n",
      "Epoch [1/3], Step [108600/138038], Loss: 3.0412, Perplexity: 20.92972\n",
      "Epoch [1/3], Step [108700/138038], Loss: 5.1511, Perplexity: 172.6238\n",
      "Epoch [1/3], Step [108800/138038], Loss: 2.7241, Perplexity: 15.2427\n",
      "Epoch [1/3], Step [108900/138038], Loss: 3.7861, Perplexity: 44.08302\n",
      "Epoch [1/3], Step [109000/138038], Loss: 2.7501, Perplexity: 15.64404\n",
      "Epoch [1/3], Step [109100/138038], Loss: 3.2400, Perplexity: 25.5330\n",
      "Epoch [1/3], Step [109200/138038], Loss: 1.9694, Perplexity: 7.16637\n",
      "Epoch [1/3], Step [109300/138038], Loss: 3.2798, Perplexity: 26.56944\n",
      "Epoch [1/3], Step [109400/138038], Loss: 2.7738, Perplexity: 16.02013\n",
      "Epoch [1/3], Step [109500/138038], Loss: 2.5248, Perplexity: 12.4883\n",
      "Epoch [1/3], Step [109600/138038], Loss: 2.7250, Perplexity: 15.25626\n",
      "Epoch [1/3], Step [109700/138038], Loss: 2.5929, Perplexity: 13.3686\n",
      "Epoch [1/3], Step [109800/138038], Loss: 2.4709, Perplexity: 11.83342\n",
      "Epoch [1/3], Step [109900/138038], Loss: 1.5864, Perplexity: 4.88602\n",
      "Epoch [1/3], Step [110000/138038], Loss: 3.0633, Perplexity: 21.3971\n",
      "Epoch [1/3], Step [110100/138038], Loss: 3.8855, Perplexity: 48.68934\n",
      "Epoch [1/3], Step [110200/138038], Loss: 2.5037, Perplexity: 12.2276\n",
      "Epoch [1/3], Step [110300/138038], Loss: 2.2998, Perplexity: 9.971850\n",
      "Epoch [1/3], Step [110400/138038], Loss: 1.8633, Perplexity: 6.444951\n",
      "Epoch [1/3], Step [110500/138038], Loss: 2.9476, Perplexity: 19.06042\n",
      "Epoch [1/3], Step [110600/138038], Loss: 2.6444, Perplexity: 14.07446\n",
      "Epoch [1/3], Step [110700/138038], Loss: 3.4005, Perplexity: 29.97791\n",
      "Epoch [1/3], Step [110800/138038], Loss: 1.6518, Perplexity: 5.21633\n",
      "Epoch [1/3], Step [110900/138038], Loss: 2.7850, Perplexity: 16.19946\n",
      "Epoch [1/3], Step [111000/138038], Loss: 2.5454, Perplexity: 12.74897\n",
      "Epoch [1/3], Step [111100/138038], Loss: 3.7058, Perplexity: 40.6808\n",
      "Epoch [1/3], Step [111200/138038], Loss: 3.3776, Perplexity: 29.3016\n",
      "Epoch [1/3], Step [111300/138038], Loss: 1.6216, Perplexity: 5.06135\n",
      "Epoch [1/3], Step [111400/138038], Loss: 2.4271, Perplexity: 11.32598\n",
      "Epoch [1/3], Step [111500/138038], Loss: 3.1255, Perplexity: 22.77028\n",
      "Epoch [1/3], Step [111600/138038], Loss: 1.9435, Perplexity: 6.983281\n",
      "Epoch [1/3], Step [111700/138038], Loss: 2.9479, Perplexity: 19.06543\n",
      "Epoch [1/3], Step [111800/138038], Loss: 3.3282, Perplexity: 27.8870\n",
      "Epoch [1/3], Step [111900/138038], Loss: 3.0218, Perplexity: 20.5279\n",
      "Epoch [1/3], Step [112000/138038], Loss: 2.7344, Perplexity: 15.4002\n",
      "Epoch [1/3], Step [112100/138038], Loss: 2.5020, Perplexity: 12.20741\n",
      "Epoch [1/3], Step [112200/138038], Loss: 3.1658, Perplexity: 23.7068\n",
      "Epoch [1/3], Step [112300/138038], Loss: 3.2575, Perplexity: 25.98399\n",
      "Epoch [1/3], Step [112400/138038], Loss: 2.6563, Perplexity: 14.2434\n",
      "Epoch [1/3], Step [112500/138038], Loss: 2.2945, Perplexity: 9.91940\n",
      "Epoch [1/3], Step [112600/138038], Loss: 2.9602, Perplexity: 19.3020\n",
      "Epoch [1/3], Step [112700/138038], Loss: 2.6281, Perplexity: 13.84791\n",
      "Epoch [1/3], Step [112800/138038], Loss: 2.1999, Perplexity: 9.024571\n",
      "Epoch [1/3], Step [112900/138038], Loss: 2.9756, Perplexity: 19.6007\n",
      "Epoch [1/3], Step [113000/138038], Loss: 2.8048, Perplexity: 16.5236\n",
      "Epoch [1/3], Step [113100/138038], Loss: 3.0458, Perplexity: 21.0265\n",
      "Epoch [1/3], Step [113200/138038], Loss: 2.4197, Perplexity: 11.2420\n",
      "Epoch [1/3], Step [113300/138038], Loss: 2.1850, Perplexity: 8.89051\n",
      "Epoch [1/3], Step [113400/138038], Loss: 2.5672, Perplexity: 13.0290\n",
      "Epoch [1/3], Step [113500/138038], Loss: 2.5538, Perplexity: 12.8556\n",
      "Epoch [1/3], Step [113600/138038], Loss: 2.9848, Perplexity: 19.7833\n",
      "Epoch [1/3], Step [113700/138038], Loss: 2.8609, Perplexity: 17.4775\n",
      "Epoch [1/3], Step [113800/138038], Loss: 2.0789, Perplexity: 7.995765\n",
      "Epoch [1/3], Step [113900/138038], Loss: 3.2796, Perplexity: 26.5641\n",
      "Epoch [1/3], Step [114000/138038], Loss: 2.1742, Perplexity: 8.795687\n",
      "Epoch [1/3], Step [114100/138038], Loss: 3.3916, Perplexity: 29.71404\n",
      "Epoch [1/3], Step [114200/138038], Loss: 3.1757, Perplexity: 23.9441\n",
      "Epoch [1/3], Step [114300/138038], Loss: 2.1108, Perplexity: 8.25461\n",
      "Epoch [1/3], Step [114400/138038], Loss: 3.4030, Perplexity: 30.05417\n",
      "Epoch [1/3], Step [114500/138038], Loss: 3.2707, Perplexity: 26.3308\n",
      "Epoch [1/3], Step [114600/138038], Loss: 1.9221, Perplexity: 6.835696\n",
      "Epoch [1/3], Step [114700/138038], Loss: 3.3471, Perplexity: 28.41941\n",
      "Epoch [1/3], Step [114800/138038], Loss: 2.5323, Perplexity: 12.58235\n",
      "Epoch [1/3], Step [114900/138038], Loss: 2.7075, Perplexity: 14.9918\n",
      "Epoch [1/3], Step [115000/138038], Loss: 2.8555, Perplexity: 17.3838\n",
      "Epoch [1/3], Step [115100/138038], Loss: 2.3693, Perplexity: 10.69038\n",
      "Epoch [1/3], Step [115200/138038], Loss: 2.1203, Perplexity: 8.333723\n",
      "Epoch [1/3], Step [115300/138038], Loss: 3.9171, Perplexity: 50.25420\n",
      "Epoch [1/3], Step [115400/138038], Loss: 2.6654, Perplexity: 14.3733\n",
      "Epoch [1/3], Step [115500/138038], Loss: 2.6723, Perplexity: 14.47392\n",
      "Epoch [1/3], Step [115600/138038], Loss: 2.8896, Perplexity: 17.9853\n",
      "Epoch [1/3], Step [115700/138038], Loss: 2.2999, Perplexity: 9.97346\n",
      "Epoch [1/3], Step [115800/138038], Loss: 3.5119, Perplexity: 33.5110\n",
      "Epoch [1/3], Step [115900/138038], Loss: 3.2009, Perplexity: 24.55399\n",
      "Epoch [1/3], Step [116000/138038], Loss: 2.4851, Perplexity: 12.00232\n",
      "Epoch [1/3], Step [116100/138038], Loss: 3.4513, Perplexity: 31.54090\n",
      "Epoch [1/3], Step [116200/138038], Loss: 2.7678, Perplexity: 15.92302\n",
      "Epoch [1/3], Step [116300/138038], Loss: 2.7405, Perplexity: 15.4941\n",
      "Epoch [1/3], Step [116400/138038], Loss: 2.5142, Perplexity: 12.35689\n",
      "Epoch [1/3], Step [116500/138038], Loss: 2.8196, Perplexity: 16.77025\n",
      "Epoch [1/3], Step [116600/138038], Loss: 3.2636, Perplexity: 26.14470\n",
      "Epoch [1/3], Step [116700/138038], Loss: 3.5795, Perplexity: 35.8574\n",
      "Epoch [1/3], Step [116800/138038], Loss: 3.4424, Perplexity: 31.2621\n",
      "Epoch [1/3], Step [116900/138038], Loss: 2.5692, Perplexity: 13.0559\n",
      "Epoch [1/3], Step [117000/138038], Loss: 2.7974, Perplexity: 16.4015\n",
      "Epoch [1/3], Step [117100/138038], Loss: 2.8703, Perplexity: 17.64222\n",
      "Epoch [1/3], Step [117200/138038], Loss: 1.8467, Perplexity: 6.338682\n",
      "Epoch [1/3], Step [117300/138038], Loss: 3.1967, Perplexity: 24.4505\n",
      "Epoch [1/3], Step [117400/138038], Loss: 2.2219, Perplexity: 9.22467\n",
      "Epoch [1/3], Step [117500/138038], Loss: 2.2094, Perplexity: 9.110563\n",
      "Epoch [1/3], Step [117600/138038], Loss: 1.8771, Perplexity: 6.53449\n",
      "Epoch [1/3], Step [117700/138038], Loss: 3.6024, Perplexity: 36.6868\n",
      "Epoch [1/3], Step [117800/138038], Loss: 3.1014, Perplexity: 22.2295\n",
      "Epoch [1/3], Step [117900/138038], Loss: 1.8372, Perplexity: 6.27920\n",
      "Epoch [1/3], Step [118000/138038], Loss: 2.1260, Perplexity: 8.38158\n",
      "Epoch [1/3], Step [118100/138038], Loss: 3.1318, Perplexity: 22.9161\n",
      "Epoch [1/3], Step [118200/138038], Loss: 3.1410, Perplexity: 23.1272\n",
      "Epoch [1/3], Step [118300/138038], Loss: 3.8096, Perplexity: 45.13083\n",
      "Epoch [1/3], Step [118400/138038], Loss: 3.3596, Perplexity: 28.77679\n",
      "Epoch [1/3], Step [118500/138038], Loss: 1.8681, Perplexity: 6.476145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [118600/138038], Loss: 2.8330, Perplexity: 16.99675\n",
      "Epoch [1/3], Step [118700/138038], Loss: 3.4954, Perplexity: 32.96263\n",
      "Epoch [1/3], Step [118800/138038], Loss: 3.4174, Perplexity: 30.48928\n",
      "Epoch [1/3], Step [118900/138038], Loss: 2.8034, Perplexity: 16.5009\n",
      "Epoch [1/3], Step [119000/138038], Loss: 2.6590, Perplexity: 14.28181\n",
      "Epoch [1/3], Step [119100/138038], Loss: 2.6279, Perplexity: 13.8453\n",
      "Epoch [1/3], Step [119200/138038], Loss: 3.6638, Perplexity: 39.0080\n",
      "Epoch [1/3], Step [119300/138038], Loss: 2.6813, Perplexity: 14.6033\n",
      "Epoch [1/3], Step [119400/138038], Loss: 2.4453, Perplexity: 11.53366\n",
      "Epoch [1/3], Step [119500/138038], Loss: 1.9819, Perplexity: 7.256816\n",
      "Epoch [1/3], Step [119600/138038], Loss: 3.4162, Perplexity: 30.4538\n",
      "Epoch [1/3], Step [119700/138038], Loss: 2.2672, Perplexity: 9.65253\n",
      "Epoch [1/3], Step [119800/138038], Loss: 2.7053, Perplexity: 14.95898\n",
      "Epoch [1/3], Step [119900/138038], Loss: 3.0473, Perplexity: 21.05955\n",
      "Epoch [1/3], Step [120000/138038], Loss: 2.3881, Perplexity: 10.8927\n",
      "Epoch [1/3], Step [120100/138038], Loss: 2.2366, Perplexity: 9.361825\n",
      "Epoch [1/3], Step [120200/138038], Loss: 2.0401, Perplexity: 7.691591\n",
      "Epoch [1/3], Step [120300/138038], Loss: 3.8072, Perplexity: 45.0230\n",
      "Epoch [1/3], Step [120400/138038], Loss: 2.4707, Perplexity: 11.83112\n",
      "Epoch [1/3], Step [120500/138038], Loss: 2.0967, Perplexity: 8.13940\n",
      "Epoch [1/3], Step [120600/138038], Loss: 1.9072, Perplexity: 6.73449\n",
      "Epoch [1/3], Step [120700/138038], Loss: 2.9944, Perplexity: 19.9736\n",
      "Epoch [1/3], Step [120800/138038], Loss: 2.0000, Perplexity: 7.389278\n",
      "Epoch [1/3], Step [120900/138038], Loss: 3.5504, Perplexity: 34.82775\n",
      "Epoch [1/3], Step [121000/138038], Loss: 2.3837, Perplexity: 10.8449\n",
      "Epoch [1/3], Step [121100/138038], Loss: 3.0258, Perplexity: 20.61083\n",
      "Epoch [1/3], Step [121200/138038], Loss: 3.4270, Perplexity: 30.7852\n",
      "Epoch [1/3], Step [121300/138038], Loss: 3.3527, Perplexity: 28.5793\n",
      "Epoch [1/3], Step [121400/138038], Loss: 2.2978, Perplexity: 9.95221\n",
      "Epoch [1/3], Step [121500/138038], Loss: 1.9659, Perplexity: 7.141408\n",
      "Epoch [1/3], Step [121600/138038], Loss: 3.4447, Perplexity: 31.3340\n",
      "Epoch [1/3], Step [121700/138038], Loss: 3.9825, Perplexity: 53.64845\n",
      "Epoch [1/3], Step [121800/138038], Loss: 2.5763, Perplexity: 13.1478\n",
      "Epoch [1/3], Step [121900/138038], Loss: 3.6399, Perplexity: 38.0898\n",
      "Epoch [1/3], Step [122000/138038], Loss: 3.1111, Perplexity: 22.4464\n",
      "Epoch [1/3], Step [122100/138038], Loss: 2.2924, Perplexity: 9.898880\n",
      "Epoch [1/3], Step [122200/138038], Loss: 2.2897, Perplexity: 9.87195\n",
      "Epoch [1/3], Step [122300/138038], Loss: 3.4917, Perplexity: 32.8432\n",
      "Epoch [1/3], Step [122400/138038], Loss: 3.0825, Perplexity: 21.8131\n",
      "Epoch [1/3], Step [122500/138038], Loss: 2.3332, Perplexity: 10.31052\n",
      "Epoch [1/3], Step [122600/138038], Loss: 3.2080, Perplexity: 24.7296\n",
      "Epoch [1/3], Step [122700/138038], Loss: 1.8155, Perplexity: 6.14431\n",
      "Epoch [1/3], Step [122800/138038], Loss: 2.7121, Perplexity: 15.0601\n",
      "Epoch [1/3], Step [122900/138038], Loss: 2.4834, Perplexity: 11.98160\n",
      "Epoch [1/3], Step [123000/138038], Loss: 2.2405, Perplexity: 9.39768\n",
      "Epoch [1/3], Step [123100/138038], Loss: 2.7918, Perplexity: 16.31033\n",
      "Epoch [1/3], Step [123200/138038], Loss: 2.6786, Perplexity: 14.56411\n",
      "Epoch [1/3], Step [123300/138038], Loss: 3.5283, Perplexity: 34.0666\n",
      "Epoch [1/3], Step [123400/138038], Loss: 3.3767, Perplexity: 29.2741\n",
      "Epoch [1/3], Step [123500/138038], Loss: 1.7002, Perplexity: 5.47538\n",
      "Epoch [1/3], Step [123600/138038], Loss: 2.4266, Perplexity: 11.3202\n",
      "Epoch [1/3], Step [123700/138038], Loss: 2.5526, Perplexity: 12.8408\n",
      "Epoch [1/3], Step [123800/138038], Loss: 2.3912, Perplexity: 10.92679\n",
      "Epoch [1/3], Step [123900/138038], Loss: 2.9690, Perplexity: 19.4715\n",
      "Epoch [1/3], Step [124000/138038], Loss: 2.0583, Perplexity: 7.832915\n",
      "Epoch [1/3], Step [124100/138038], Loss: 3.2988, Perplexity: 27.08104\n",
      "Epoch [1/3], Step [124200/138038], Loss: 3.7258, Perplexity: 41.5064\n",
      "Epoch [1/3], Step [124300/138038], Loss: 3.9928, Perplexity: 54.20581\n",
      "Epoch [1/3], Step [124400/138038], Loss: 2.4759, Perplexity: 11.8921\n",
      "Epoch [1/3], Step [124500/138038], Loss: 2.2565, Perplexity: 9.54961\n",
      "Epoch [1/3], Step [124600/138038], Loss: 2.4271, Perplexity: 11.32559\n",
      "Epoch [1/3], Step [124700/138038], Loss: 3.2832, Perplexity: 26.66006\n",
      "Epoch [1/3], Step [124800/138038], Loss: 2.7201, Perplexity: 15.18169\n",
      "Epoch [1/3], Step [124900/138038], Loss: 2.5391, Perplexity: 12.6687\n",
      "Epoch [1/3], Step [125000/138038], Loss: 2.6215, Perplexity: 13.7564\n",
      "Epoch [1/3], Step [125100/138038], Loss: 3.0267, Perplexity: 20.6285\n",
      "Epoch [1/3], Step [125200/138038], Loss: 3.3998, Perplexity: 29.95939\n",
      "Epoch [1/3], Step [125300/138038], Loss: 1.9989, Perplexity: 7.380891\n",
      "Epoch [1/3], Step [125400/138038], Loss: 3.2924, Perplexity: 26.90711\n",
      "Epoch [1/3], Step [125500/138038], Loss: 3.9008, Perplexity: 49.4395\n",
      "Epoch [1/3], Step [125600/138038], Loss: 1.1596, Perplexity: 3.18889\n",
      "Epoch [1/3], Step [125700/138038], Loss: 2.9857, Perplexity: 19.8007\n",
      "Epoch [1/3], Step [125800/138038], Loss: 2.7363, Perplexity: 15.42987\n",
      "Epoch [1/3], Step [125900/138038], Loss: 2.5662, Perplexity: 13.01593\n",
      "Epoch [1/3], Step [126000/138038], Loss: 2.3392, Perplexity: 10.3729\n",
      "Epoch [1/3], Step [126100/138038], Loss: 2.7729, Perplexity: 16.0055\n",
      "Epoch [1/3], Step [126200/138038], Loss: 2.3465, Perplexity: 10.44930\n",
      "Epoch [1/3], Step [126300/138038], Loss: 2.8185, Perplexity: 16.75152\n",
      "Epoch [1/3], Step [126400/138038], Loss: 2.2635, Perplexity: 9.61674\n",
      "Epoch [1/3], Step [126500/138038], Loss: 2.5576, Perplexity: 12.90535\n",
      "Epoch [1/3], Step [126600/138038], Loss: 3.2786, Perplexity: 26.5396\n",
      "Epoch [1/3], Step [126700/138038], Loss: 3.5528, Perplexity: 34.90933\n",
      "Epoch [1/3], Step [126800/138038], Loss: 2.2626, Perplexity: 9.608321\n",
      "Epoch [1/3], Step [126900/138038], Loss: 2.6232, Perplexity: 13.78024\n",
      "Epoch [1/3], Step [127000/138038], Loss: 2.6917, Perplexity: 14.75658\n",
      "Epoch [1/3], Step [127100/138038], Loss: 2.1954, Perplexity: 8.98400\n",
      "Epoch [1/3], Step [127200/138038], Loss: 3.5075, Perplexity: 33.3656\n",
      "Epoch [1/3], Step [127300/138038], Loss: 3.5139, Perplexity: 33.5793\n",
      "Epoch [1/3], Step [127400/138038], Loss: 3.1190, Perplexity: 22.62343\n",
      "Epoch [1/3], Step [127500/138038], Loss: 4.5572, Perplexity: 95.3156\n",
      "Epoch [1/3], Step [127600/138038], Loss: 2.5105, Perplexity: 12.31162\n",
      "Epoch [1/3], Step [127700/138038], Loss: 3.4627, Perplexity: 31.90316\n",
      "Epoch [1/3], Step [127800/138038], Loss: 2.7739, Perplexity: 16.02051\n",
      "Epoch [1/3], Step [127900/138038], Loss: 2.9455, Perplexity: 19.02064\n",
      "Epoch [1/3], Step [128000/138038], Loss: 2.8496, Perplexity: 17.28159\n",
      "Epoch [1/3], Step [128100/138038], Loss: 3.2690, Perplexity: 26.2858\n",
      "Epoch [1/3], Step [128200/138038], Loss: 2.7269, Perplexity: 15.28568\n",
      "Epoch [1/3], Step [128300/138038], Loss: 2.7180, Perplexity: 15.15007\n",
      "Epoch [1/3], Step [128400/138038], Loss: 3.5777, Perplexity: 35.7900\n",
      "Epoch [1/3], Step [128500/138038], Loss: 1.7634, Perplexity: 5.83235\n",
      "Epoch [1/3], Step [128600/138038], Loss: 3.2154, Perplexity: 24.9133\n",
      "Epoch [1/3], Step [128700/138038], Loss: 3.0525, Perplexity: 21.1675\n",
      "Epoch [1/3], Step [128800/138038], Loss: 3.7705, Perplexity: 43.4004\n",
      "Epoch [1/3], Step [128900/138038], Loss: 2.1475, Perplexity: 8.563156\n",
      "Epoch [1/3], Step [129000/138038], Loss: 2.1953, Perplexity: 8.982970\n",
      "Epoch [1/3], Step [129100/138038], Loss: 2.0927, Perplexity: 8.107075\n",
      "Epoch [1/3], Step [129200/138038], Loss: 3.5876, Perplexity: 36.1477\n",
      "Epoch [1/3], Step [129300/138038], Loss: 3.2117, Perplexity: 24.8222\n",
      "Epoch [1/3], Step [129400/138038], Loss: 3.0933, Perplexity: 22.05016\n",
      "Epoch [1/3], Step [129500/138038], Loss: 3.7365, Perplexity: 41.94930\n",
      "Epoch [1/3], Step [129600/138038], Loss: 3.2145, Perplexity: 24.8897\n",
      "Epoch [1/3], Step [129700/138038], Loss: 2.7233, Perplexity: 15.2308\n",
      "Epoch [1/3], Step [129800/138038], Loss: 2.0362, Perplexity: 7.66162\n",
      "Epoch [1/3], Step [129900/138038], Loss: 2.8088, Perplexity: 16.5906\n",
      "Epoch [1/3], Step [130000/138038], Loss: 3.0492, Perplexity: 21.0983\n",
      "Epoch [1/3], Step [130100/138038], Loss: 2.3918, Perplexity: 10.9334\n",
      "Epoch [1/3], Step [130200/138038], Loss: 2.2636, Perplexity: 9.617991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [130300/138038], Loss: 2.3817, Perplexity: 10.82320\n",
      "Epoch [1/3], Step [130400/138038], Loss: 3.7157, Perplexity: 41.08612\n",
      "Epoch [1/3], Step [130500/138038], Loss: 3.5638, Perplexity: 35.2971\n",
      "Epoch [1/3], Step [130600/138038], Loss: 3.2072, Perplexity: 24.7107\n",
      "Epoch [1/3], Step [130700/138038], Loss: 3.1776, Perplexity: 23.98862\n",
      "Epoch [1/3], Step [130800/138038], Loss: 3.3794, Perplexity: 29.3541\n",
      "Epoch [1/3], Step [130900/138038], Loss: 2.9580, Perplexity: 19.2592\n",
      "Epoch [1/3], Step [131000/138038], Loss: 2.4329, Perplexity: 11.39245\n",
      "Epoch [1/3], Step [131100/138038], Loss: 1.9997, Perplexity: 7.387247\n",
      "Epoch [1/3], Step [131200/138038], Loss: 2.6294, Perplexity: 13.86599\n",
      "Epoch [1/3], Step [131300/138038], Loss: 2.5550, Perplexity: 12.87193\n",
      "Epoch [1/3], Step [131400/138038], Loss: 2.5588, Perplexity: 12.9203\n",
      "Epoch [1/3], Step [131500/138038], Loss: 2.6404, Perplexity: 14.0190\n",
      "Epoch [1/3], Step [131600/138038], Loss: 2.8090, Perplexity: 16.5941\n",
      "Epoch [1/3], Step [131700/138038], Loss: 4.2847, Perplexity: 72.5835\n",
      "Epoch [1/3], Step [131800/138038], Loss: 2.5954, Perplexity: 13.4013\n",
      "Epoch [1/3], Step [131900/138038], Loss: 4.0863, Perplexity: 59.51879\n",
      "Epoch [1/3], Step [132000/138038], Loss: 2.9141, Perplexity: 18.43194\n",
      "Epoch [1/3], Step [132100/138038], Loss: 2.7351, Perplexity: 15.4109\n",
      "Epoch [1/3], Step [132200/138038], Loss: 3.8055, Perplexity: 44.9486\n",
      "Epoch [1/3], Step [132300/138038], Loss: 2.7287, Perplexity: 15.31316\n",
      "Epoch [1/3], Step [132400/138038], Loss: 2.0281, Perplexity: 7.59956\n",
      "Epoch [1/3], Step [132500/138038], Loss: 2.2417, Perplexity: 9.40910\n",
      "Epoch [1/3], Step [132600/138038], Loss: 2.2927, Perplexity: 9.90122\n",
      "Epoch [1/3], Step [132700/138038], Loss: 3.3024, Perplexity: 27.17883\n",
      "Epoch [1/3], Step [132800/138038], Loss: 3.3682, Perplexity: 29.0255\n",
      "Epoch [1/3], Step [132900/138038], Loss: 3.1984, Perplexity: 24.4937\n",
      "Epoch [1/3], Step [133000/138038], Loss: 2.8193, Perplexity: 16.76562\n",
      "Epoch [1/3], Step [133100/138038], Loss: 3.1794, Perplexity: 24.0317\n",
      "Epoch [1/3], Step [133200/138038], Loss: 3.3686, Perplexity: 29.03748\n",
      "Epoch [1/3], Step [133300/138038], Loss: 2.2939, Perplexity: 9.913425\n",
      "Epoch [1/3], Step [133400/138038], Loss: 3.0631, Perplexity: 21.3935\n",
      "Epoch [1/3], Step [133500/138038], Loss: 2.3213, Perplexity: 10.1893\n",
      "Epoch [1/3], Step [133600/138038], Loss: 2.3055, Perplexity: 10.0291\n",
      "Epoch [1/3], Step [133700/138038], Loss: 3.3586, Perplexity: 28.74925\n",
      "Epoch [1/3], Step [133800/138038], Loss: 3.2250, Perplexity: 25.1544\n",
      "Epoch [1/3], Step [133900/138038], Loss: 2.0953, Perplexity: 8.128066\n",
      "Epoch [1/3], Step [134000/138038], Loss: 2.9195, Perplexity: 18.53250\n",
      "Epoch [1/3], Step [134100/138038], Loss: 2.7252, Perplexity: 15.25902\n",
      "Epoch [1/3], Step [134200/138038], Loss: 3.3129, Perplexity: 27.4653\n",
      "Epoch [1/3], Step [134300/138038], Loss: 2.6505, Perplexity: 14.16185\n",
      "Epoch [1/3], Step [134400/138038], Loss: 3.2264, Perplexity: 25.18812\n",
      "Epoch [1/3], Step [134500/138038], Loss: 2.8299, Perplexity: 16.9441\n",
      "Epoch [1/3], Step [134600/138038], Loss: 1.9374, Perplexity: 6.940546\n",
      "Epoch [1/3], Step [134700/138038], Loss: 2.4858, Perplexity: 12.01033\n",
      "Epoch [1/3], Step [134800/138038], Loss: 3.5640, Perplexity: 35.30552\n",
      "Epoch [1/3], Step [134900/138038], Loss: 3.7640, Perplexity: 43.1209\n",
      "Epoch [1/3], Step [135000/138038], Loss: 1.5967, Perplexity: 4.93691\n",
      "Epoch [1/3], Step [135100/138038], Loss: 2.2020, Perplexity: 9.04278\n",
      "Epoch [1/3], Step [135200/138038], Loss: 2.3006, Perplexity: 9.98041\n",
      "Epoch [1/3], Step [135300/138038], Loss: 2.3715, Perplexity: 10.7136\n",
      "Epoch [1/3], Step [135400/138038], Loss: 2.1858, Perplexity: 8.897832\n",
      "Epoch [1/3], Step [135500/138038], Loss: 2.4288, Perplexity: 11.34552\n",
      "Epoch [1/3], Step [135600/138038], Loss: 2.9560, Perplexity: 19.2210\n",
      "Epoch [1/3], Step [135700/138038], Loss: 2.4157, Perplexity: 11.19714\n",
      "Epoch [1/3], Step [135800/138038], Loss: 3.1357, Perplexity: 23.00454\n",
      "Epoch [1/3], Step [135900/138038], Loss: 3.1128, Perplexity: 22.4830\n",
      "Epoch [1/3], Step [136000/138038], Loss: 2.9320, Perplexity: 18.7647\n",
      "Epoch [1/3], Step [136100/138038], Loss: 4.3609, Perplexity: 78.3270\n",
      "Epoch [1/3], Step [136200/138038], Loss: 3.7123, Perplexity: 40.94779\n",
      "Epoch [1/3], Step [136300/138038], Loss: 2.5655, Perplexity: 13.00728\n",
      "Epoch [1/3], Step [136400/138038], Loss: 3.6795, Perplexity: 39.62539\n",
      "Epoch [1/3], Step [136500/138038], Loss: 3.1278, Perplexity: 22.82364\n",
      "Epoch [1/3], Step [136600/138038], Loss: 2.4638, Perplexity: 11.7497\n",
      "Epoch [1/3], Step [136700/138038], Loss: 3.9544, Perplexity: 52.16483\n",
      "Epoch [1/3], Step [136800/138038], Loss: 2.8216, Perplexity: 16.80354\n",
      "Epoch [1/3], Step [136900/138038], Loss: 2.9637, Perplexity: 19.36986\n",
      "Epoch [1/3], Step [137000/138038], Loss: 2.9686, Perplexity: 19.46518\n",
      "Epoch [1/3], Step [137100/138038], Loss: 2.2681, Perplexity: 9.661072\n",
      "Epoch [1/3], Step [137200/138038], Loss: 2.4889, Perplexity: 12.04769\n",
      "Epoch [1/3], Step [137300/138038], Loss: 2.6260, Perplexity: 13.81899\n",
      "Epoch [1/3], Step [137400/138038], Loss: 2.6105, Perplexity: 13.60578\n",
      "Epoch [1/3], Step [137500/138038], Loss: 2.8031, Perplexity: 16.4963\n",
      "Epoch [1/3], Step [137600/138038], Loss: 3.6201, Perplexity: 37.34161\n",
      "Epoch [1/3], Step [137700/138038], Loss: 3.6804, Perplexity: 39.6628\n",
      "Epoch [1/3], Step [137800/138038], Loss: 1.9137, Perplexity: 6.778028\n",
      "Epoch [1/3], Step [137900/138038], Loss: 3.7107, Perplexity: 40.8826\n",
      "Epoch [1/3], Step [138000/138038], Loss: 2.1346, Perplexity: 8.453612\n",
      "Epoch [2/3], Step [100/138038], Loss: 2.9389, Perplexity: 18.8941633\n",
      "Epoch [2/3], Step [200/138038], Loss: 2.8267, Perplexity: 16.88938\n",
      "Epoch [2/3], Step [300/138038], Loss: 2.8041, Perplexity: 16.5124\n",
      "Epoch [2/3], Step [400/138038], Loss: 3.0150, Perplexity: 20.3896\n",
      "Epoch [2/3], Step [500/138038], Loss: 3.6243, Perplexity: 37.4979\n",
      "Epoch [2/3], Step [600/138038], Loss: 2.6558, Perplexity: 14.2369\n",
      "Epoch [2/3], Step [700/138038], Loss: 1.9268, Perplexity: 6.86772\n",
      "Epoch [2/3], Step [800/138038], Loss: 2.5050, Perplexity: 12.24385\n",
      "Epoch [2/3], Step [900/138038], Loss: 2.6541, Perplexity: 14.21234\n",
      "Epoch [2/3], Step [1000/138038], Loss: 1.9535, Perplexity: 7.0531\n",
      "Epoch [2/3], Step [1100/138038], Loss: 4.0349, Perplexity: 56.5371\n",
      "Epoch [2/3], Step [1200/138038], Loss: 3.3400, Perplexity: 28.2192\n",
      "Epoch [2/3], Step [1300/138038], Loss: 2.3209, Perplexity: 10.18467\n",
      "Epoch [2/3], Step [1400/138038], Loss: 2.4099, Perplexity: 11.13266\n",
      "Epoch [2/3], Step [1500/138038], Loss: 1.9925, Perplexity: 7.334007\n",
      "Epoch [2/3], Step [1600/138038], Loss: 3.1728, Perplexity: 23.8739\n",
      "Epoch [2/3], Step [1700/138038], Loss: 3.3733, Perplexity: 29.17411\n",
      "Epoch [2/3], Step [1800/138038], Loss: 2.5312, Perplexity: 12.5684\n",
      "Epoch [2/3], Step [1900/138038], Loss: 2.4940, Perplexity: 12.11025\n",
      "Epoch [2/3], Step [2000/138038], Loss: 3.0228, Perplexity: 20.5482\n",
      "Epoch [2/3], Step [2100/138038], Loss: 2.9318, Perplexity: 18.76069\n",
      "Epoch [2/3], Step [2200/138038], Loss: 2.4031, Perplexity: 11.05719\n",
      "Epoch [2/3], Step [2300/138038], Loss: 3.5239, Perplexity: 33.9178\n",
      "Epoch [2/3], Step [2400/138038], Loss: 2.8784, Perplexity: 17.7861\n",
      "Epoch [2/3], Step [2500/138038], Loss: 3.2211, Perplexity: 25.05455\n",
      "Epoch [2/3], Step [2600/138038], Loss: 3.1578, Perplexity: 23.51816\n",
      "Epoch [2/3], Step [2700/138038], Loss: 4.1518, Perplexity: 63.5472\n",
      "Epoch [2/3], Step [2800/138038], Loss: 3.3897, Perplexity: 29.65750\n",
      "Epoch [2/3], Step [2900/138038], Loss: 1.9303, Perplexity: 6.89188\n",
      "Epoch [2/3], Step [3000/138038], Loss: 3.4075, Perplexity: 30.18978\n",
      "Epoch [2/3], Step [3100/138038], Loss: 3.3321, Perplexity: 27.99649\n",
      "Epoch [2/3], Step [3200/138038], Loss: 4.6731, Perplexity: 107.0279\n",
      "Epoch [2/3], Step [3300/138038], Loss: 1.5849, Perplexity: 4.87870\n",
      "Epoch [2/3], Step [3400/138038], Loss: 3.7384, Perplexity: 42.03271\n",
      "Epoch [2/3], Step [3500/138038], Loss: 2.4141, Perplexity: 11.18015\n",
      "Epoch [2/3], Step [3600/138038], Loss: 2.4746, Perplexity: 11.87655\n",
      "Epoch [2/3], Step [3700/138038], Loss: 3.1463, Perplexity: 23.24965\n",
      "Epoch [2/3], Step [3800/138038], Loss: 3.7695, Perplexity: 43.3588\n",
      "Epoch [2/3], Step [3900/138038], Loss: 2.6757, Perplexity: 14.5218\n",
      "Epoch [2/3], Step [4000/138038], Loss: 2.9375, Perplexity: 18.8695\n",
      "Epoch [2/3], Step [4100/138038], Loss: 3.0221, Perplexity: 20.5350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [4200/138038], Loss: 2.6225, Perplexity: 13.7699\n",
      "Epoch [2/3], Step [4300/138038], Loss: 3.2807, Perplexity: 26.5951\n",
      "Epoch [2/3], Step [4400/138038], Loss: 3.2105, Perplexity: 24.7903\n",
      "Epoch [2/3], Step [4500/138038], Loss: 2.3529, Perplexity: 10.51564\n",
      "Epoch [2/3], Step [4600/138038], Loss: 2.3536, Perplexity: 10.52293\n",
      "Epoch [2/3], Step [4700/138038], Loss: 5.3407, Perplexity: 208.6662\n",
      "Epoch [2/3], Step [4800/138038], Loss: 2.1951, Perplexity: 8.98100\n",
      "Epoch [2/3], Step [4900/138038], Loss: 3.6114, Perplexity: 37.0164\n",
      "Epoch [2/3], Step [5000/138038], Loss: 2.5498, Perplexity: 12.80463\n",
      "Epoch [2/3], Step [5100/138038], Loss: 2.4323, Perplexity: 11.3853\n",
      "Epoch [2/3], Step [5200/138038], Loss: 2.8539, Perplexity: 17.3555\n",
      "Epoch [2/3], Step [5300/138038], Loss: 1.8082, Perplexity: 6.099680\n",
      "Epoch [2/3], Step [5400/138038], Loss: 2.4579, Perplexity: 11.6798\n",
      "Epoch [2/3], Step [5500/138038], Loss: 3.6737, Perplexity: 39.3991\n",
      "Epoch [2/3], Step [5600/138038], Loss: 3.3349, Perplexity: 28.07432\n",
      "Epoch [2/3], Step [5700/138038], Loss: 3.0688, Perplexity: 21.5157\n",
      "Epoch [2/3], Step [5800/138038], Loss: 2.4647, Perplexity: 11.76032\n",
      "Epoch [2/3], Step [5900/138038], Loss: 2.8667, Perplexity: 17.57866\n",
      "Epoch [2/3], Step [6000/138038], Loss: 1.6418, Perplexity: 5.16432\n",
      "Epoch [2/3], Step [6100/138038], Loss: 3.9347, Perplexity: 51.1469\n",
      "Epoch [2/3], Step [6200/138038], Loss: 2.4599, Perplexity: 11.7036\n",
      "Epoch [2/3], Step [6300/138038], Loss: 3.1571, Perplexity: 23.50359\n",
      "Epoch [2/3], Step [6400/138038], Loss: 3.1470, Perplexity: 23.26714\n",
      "Epoch [2/3], Step [6500/138038], Loss: 2.5805, Perplexity: 13.2033\n",
      "Epoch [2/3], Step [6600/138038], Loss: 2.9410, Perplexity: 18.93460\n",
      "Epoch [2/3], Step [6700/138038], Loss: 2.5306, Perplexity: 12.56091\n",
      "Epoch [2/3], Step [6800/138038], Loss: 2.9947, Perplexity: 19.97871\n",
      "Epoch [2/3], Step [6900/138038], Loss: 2.4871, Perplexity: 12.0269\n",
      "Epoch [2/3], Step [7000/138038], Loss: 2.1670, Perplexity: 8.732298\n",
      "Epoch [2/3], Step [7100/138038], Loss: 3.1255, Perplexity: 22.7723\n",
      "Epoch [2/3], Step [7200/138038], Loss: 2.2200, Perplexity: 9.20721\n",
      "Epoch [2/3], Step [7300/138038], Loss: 3.3296, Perplexity: 27.92820\n",
      "Epoch [2/3], Step [7400/138038], Loss: 3.0787, Perplexity: 21.72993\n",
      "Epoch [2/3], Step [7500/138038], Loss: 2.5469, Perplexity: 12.76732\n",
      "Epoch [2/3], Step [7600/138038], Loss: 2.6855, Perplexity: 14.6661\n",
      "Epoch [2/3], Step [7700/138038], Loss: 3.2548, Perplexity: 25.9144\n",
      "Epoch [2/3], Step [7800/138038], Loss: 2.3504, Perplexity: 10.4897\n",
      "Epoch [2/3], Step [7900/138038], Loss: 2.4146, Perplexity: 11.1856\n",
      "Epoch [2/3], Step [8000/138038], Loss: 2.1371, Perplexity: 8.474407\n",
      "Epoch [2/3], Step [8100/138038], Loss: 3.8754, Perplexity: 48.20119\n",
      "Epoch [2/3], Step [8200/138038], Loss: 2.0072, Perplexity: 7.442646\n",
      "Epoch [2/3], Step [8300/138038], Loss: 2.4201, Perplexity: 11.2475\n",
      "Epoch [2/3], Step [8400/138038], Loss: 3.0540, Perplexity: 21.2001\n",
      "Epoch [2/3], Step [8500/138038], Loss: 3.8491, Perplexity: 46.9493\n",
      "Epoch [2/3], Step [8600/138038], Loss: 2.7587, Perplexity: 15.7800\n",
      "Epoch [2/3], Step [8700/138038], Loss: 3.1812, Perplexity: 24.0756\n",
      "Epoch [2/3], Step [8800/138038], Loss: 2.6334, Perplexity: 13.9206\n",
      "Epoch [2/3], Step [8900/138038], Loss: 2.7251, Perplexity: 15.25777\n",
      "Epoch [2/3], Step [9000/138038], Loss: 3.4925, Perplexity: 32.86954\n",
      "Epoch [2/3], Step [9100/138038], Loss: 3.7552, Perplexity: 42.74137\n",
      "Epoch [2/3], Step [9200/138038], Loss: 3.3199, Perplexity: 27.6565\n",
      "Epoch [2/3], Step [9300/138038], Loss: 2.6134, Perplexity: 13.6449\n",
      "Epoch [2/3], Step [9400/138038], Loss: 2.9292, Perplexity: 18.71286\n",
      "Epoch [2/3], Step [9500/138038], Loss: 3.4915, Perplexity: 32.83535\n",
      "Epoch [2/3], Step [9600/138038], Loss: 2.8342, Perplexity: 17.0164\n",
      "Epoch [2/3], Step [9700/138038], Loss: 3.1425, Perplexity: 23.16216\n",
      "Epoch [2/3], Step [9800/138038], Loss: 3.5812, Perplexity: 35.91787\n",
      "Epoch [2/3], Step [9900/138038], Loss: 2.2824, Perplexity: 9.800358\n",
      "Epoch [2/3], Step [10000/138038], Loss: 2.8273, Perplexity: 16.8998\n",
      "Epoch [2/3], Step [10100/138038], Loss: 3.7241, Perplexity: 41.4323\n",
      "Epoch [2/3], Step [10200/138038], Loss: 2.7408, Perplexity: 15.49933\n",
      "Epoch [2/3], Step [10300/138038], Loss: 2.8191, Perplexity: 16.76245\n",
      "Epoch [2/3], Step [10400/138038], Loss: 3.3963, Perplexity: 29.8546\n",
      "Epoch [2/3], Step [10500/138038], Loss: 2.6932, Perplexity: 14.7782\n",
      "Epoch [2/3], Step [10600/138038], Loss: 2.1556, Perplexity: 8.632887\n",
      "Epoch [2/3], Step [10700/138038], Loss: 3.5967, Perplexity: 36.4792\n",
      "Epoch [2/3], Step [10800/138038], Loss: 3.1673, Perplexity: 23.7440\n",
      "Epoch [2/3], Step [10900/138038], Loss: 2.1790, Perplexity: 8.83727\n",
      "Epoch [2/3], Step [11000/138038], Loss: 2.2942, Perplexity: 9.91678\n",
      "Epoch [2/3], Step [11100/138038], Loss: 3.1477, Perplexity: 23.28305\n",
      "Epoch [2/3], Step [11200/138038], Loss: 3.2586, Perplexity: 26.01372\n",
      "Epoch [2/3], Step [11300/138038], Loss: 3.0245, Perplexity: 20.5842\n",
      "Epoch [2/3], Step [11400/138038], Loss: 2.0888, Perplexity: 8.075561\n",
      "Epoch [2/3], Step [11500/138038], Loss: 2.6507, Perplexity: 14.1640\n",
      "Epoch [2/3], Step [11600/138038], Loss: 3.5624, Perplexity: 35.2488\n",
      "Epoch [2/3], Step [11700/138038], Loss: 1.6965, Perplexity: 5.45498\n",
      "Epoch [2/3], Step [11800/138038], Loss: 1.7821, Perplexity: 5.94250\n",
      "Epoch [2/3], Step [11900/138038], Loss: 1.9995, Perplexity: 7.385251\n",
      "Epoch [2/3], Step [12000/138038], Loss: 2.2398, Perplexity: 9.391871\n",
      "Epoch [2/3], Step [12100/138038], Loss: 2.9976, Perplexity: 20.03835\n",
      "Epoch [2/3], Step [12200/138038], Loss: 3.9240, Perplexity: 50.6037\n",
      "Epoch [2/3], Step [12300/138038], Loss: 2.1666, Perplexity: 8.728300\n",
      "Epoch [2/3], Step [12400/138038], Loss: 1.9264, Perplexity: 6.86496\n",
      "Epoch [2/3], Step [12500/138038], Loss: 2.5705, Perplexity: 13.07264\n",
      "Epoch [2/3], Step [12600/138038], Loss: 2.2161, Perplexity: 9.171746\n",
      "Epoch [2/3], Step [12700/138038], Loss: 1.8484, Perplexity: 6.34986\n",
      "Epoch [2/3], Step [12800/138038], Loss: 4.0644, Perplexity: 58.2272\n",
      "Epoch [2/3], Step [12900/138038], Loss: 1.9758, Perplexity: 7.21248\n",
      "Epoch [2/3], Step [13000/138038], Loss: 2.9589, Perplexity: 19.27769\n",
      "Epoch [2/3], Step [13100/138038], Loss: 2.3116, Perplexity: 10.0902\n",
      "Epoch [2/3], Step [13200/138038], Loss: 2.3946, Perplexity: 10.96390\n",
      "Epoch [2/3], Step [13300/138038], Loss: 2.9706, Perplexity: 19.50420\n",
      "Epoch [2/3], Step [13400/138038], Loss: 2.3976, Perplexity: 10.9962\n",
      "Epoch [2/3], Step [13500/138038], Loss: 2.4748, Perplexity: 11.87923\n",
      "Epoch [2/3], Step [13600/138038], Loss: 2.4911, Perplexity: 12.07424\n",
      "Epoch [2/3], Step [13700/138038], Loss: 2.2060, Perplexity: 9.079082\n",
      "Epoch [2/3], Step [13800/138038], Loss: 1.8720, Perplexity: 6.50123\n",
      "Epoch [2/3], Step [13900/138038], Loss: 2.9991, Perplexity: 20.06733\n",
      "Epoch [2/3], Step [14000/138038], Loss: 4.1231, Perplexity: 61.7484\n",
      "Epoch [2/3], Step [14100/138038], Loss: 2.7668, Perplexity: 15.9082\n",
      "Epoch [2/3], Step [14200/138038], Loss: 3.0502, Perplexity: 21.1203\n",
      "Epoch [2/3], Step [14300/138038], Loss: 3.8472, Perplexity: 46.8619\n",
      "Epoch [2/3], Step [14400/138038], Loss: 2.3678, Perplexity: 10.67413\n",
      "Epoch [2/3], Step [14500/138038], Loss: 2.6880, Perplexity: 14.7016\n",
      "Epoch [2/3], Step [14600/138038], Loss: 2.3761, Perplexity: 10.7630\n",
      "Epoch [2/3], Step [14700/138038], Loss: 2.6019, Perplexity: 13.4887\n",
      "Epoch [2/3], Step [14800/138038], Loss: 2.3424, Perplexity: 10.40590\n",
      "Epoch [2/3], Step [14900/138038], Loss: 1.5821, Perplexity: 4.865217\n",
      "Epoch [2/3], Step [15000/138038], Loss: 2.8940, Perplexity: 18.06525\n",
      "Epoch [2/3], Step [15100/138038], Loss: 2.3736, Perplexity: 10.73556\n",
      "Epoch [2/3], Step [15200/138038], Loss: 2.2053, Perplexity: 9.072890\n",
      "Epoch [2/3], Step [15300/138038], Loss: 2.6473, Perplexity: 14.1160\n",
      "Epoch [2/3], Step [15400/138038], Loss: 2.4637, Perplexity: 11.7485\n",
      "Epoch [2/3], Step [15500/138038], Loss: 3.7762, Perplexity: 43.6498\n",
      "Epoch [2/3], Step [15600/138038], Loss: 2.2404, Perplexity: 9.39748\n",
      "Epoch [2/3], Step [15700/138038], Loss: 3.2398, Perplexity: 25.5274\n",
      "Epoch [2/3], Step [15800/138038], Loss: 2.3187, Perplexity: 10.1621\n",
      "Epoch [2/3], Step [15900/138038], Loss: 2.5476, Perplexity: 12.7761\n",
      "Epoch [2/3], Step [16000/138038], Loss: 2.6692, Perplexity: 14.4284\n",
      "Epoch [2/3], Step [16100/138038], Loss: 2.5128, Perplexity: 12.33952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [16200/138038], Loss: 2.2146, Perplexity: 9.15749\n",
      "Epoch [2/3], Step [16300/138038], Loss: 2.0966, Perplexity: 8.138549\n",
      "Epoch [2/3], Step [16400/138038], Loss: 2.5290, Perplexity: 12.5409\n",
      "Epoch [2/3], Step [16500/138038], Loss: 2.7139, Perplexity: 15.08783\n",
      "Epoch [2/3], Step [16600/138038], Loss: 2.7462, Perplexity: 15.58319\n",
      "Epoch [2/3], Step [16700/138038], Loss: 2.9715, Perplexity: 19.5209\n",
      "Epoch [2/3], Step [16800/138038], Loss: 2.8013, Perplexity: 16.46651\n",
      "Epoch [2/3], Step [16900/138038], Loss: 2.2405, Perplexity: 9.39769\n",
      "Epoch [2/3], Step [17000/138038], Loss: 3.7010, Perplexity: 40.48600\n",
      "Epoch [2/3], Step [17100/138038], Loss: 2.8438, Perplexity: 17.18082\n",
      "Epoch [2/3], Step [17200/138038], Loss: 3.3751, Perplexity: 29.2276\n",
      "Epoch [2/3], Step [17300/138038], Loss: 2.0858, Perplexity: 8.05065\n",
      "Epoch [2/3], Step [17400/138038], Loss: 2.8076, Perplexity: 16.5705\n",
      "Epoch [2/3], Step [17500/138038], Loss: 2.9747, Perplexity: 19.5837\n",
      "Epoch [2/3], Step [17600/138038], Loss: 4.0969, Perplexity: 60.1565\n",
      "Epoch [2/3], Step [17700/138038], Loss: 3.1765, Perplexity: 23.96398\n",
      "Epoch [2/3], Step [17800/138038], Loss: 3.1060, Perplexity: 22.33161\n",
      "Epoch [2/3], Step [17900/138038], Loss: 2.6989, Perplexity: 14.8631\n",
      "Epoch [2/3], Step [18000/138038], Loss: 3.9582, Perplexity: 52.3638\n",
      "Epoch [2/3], Step [18100/138038], Loss: 3.0892, Perplexity: 21.9590\n",
      "Epoch [2/3], Step [18200/138038], Loss: 2.2831, Perplexity: 9.80725\n",
      "Epoch [2/3], Step [18300/138038], Loss: 2.0614, Perplexity: 7.85689\n",
      "Epoch [2/3], Step [18400/138038], Loss: 2.4321, Perplexity: 11.3833\n",
      "Epoch [2/3], Step [18500/138038], Loss: 2.6798, Perplexity: 14.5817\n",
      "Epoch [2/3], Step [18600/138038], Loss: 2.5835, Perplexity: 13.2433\n",
      "Epoch [2/3], Step [18700/138038], Loss: 2.0517, Perplexity: 7.781036\n",
      "Epoch [2/3], Step [18800/138038], Loss: 3.1039, Perplexity: 22.28557\n",
      "Epoch [2/3], Step [18900/138038], Loss: 3.1800, Perplexity: 24.0464\n",
      "Epoch [2/3], Step [19000/138038], Loss: 3.0472, Perplexity: 21.05535\n",
      "Epoch [2/3], Step [19100/138038], Loss: 2.4110, Perplexity: 11.1456\n",
      "Epoch [2/3], Step [19200/138038], Loss: 3.6680, Perplexity: 39.1724\n",
      "Epoch [2/3], Step [19300/138038], Loss: 2.8315, Perplexity: 16.97080\n",
      "Epoch [2/3], Step [19400/138038], Loss: 3.9205, Perplexity: 50.4265\n",
      "Epoch [2/3], Step [19500/138038], Loss: 3.1276, Perplexity: 22.81901\n",
      "Epoch [2/3], Step [19600/138038], Loss: 2.2045, Perplexity: 9.065994\n",
      "Epoch [2/3], Step [19700/138038], Loss: 2.1261, Perplexity: 8.38233\n",
      "Epoch [2/3], Step [19800/138038], Loss: 2.1337, Perplexity: 8.446155\n",
      "Epoch [2/3], Step [19900/138038], Loss: 2.8576, Perplexity: 17.41930\n",
      "Epoch [2/3], Step [20000/138038], Loss: 3.7227, Perplexity: 41.3765\n",
      "Epoch [2/3], Step [20100/138038], Loss: 2.2033, Perplexity: 9.054816\n",
      "Epoch [2/3], Step [20200/138038], Loss: 3.0312, Perplexity: 20.72274\n",
      "Epoch [2/3], Step [20300/138038], Loss: 3.4845, Perplexity: 32.6074\n",
      "Epoch [2/3], Step [20400/138038], Loss: 2.8628, Perplexity: 17.5098\n",
      "Epoch [2/3], Step [20500/138038], Loss: 1.8557, Perplexity: 6.39590\n",
      "Epoch [2/3], Step [20600/138038], Loss: 3.1146, Perplexity: 22.52517\n",
      "Epoch [2/3], Step [20700/138038], Loss: 2.9824, Perplexity: 19.73586\n",
      "Epoch [2/3], Step [20800/138038], Loss: 3.3617, Perplexity: 28.83885\n",
      "Epoch [2/3], Step [20900/138038], Loss: 3.4675, Perplexity: 32.05762\n",
      "Epoch [2/3], Step [21000/138038], Loss: 2.3318, Perplexity: 10.2964\n",
      "Epoch [2/3], Step [21100/138038], Loss: 3.0869, Perplexity: 21.90920\n",
      "Epoch [2/3], Step [21200/138038], Loss: 2.5386, Perplexity: 12.66219\n",
      "Epoch [2/3], Step [21300/138038], Loss: 2.1176, Perplexity: 8.311373\n",
      "Epoch [2/3], Step [21400/138038], Loss: 2.4832, Perplexity: 11.9792\n",
      "Epoch [2/3], Step [21500/138038], Loss: 3.9716, Perplexity: 53.0669\n",
      "Epoch [2/3], Step [21600/138038], Loss: 2.3327, Perplexity: 10.3057\n",
      "Epoch [2/3], Step [21700/138038], Loss: 3.3354, Perplexity: 28.0899\n",
      "Epoch [2/3], Step [21800/138038], Loss: 2.7188, Perplexity: 15.1623\n",
      "Epoch [2/3], Step [21900/138038], Loss: 2.0300, Perplexity: 7.614576\n",
      "Epoch [2/3], Step [22000/138038], Loss: 2.2346, Perplexity: 9.34309\n",
      "Epoch [2/3], Step [22100/138038], Loss: 3.8406, Perplexity: 46.55202\n",
      "Epoch [2/3], Step [22200/138038], Loss: 3.0815, Perplexity: 21.7901\n",
      "Epoch [2/3], Step [22300/138038], Loss: 2.3453, Perplexity: 10.4362\n",
      "Epoch [2/3], Step [22400/138038], Loss: 3.8253, Perplexity: 45.84684\n",
      "Epoch [2/3], Step [22500/138038], Loss: 1.9332, Perplexity: 6.911784\n",
      "Epoch [2/3], Step [22600/138038], Loss: 1.8573, Perplexity: 6.406756\n",
      "Epoch [2/3], Step [22700/138038], Loss: 3.0428, Perplexity: 20.96411\n",
      "Epoch [2/3], Step [22800/138038], Loss: 3.6959, Perplexity: 40.2820\n",
      "Epoch [2/3], Step [22900/138038], Loss: 2.8849, Perplexity: 17.9021\n",
      "Epoch [2/3], Step [23000/138038], Loss: 3.7436, Perplexity: 42.2510\n",
      "Epoch [2/3], Step [23100/138038], Loss: 2.5796, Perplexity: 13.1915\n",
      "Epoch [2/3], Step [23200/138038], Loss: 3.5478, Perplexity: 34.73566\n",
      "Epoch [2/3], Step [23300/138038], Loss: 2.1949, Perplexity: 8.97927\n",
      "Epoch [2/3], Step [23400/138038], Loss: 2.5920, Perplexity: 13.35718\n",
      "Epoch [2/3], Step [23500/138038], Loss: 4.4725, Perplexity: 87.57209\n",
      "Epoch [2/3], Step [23600/138038], Loss: 2.2623, Perplexity: 9.60533\n",
      "Epoch [2/3], Step [23700/138038], Loss: 4.3098, Perplexity: 74.4226\n",
      "Epoch [2/3], Step [23800/138038], Loss: 2.5218, Perplexity: 12.4509\n",
      "Epoch [2/3], Step [23900/138038], Loss: 2.7216, Perplexity: 15.20420\n",
      "Epoch [2/3], Step [24000/138038], Loss: 3.6242, Perplexity: 37.49613\n",
      "Epoch [2/3], Step [24100/138038], Loss: 2.5523, Perplexity: 12.8363\n",
      "Epoch [2/3], Step [24200/138038], Loss: 2.3992, Perplexity: 11.01449\n",
      "Epoch [2/3], Step [24300/138038], Loss: 2.1187, Perplexity: 8.320278\n",
      "Epoch [2/3], Step [24400/138038], Loss: 2.1947, Perplexity: 8.97762\n",
      "Epoch [2/3], Step [24500/138038], Loss: 2.9956, Perplexity: 19.99712\n",
      "Epoch [2/3], Step [24600/138038], Loss: 2.0945, Perplexity: 8.12168\n",
      "Epoch [2/3], Step [24700/138038], Loss: 3.4186, Perplexity: 30.52554\n",
      "Epoch [2/3], Step [24800/138038], Loss: 3.1547, Perplexity: 23.4453\n",
      "Epoch [2/3], Step [24900/138038], Loss: 3.2182, Perplexity: 24.9832\n",
      "Epoch [2/3], Step [25000/138038], Loss: 2.4791, Perplexity: 11.93056\n",
      "Epoch [2/3], Step [25100/138038], Loss: 3.7753, Perplexity: 43.61014\n",
      "Epoch [2/3], Step [25200/138038], Loss: 2.1423, Perplexity: 8.51901\n",
      "Epoch [2/3], Step [25300/138038], Loss: 3.3443, Perplexity: 28.34055\n",
      "Epoch [2/3], Step [25400/138038], Loss: 3.5372, Perplexity: 34.3701\n",
      "Epoch [2/3], Step [25500/138038], Loss: 2.4712, Perplexity: 11.8366\n",
      "Epoch [2/3], Step [25600/138038], Loss: 4.1710, Perplexity: 64.7824\n",
      "Epoch [2/3], Step [25700/138038], Loss: 3.3593, Perplexity: 28.77000\n",
      "Epoch [2/3], Step [25800/138038], Loss: 2.9227, Perplexity: 18.5908\n",
      "Epoch [2/3], Step [25900/138038], Loss: 3.1649, Perplexity: 23.6859\n",
      "Epoch [2/3], Step [26000/138038], Loss: 4.9028, Perplexity: 134.6644\n",
      "Epoch [2/3], Step [26100/138038], Loss: 3.2327, Perplexity: 25.3487\n",
      "Epoch [2/3], Step [26200/138038], Loss: 2.8928, Perplexity: 18.0437\n",
      "Epoch [2/3], Step [26300/138038], Loss: 3.3304, Perplexity: 27.95046\n",
      "Epoch [2/3], Step [26400/138038], Loss: 1.8993, Perplexity: 6.680924\n",
      "Epoch [2/3], Step [26500/138038], Loss: 2.9215, Perplexity: 18.5689\n",
      "Epoch [2/3], Step [26600/138038], Loss: 2.9867, Perplexity: 19.81933\n",
      "Epoch [2/3], Step [26700/138038], Loss: 2.4332, Perplexity: 11.39503\n",
      "Epoch [2/3], Step [26800/138038], Loss: 5.0324, Perplexity: 153.3006\n",
      "Epoch [2/3], Step [26900/138038], Loss: 2.8592, Perplexity: 17.4478\n",
      "Epoch [2/3], Step [27000/138038], Loss: 2.8708, Perplexity: 17.6505\n",
      "Epoch [2/3], Step [27100/138038], Loss: 2.9020, Perplexity: 18.2111\n",
      "Epoch [2/3], Step [27200/138038], Loss: 3.4811, Perplexity: 32.4942\n",
      "Epoch [2/3], Step [27300/138038], Loss: 3.2354, Perplexity: 25.4161\n",
      "Epoch [2/3], Step [27400/138038], Loss: 2.6084, Perplexity: 13.5771\n",
      "Epoch [2/3], Step [27500/138038], Loss: 2.4068, Perplexity: 11.09884\n",
      "Epoch [2/3], Step [27600/138038], Loss: 3.6995, Perplexity: 40.4252\n",
      "Epoch [2/3], Step [27700/138038], Loss: 2.3536, Perplexity: 10.52353\n",
      "Epoch [2/3], Step [27800/138038], Loss: 2.3871, Perplexity: 10.8816\n",
      "Epoch [2/3], Step [27900/138038], Loss: 3.5140, Perplexity: 33.5812\n",
      "Epoch [2/3], Step [28000/138038], Loss: 1.8625, Perplexity: 6.439649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [28100/138038], Loss: 2.3354, Perplexity: 10.3331\n",
      "Epoch [2/3], Step [28200/138038], Loss: 3.2090, Perplexity: 24.75499\n",
      "Epoch [2/3], Step [28300/138038], Loss: 2.3272, Perplexity: 10.2494\n",
      "Epoch [2/3], Step [28400/138038], Loss: 3.4425, Perplexity: 31.2637\n",
      "Epoch [2/3], Step [28500/138038], Loss: 3.0801, Perplexity: 21.7603\n",
      "Epoch [2/3], Step [28600/138038], Loss: 2.4903, Perplexity: 12.06515\n",
      "Epoch [2/3], Step [28700/138038], Loss: 3.4151, Perplexity: 30.4202\n",
      "Epoch [2/3], Step [28800/138038], Loss: 2.6644, Perplexity: 14.3590\n",
      "Epoch [2/3], Step [28900/138038], Loss: 3.2219, Perplexity: 25.0761\n",
      "Epoch [2/3], Step [29000/138038], Loss: 3.1582, Perplexity: 23.52854\n",
      "Epoch [2/3], Step [29100/138038], Loss: 3.7089, Perplexity: 40.8072\n",
      "Epoch [2/3], Step [29200/138038], Loss: 1.9516, Perplexity: 7.040042\n",
      "Epoch [2/3], Step [29300/138038], Loss: 2.1647, Perplexity: 8.712474\n",
      "Epoch [2/3], Step [29400/138038], Loss: 2.8234, Perplexity: 16.8344\n",
      "Epoch [2/3], Step [29500/138038], Loss: 2.6942, Perplexity: 14.7930\n",
      "Epoch [2/3], Step [29600/138038], Loss: 3.3055, Perplexity: 27.2633\n",
      "Epoch [2/3], Step [29700/138038], Loss: 2.6132, Perplexity: 13.6425\n",
      "Epoch [2/3], Step [29800/138038], Loss: 4.6959, Perplexity: 109.4975\n",
      "Epoch [2/3], Step [29900/138038], Loss: 3.3589, Perplexity: 28.7577\n",
      "Epoch [2/3], Step [30000/138038], Loss: 2.0444, Perplexity: 7.724972\n",
      "Epoch [2/3], Step [30100/138038], Loss: 2.7765, Perplexity: 16.06265\n",
      "Epoch [2/3], Step [30200/138038], Loss: 1.9243, Perplexity: 6.850617\n",
      "Epoch [2/3], Step [30300/138038], Loss: 2.7629, Perplexity: 15.8464\n",
      "Epoch [2/3], Step [30400/138038], Loss: 2.2541, Perplexity: 9.526436\n",
      "Epoch [2/3], Step [30500/138038], Loss: 2.8445, Perplexity: 17.19246\n",
      "Epoch [2/3], Step [30600/138038], Loss: 2.8078, Perplexity: 16.5740\n",
      "Epoch [2/3], Step [30700/138038], Loss: 2.0917, Perplexity: 8.09888\n",
      "Epoch [2/3], Step [30800/138038], Loss: 3.6288, Perplexity: 37.6671\n",
      "Epoch [2/3], Step [30900/138038], Loss: 2.2388, Perplexity: 9.381841\n",
      "Epoch [2/3], Step [31000/138038], Loss: 2.1118, Perplexity: 8.263120\n",
      "Epoch [2/3], Step [31100/138038], Loss: 2.3598, Perplexity: 10.5890\n",
      "Epoch [2/3], Step [31200/138038], Loss: 1.5544, Perplexity: 4.732344\n",
      "Epoch [2/3], Step [31300/138038], Loss: 1.8367, Perplexity: 6.275674\n",
      "Epoch [2/3], Step [31400/138038], Loss: 3.2615, Perplexity: 26.08962\n",
      "Epoch [2/3], Step [31500/138038], Loss: 2.3447, Perplexity: 10.42973\n",
      "Epoch [2/3], Step [31600/138038], Loss: 2.9897, Perplexity: 19.87889\n",
      "Epoch [2/3], Step [31700/138038], Loss: 2.3911, Perplexity: 10.92543\n",
      "Epoch [2/3], Step [31800/138038], Loss: 3.3552, Perplexity: 28.6514\n",
      "Epoch [2/3], Step [31900/138038], Loss: 3.6165, Perplexity: 37.2082\n",
      "Epoch [2/3], Step [32000/138038], Loss: 2.3951, Perplexity: 10.9693\n",
      "Epoch [2/3], Step [32100/138038], Loss: 2.7595, Perplexity: 15.7914\n",
      "Epoch [2/3], Step [32200/138038], Loss: 3.3856, Perplexity: 29.5370\n",
      "Epoch [2/3], Step [32300/138038], Loss: 2.9988, Perplexity: 20.0613\n",
      "Epoch [2/3], Step [32400/138038], Loss: 2.9118, Perplexity: 18.3903\n",
      "Epoch [2/3], Step [32500/138038], Loss: 2.8617, Perplexity: 17.49146\n",
      "Epoch [2/3], Step [32600/138038], Loss: 3.2370, Perplexity: 25.4560\n",
      "Epoch [2/3], Step [32700/138038], Loss: 2.6891, Perplexity: 14.71803\n",
      "Epoch [2/3], Step [32800/138038], Loss: 2.3726, Perplexity: 10.72477\n",
      "Epoch [2/3], Step [32900/138038], Loss: 1.9570, Perplexity: 7.077724\n",
      "Epoch [2/3], Step [33000/138038], Loss: 2.1355, Perplexity: 8.46113\n",
      "Epoch [2/3], Step [33100/138038], Loss: 3.0659, Perplexity: 21.45357\n",
      "Epoch [2/3], Step [33200/138038], Loss: 3.0703, Perplexity: 21.54891\n",
      "Epoch [2/3], Step [33300/138038], Loss: 3.4338, Perplexity: 30.99436\n",
      "Epoch [2/3], Step [33400/138038], Loss: 3.0797, Perplexity: 21.7518\n",
      "Epoch [2/3], Step [33500/138038], Loss: 2.1637, Perplexity: 8.702958\n",
      "Epoch [2/3], Step [33600/138038], Loss: 3.6624, Perplexity: 38.9540\n",
      "Epoch [2/3], Step [33700/138038], Loss: 3.2556, Perplexity: 25.9343\n",
      "Epoch [2/3], Step [33800/138038], Loss: 3.2083, Perplexity: 24.73651\n",
      "Epoch [2/3], Step [33900/138038], Loss: 3.9294, Perplexity: 50.87663\n",
      "Epoch [2/3], Step [34000/138038], Loss: 3.5447, Perplexity: 34.62907\n",
      "Epoch [2/3], Step [34100/138038], Loss: 2.6479, Perplexity: 14.1244\n",
      "Epoch [2/3], Step [34200/138038], Loss: 3.8977, Perplexity: 49.2881\n",
      "Epoch [2/3], Step [34300/138038], Loss: 2.5049, Perplexity: 12.24233\n",
      "Epoch [2/3], Step [34400/138038], Loss: 2.8765, Perplexity: 17.75209\n",
      "Epoch [2/3], Step [34500/138038], Loss: 3.1895, Perplexity: 24.27636\n",
      "Epoch [2/3], Step [34600/138038], Loss: 2.6506, Perplexity: 14.1629\n",
      "Epoch [2/3], Step [34700/138038], Loss: 2.7914, Perplexity: 16.30312\n",
      "Epoch [2/3], Step [34800/138038], Loss: 2.7661, Perplexity: 15.8962\n",
      "Epoch [2/3], Step [34900/138038], Loss: 3.4686, Perplexity: 32.0923\n",
      "Epoch [2/3], Step [35000/138038], Loss: 2.5761, Perplexity: 13.1452\n",
      "Epoch [2/3], Step [35100/138038], Loss: 1.9650, Perplexity: 7.13482\n",
      "Epoch [2/3], Step [35200/138038], Loss: 2.9334, Perplexity: 18.7918\n",
      "Epoch [2/3], Step [35300/138038], Loss: 3.6124, Perplexity: 37.0555\n",
      "Epoch [2/3], Step [35400/138038], Loss: 3.0702, Perplexity: 21.54694\n",
      "Epoch [2/3], Step [35500/138038], Loss: 2.9699, Perplexity: 19.4895\n",
      "Epoch [2/3], Step [35600/138038], Loss: 2.7371, Perplexity: 15.4428\n",
      "Epoch [2/3], Step [35700/138038], Loss: 3.2633, Perplexity: 26.1352\n",
      "Epoch [2/3], Step [35800/138038], Loss: 2.8696, Perplexity: 17.6297\n",
      "Epoch [2/3], Step [35900/138038], Loss: 3.6941, Perplexity: 40.2106\n",
      "Epoch [2/3], Step [36000/138038], Loss: 2.3536, Perplexity: 10.5235\n",
      "Epoch [2/3], Step [36100/138038], Loss: 1.9115, Perplexity: 6.763120\n",
      "Epoch [2/3], Step [36200/138038], Loss: 2.9080, Perplexity: 18.3196\n",
      "Epoch [2/3], Step [36300/138038], Loss: 3.0503, Perplexity: 21.1219\n",
      "Epoch [2/3], Step [36400/138038], Loss: 2.7590, Perplexity: 15.7847\n",
      "Epoch [2/3], Step [36500/138038], Loss: 3.0006, Perplexity: 20.0983\n",
      "Epoch [2/3], Step [36600/138038], Loss: 3.1928, Perplexity: 24.3571\n",
      "Epoch [2/3], Step [36700/138038], Loss: 3.0382, Perplexity: 20.8677\n",
      "Epoch [2/3], Step [36800/138038], Loss: 3.2639, Perplexity: 26.1505\n",
      "Epoch [2/3], Step [36900/138038], Loss: 2.4552, Perplexity: 11.6490\n",
      "Epoch [2/3], Step [37000/138038], Loss: 3.4508, Perplexity: 31.52460\n",
      "Epoch [2/3], Step [37100/138038], Loss: 1.9370, Perplexity: 6.937710\n",
      "Epoch [2/3], Step [37200/138038], Loss: 2.5625, Perplexity: 12.9676\n",
      "Epoch [2/3], Step [37300/138038], Loss: 3.1427, Perplexity: 23.1670\n",
      "Epoch [2/3], Step [37400/138038], Loss: 2.9958, Perplexity: 20.00071\n",
      "Epoch [2/3], Step [37500/138038], Loss: 2.9141, Perplexity: 18.4320\n",
      "Epoch [2/3], Step [37600/138038], Loss: 3.9865, Perplexity: 53.86469\n",
      "Epoch [2/3], Step [37700/138038], Loss: 3.3724, Perplexity: 29.14745\n",
      "Epoch [2/3], Step [37800/138038], Loss: 3.4813, Perplexity: 32.5017\n",
      "Epoch [2/3], Step [37900/138038], Loss: 1.9925, Perplexity: 7.33417\n",
      "Epoch [2/3], Step [38000/138038], Loss: 3.7000, Perplexity: 40.4454\n",
      "Epoch [2/3], Step [38100/138038], Loss: 2.4643, Perplexity: 11.75497\n",
      "Epoch [2/3], Step [38200/138038], Loss: 3.5199, Perplexity: 33.78211\n",
      "Epoch [2/3], Step [38300/138038], Loss: 2.7185, Perplexity: 15.1571\n",
      "Epoch [2/3], Step [38400/138038], Loss: 2.6350, Perplexity: 13.94373\n",
      "Epoch [2/3], Step [38500/138038], Loss: 2.6788, Perplexity: 14.56788\n",
      "Epoch [2/3], Step [38600/138038], Loss: 2.9652, Perplexity: 19.39869\n",
      "Epoch [2/3], Step [38700/138038], Loss: 2.8710, Perplexity: 17.6543\n",
      "Epoch [2/3], Step [38800/138038], Loss: 2.9796, Perplexity: 19.6796\n",
      "Epoch [2/3], Step [38900/138038], Loss: 2.7129, Perplexity: 15.0723\n",
      "Epoch [2/3], Step [39000/138038], Loss: 2.1575, Perplexity: 8.64967\n",
      "Epoch [2/3], Step [39100/138038], Loss: 2.5780, Perplexity: 13.1713\n",
      "Epoch [2/3], Step [39200/138038], Loss: 2.7588, Perplexity: 15.7812\n",
      "Epoch [2/3], Step [39300/138038], Loss: 2.7137, Perplexity: 15.0856\n",
      "Epoch [2/3], Step [39400/138038], Loss: 2.9085, Perplexity: 18.33003\n",
      "Epoch [2/3], Step [39500/138038], Loss: 2.3377, Perplexity: 10.35753\n",
      "Epoch [2/3], Step [39600/138038], Loss: 2.4367, Perplexity: 11.4351\n",
      "Epoch [2/3], Step [39700/138038], Loss: 4.2260, Perplexity: 68.44615\n",
      "Epoch [2/3], Step [39800/138038], Loss: 2.3582, Perplexity: 10.57222\n",
      "Epoch [2/3], Step [39900/138038], Loss: 2.5129, Perplexity: 12.3412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [40000/138038], Loss: 3.1610, Perplexity: 23.5937\n",
      "Epoch [2/3], Step [40100/138038], Loss: 3.5896, Perplexity: 36.2183\n",
      "Epoch [2/3], Step [40200/138038], Loss: 3.5441, Perplexity: 34.6088\n",
      "Epoch [2/3], Step [40300/138038], Loss: 2.2944, Perplexity: 9.918609\n",
      "Epoch [2/3], Step [40400/138038], Loss: 2.4163, Perplexity: 11.2048\n",
      "Epoch [2/3], Step [40500/138038], Loss: 2.0583, Perplexity: 7.832642\n",
      "Epoch [2/3], Step [40600/138038], Loss: 3.0159, Perplexity: 20.4070\n",
      "Epoch [2/3], Step [40700/138038], Loss: 3.3462, Perplexity: 28.3958\n",
      "Epoch [2/3], Step [40800/138038], Loss: 4.2988, Perplexity: 73.6095\n",
      "Epoch [2/3], Step [40900/138038], Loss: 2.8282, Perplexity: 16.91421\n",
      "Epoch [2/3], Step [41000/138038], Loss: 2.8749, Perplexity: 17.7237\n",
      "Epoch [2/3], Step [41100/138038], Loss: 3.0457, Perplexity: 21.0237\n",
      "Epoch [2/3], Step [41200/138038], Loss: 2.8078, Perplexity: 16.57354\n",
      "Epoch [2/3], Step [41300/138038], Loss: 2.3266, Perplexity: 10.2426\n",
      "Epoch [2/3], Step [41400/138038], Loss: 3.2826, Perplexity: 26.64447\n",
      "Epoch [2/3], Step [41500/138038], Loss: 3.3270, Perplexity: 27.8536\n",
      "Epoch [2/3], Step [41600/138038], Loss: 2.2482, Perplexity: 9.47060\n",
      "Epoch [2/3], Step [41700/138038], Loss: 3.4964, Perplexity: 32.99529\n",
      "Epoch [2/3], Step [41800/138038], Loss: 4.6500, Perplexity: 104.5843\n",
      "Epoch [2/3], Step [41900/138038], Loss: 2.8633, Perplexity: 17.52007\n",
      "Epoch [2/3], Step [42000/138038], Loss: 2.7193, Perplexity: 15.16937\n",
      "Epoch [2/3], Step [42100/138038], Loss: 1.7532, Perplexity: 5.773212\n",
      "Epoch [2/3], Step [42200/138038], Loss: 2.6137, Perplexity: 13.6492\n",
      "Epoch [2/3], Step [42300/138038], Loss: 3.1263, Perplexity: 22.7905\n",
      "Epoch [2/3], Step [42400/138038], Loss: 2.2530, Perplexity: 9.51620\n",
      "Epoch [2/3], Step [42500/138038], Loss: 3.3829, Perplexity: 29.4572\n",
      "Epoch [2/3], Step [42600/138038], Loss: 2.7045, Perplexity: 14.9470\n",
      "Epoch [2/3], Step [42700/138038], Loss: 3.1550, Perplexity: 23.45274\n",
      "Epoch [2/3], Step [42800/138038], Loss: 3.3116, Perplexity: 27.4297\n",
      "Epoch [2/3], Step [42900/138038], Loss: 3.2200, Perplexity: 25.02761\n",
      "Epoch [2/3], Step [43000/138038], Loss: 3.0486, Perplexity: 21.08661\n",
      "Epoch [2/3], Step [43100/138038], Loss: 2.7479, Perplexity: 15.6094\n",
      "Epoch [2/3], Step [43200/138038], Loss: 2.3541, Perplexity: 10.52884\n",
      "Epoch [2/3], Step [43300/138038], Loss: 2.1590, Perplexity: 8.66276\n",
      "Epoch [2/3], Step [43400/138038], Loss: 2.3124, Perplexity: 10.09907\n",
      "Epoch [2/3], Step [43500/138038], Loss: 3.1458, Perplexity: 23.2376\n",
      "Epoch [2/3], Step [43600/138038], Loss: 3.0183, Perplexity: 20.45595\n",
      "Epoch [2/3], Step [43700/138038], Loss: 2.3740, Perplexity: 10.74053\n",
      "Epoch [2/3], Step [43800/138038], Loss: 2.1389, Perplexity: 8.490539\n",
      "Epoch [2/3], Step [43900/138038], Loss: 2.6007, Perplexity: 13.4733\n",
      "Epoch [2/3], Step [44000/138038], Loss: 2.4071, Perplexity: 11.1013\n",
      "Epoch [2/3], Step [44100/138038], Loss: 2.2372, Perplexity: 9.367223\n",
      "Epoch [2/3], Step [44200/138038], Loss: 2.9448, Perplexity: 19.00603\n",
      "Epoch [2/3], Step [44300/138038], Loss: 2.7904, Perplexity: 16.2872\n",
      "Epoch [2/3], Step [44400/138038], Loss: 1.6944, Perplexity: 5.443561\n",
      "Epoch [2/3], Step [44500/138038], Loss: 2.8418, Perplexity: 17.1460\n",
      "Epoch [2/3], Step [44600/138038], Loss: 3.6587, Perplexity: 38.8128\n",
      "Epoch [2/3], Step [44700/138038], Loss: 3.1538, Perplexity: 23.42494\n",
      "Epoch [2/3], Step [44800/138038], Loss: 3.3080, Perplexity: 27.33100\n",
      "Epoch [2/3], Step [44900/138038], Loss: 1.9973, Perplexity: 7.36946\n",
      "Epoch [2/3], Step [45000/138038], Loss: 3.5114, Perplexity: 33.49652\n",
      "Epoch [2/3], Step [45100/138038], Loss: 2.9544, Perplexity: 19.19112\n",
      "Epoch [2/3], Step [45200/138038], Loss: 2.9416, Perplexity: 18.94683\n",
      "Epoch [2/3], Step [45300/138038], Loss: 2.9322, Perplexity: 18.76920\n",
      "Epoch [2/3], Step [45400/138038], Loss: 3.1791, Perplexity: 24.0251\n",
      "Epoch [2/3], Step [45500/138038], Loss: 3.6889, Perplexity: 39.9989\n",
      "Epoch [2/3], Step [45600/138038], Loss: 2.9252, Perplexity: 18.63829\n",
      "Epoch [2/3], Step [45700/138038], Loss: 2.4601, Perplexity: 11.7059\n",
      "Epoch [2/3], Step [45800/138038], Loss: 2.3545, Perplexity: 10.53336\n",
      "Epoch [2/3], Step [45900/138038], Loss: 2.2202, Perplexity: 9.20934\n",
      "Epoch [2/3], Step [46000/138038], Loss: 3.0520, Perplexity: 21.1569\n",
      "Epoch [2/3], Step [46100/138038], Loss: 2.4820, Perplexity: 11.9654\n",
      "Epoch [2/3], Step [46200/138038], Loss: 2.8334, Perplexity: 17.00295\n",
      "Epoch [2/3], Step [46300/138038], Loss: 3.3213, Perplexity: 27.6973\n",
      "Epoch [2/3], Step [46400/138038], Loss: 2.6259, Perplexity: 13.8171\n",
      "Epoch [2/3], Step [46500/138038], Loss: 2.3212, Perplexity: 10.1878\n",
      "Epoch [2/3], Step [46600/138038], Loss: 2.5367, Perplexity: 12.6378\n",
      "Epoch [2/3], Step [46700/138038], Loss: 3.0926, Perplexity: 22.0333\n",
      "Epoch [2/3], Step [46800/138038], Loss: 3.2865, Perplexity: 26.74819\n",
      "Epoch [2/3], Step [46900/138038], Loss: 3.1045, Perplexity: 22.2991\n",
      "Epoch [2/3], Step [47000/138038], Loss: 2.1858, Perplexity: 8.897849\n",
      "Epoch [2/3], Step [47100/138038], Loss: 2.9174, Perplexity: 18.4935\n",
      "Epoch [2/3], Step [47200/138038], Loss: 3.0963, Perplexity: 22.1159\n",
      "Epoch [2/3], Step [47300/138038], Loss: 2.8620, Perplexity: 17.49598\n",
      "Epoch [2/3], Step [47400/138038], Loss: 2.4408, Perplexity: 11.4824\n",
      "Epoch [2/3], Step [47500/138038], Loss: 2.0904, Perplexity: 8.08852\n",
      "Epoch [2/3], Step [47600/138038], Loss: 2.1530, Perplexity: 8.610882\n",
      "Epoch [2/3], Step [47700/138038], Loss: 2.2517, Perplexity: 9.503902\n",
      "Epoch [2/3], Step [47800/138038], Loss: 2.7010, Perplexity: 14.8947\n",
      "Epoch [2/3], Step [47900/138038], Loss: 2.3312, Perplexity: 10.29019\n",
      "Epoch [2/3], Step [48000/138038], Loss: 1.8193, Perplexity: 6.16759\n",
      "Epoch [2/3], Step [48100/138038], Loss: 1.9833, Perplexity: 7.266914\n",
      "Epoch [2/3], Step [48200/138038], Loss: 2.5985, Perplexity: 13.44414\n",
      "Epoch [2/3], Step [48300/138038], Loss: 2.6362, Perplexity: 13.95941\n",
      "Epoch [2/3], Step [48400/138038], Loss: 2.2184, Perplexity: 9.192793\n",
      "Epoch [2/3], Step [48500/138038], Loss: 3.3016, Perplexity: 27.1571\n",
      "Epoch [2/3], Step [48600/138038], Loss: 2.9269, Perplexity: 18.6691\n",
      "Epoch [2/3], Step [48700/138038], Loss: 2.1904, Perplexity: 8.93879\n",
      "Epoch [2/3], Step [48800/138038], Loss: 2.6166, Perplexity: 13.68917\n",
      "Epoch [2/3], Step [48900/138038], Loss: 4.8870, Perplexity: 132.5487\n",
      "Epoch [2/3], Step [49000/138038], Loss: 3.1251, Perplexity: 22.76307\n",
      "Epoch [2/3], Step [49100/138038], Loss: 3.4345, Perplexity: 31.01453\n",
      "Epoch [2/3], Step [49200/138038], Loss: 3.4918, Perplexity: 32.8461\n",
      "Epoch [2/3], Step [49300/138038], Loss: 3.5746, Perplexity: 35.6816\n",
      "Epoch [2/3], Step [49400/138038], Loss: 2.7926, Perplexity: 16.3235\n",
      "Epoch [2/3], Step [49500/138038], Loss: 2.1428, Perplexity: 8.52368\n",
      "Epoch [2/3], Step [49600/138038], Loss: 2.5401, Perplexity: 12.6814\n",
      "Epoch [2/3], Step [49700/138038], Loss: 4.0645, Perplexity: 58.2361\n",
      "Epoch [2/3], Step [49800/138038], Loss: 3.1937, Perplexity: 24.3778\n",
      "Epoch [2/3], Step [49900/138038], Loss: 2.1524, Perplexity: 8.60526\n",
      "Epoch [2/3], Step [50000/138038], Loss: 2.3187, Perplexity: 10.1625\n",
      "Epoch [2/3], Step [50100/138038], Loss: 2.8524, Perplexity: 17.3295\n",
      "Epoch [2/3], Step [50200/138038], Loss: 2.5766, Perplexity: 13.15292\n",
      "Epoch [2/3], Step [50300/138038], Loss: 2.8102, Perplexity: 16.61274\n",
      "Epoch [2/3], Step [50400/138038], Loss: 3.0999, Perplexity: 22.19660\n",
      "Epoch [2/3], Step [50500/138038], Loss: 3.0782, Perplexity: 21.71894\n",
      "Epoch [2/3], Step [50600/138038], Loss: 3.2648, Perplexity: 26.17517\n",
      "Epoch [2/3], Step [50700/138038], Loss: 2.9621, Perplexity: 19.3386\n",
      "Epoch [2/3], Step [50800/138038], Loss: 2.0595, Perplexity: 7.84186\n",
      "Epoch [2/3], Step [50900/138038], Loss: 2.5152, Perplexity: 12.3695\n",
      "Epoch [2/3], Step [51000/138038], Loss: 2.0599, Perplexity: 7.845506\n",
      "Epoch [2/3], Step [51100/138038], Loss: 2.4905, Perplexity: 12.06724\n",
      "Epoch [2/3], Step [51200/138038], Loss: 5.0382, Perplexity: 154.1859\n",
      "Epoch [2/3], Step [51300/138038], Loss: 2.9663, Perplexity: 19.4209\n",
      "Epoch [2/3], Step [51400/138038], Loss: 2.2775, Perplexity: 9.75247\n",
      "Epoch [2/3], Step [51500/138038], Loss: 3.2473, Perplexity: 25.7216\n",
      "Epoch [2/3], Step [51600/138038], Loss: 2.5285, Perplexity: 12.5346\n",
      "Epoch [2/3], Step [51700/138038], Loss: 3.3049, Perplexity: 27.2452\n",
      "Epoch [2/3], Step [51800/138038], Loss: 2.1343, Perplexity: 8.45104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [51900/138038], Loss: 3.1727, Perplexity: 23.8727\n",
      "Epoch [2/3], Step [52000/138038], Loss: 2.4150, Perplexity: 11.1894\n",
      "Epoch [2/3], Step [52100/138038], Loss: 3.6569, Perplexity: 38.74302\n",
      "Epoch [2/3], Step [52200/138038], Loss: 1.8380, Perplexity: 6.284199\n",
      "Epoch [2/3], Step [52300/138038], Loss: 2.0100, Perplexity: 7.46363\n",
      "Epoch [2/3], Step [52400/138038], Loss: 2.1368, Perplexity: 8.47238\n",
      "Epoch [2/3], Step [52500/138038], Loss: 2.5437, Perplexity: 12.7267\n",
      "Epoch [2/3], Step [52600/138038], Loss: 2.9068, Perplexity: 18.2978\n",
      "Epoch [2/3], Step [52700/138038], Loss: 3.3521, Perplexity: 28.5620\n",
      "Epoch [2/3], Step [52800/138038], Loss: 2.8746, Perplexity: 17.71799\n",
      "Epoch [2/3], Step [52900/138038], Loss: 3.4780, Perplexity: 32.3942\n",
      "Epoch [2/3], Step [53000/138038], Loss: 3.4525, Perplexity: 31.5803\n",
      "Epoch [2/3], Step [53100/138038], Loss: 3.0722, Perplexity: 21.5894\n",
      "Epoch [2/3], Step [53200/138038], Loss: 2.3660, Perplexity: 10.6545\n",
      "Epoch [2/3], Step [53300/138038], Loss: 5.0989, Perplexity: 163.8417\n",
      "Epoch [2/3], Step [53400/138038], Loss: 3.0902, Perplexity: 21.9815\n",
      "Epoch [2/3], Step [53500/138038], Loss: 2.4336, Perplexity: 11.39954\n",
      "Epoch [2/3], Step [53600/138038], Loss: 2.8866, Perplexity: 17.9327\n",
      "Epoch [2/3], Step [53700/138038], Loss: 3.5261, Perplexity: 33.9923\n",
      "Epoch [2/3], Step [53800/138038], Loss: 3.3196, Perplexity: 27.6498\n",
      "Epoch [2/3], Step [53900/138038], Loss: 1.7751, Perplexity: 5.900953\n",
      "Epoch [2/3], Step [54000/138038], Loss: 2.6411, Perplexity: 14.0284\n",
      "Epoch [2/3], Step [54100/138038], Loss: 3.3160, Perplexity: 27.55029\n",
      "Epoch [2/3], Step [54200/138038], Loss: 2.3214, Perplexity: 10.1901\n",
      "Epoch [2/3], Step [54300/138038], Loss: 2.7913, Perplexity: 16.3025\n",
      "Epoch [2/3], Step [54400/138038], Loss: 2.0886, Perplexity: 8.07389\n",
      "Epoch [2/3], Step [54500/138038], Loss: 2.9105, Perplexity: 18.3653\n",
      "Epoch [2/3], Step [54600/138038], Loss: 2.5783, Perplexity: 13.17549\n",
      "Epoch [2/3], Step [54700/138038], Loss: 3.1661, Perplexity: 23.7146\n",
      "Epoch [2/3], Step [54800/138038], Loss: 2.1391, Perplexity: 8.49216\n",
      "Epoch [2/3], Step [54900/138038], Loss: 2.5921, Perplexity: 13.3577\n",
      "Epoch [2/3], Step [55000/138038], Loss: 3.0721, Perplexity: 21.58648\n",
      "Epoch [2/3], Step [55100/138038], Loss: 3.3260, Perplexity: 27.82609\n",
      "Epoch [2/3], Step [55200/138038], Loss: 2.8369, Perplexity: 17.0627\n",
      "Epoch [2/3], Step [55300/138038], Loss: 2.4579, Perplexity: 11.68074\n",
      "Epoch [2/3], Step [55400/138038], Loss: 2.9600, Perplexity: 19.2985\n",
      "Epoch [2/3], Step [55500/138038], Loss: 2.2243, Perplexity: 9.24698\n",
      "Epoch [2/3], Step [55600/138038], Loss: 2.6513, Perplexity: 14.17251\n",
      "Epoch [2/3], Step [55700/138038], Loss: 2.3739, Perplexity: 10.7387\n",
      "Epoch [2/3], Step [55800/138038], Loss: 2.6054, Perplexity: 13.53713\n",
      "Epoch [2/3], Step [55900/138038], Loss: 4.3935, Perplexity: 80.9243\n",
      "Epoch [2/3], Step [56000/138038], Loss: 3.2575, Perplexity: 25.98543\n",
      "Epoch [2/3], Step [56100/138038], Loss: 3.2534, Perplexity: 25.87762\n",
      "Epoch [2/3], Step [56200/138038], Loss: 2.2265, Perplexity: 9.26751\n",
      "Epoch [2/3], Step [56300/138038], Loss: 2.9279, Perplexity: 18.6880\n",
      "Epoch [2/3], Step [56400/138038], Loss: 1.7534, Perplexity: 5.77420\n",
      "Epoch [2/3], Step [56500/138038], Loss: 3.4383, Perplexity: 31.13489\n",
      "Epoch [2/3], Step [56600/138038], Loss: 2.7854, Perplexity: 16.20578\n",
      "Epoch [2/3], Step [56700/138038], Loss: 2.8133, Perplexity: 16.6644\n",
      "Epoch [2/3], Step [56800/138038], Loss: 3.3233, Perplexity: 27.7531\n",
      "Epoch [2/3], Step [56900/138038], Loss: 2.3516, Perplexity: 10.50199\n",
      "Epoch [2/3], Step [57000/138038], Loss: 2.4168, Perplexity: 11.2100\n",
      "Epoch [2/3], Step [57100/138038], Loss: 2.7403, Perplexity: 15.49135\n",
      "Epoch [2/3], Step [57200/138038], Loss: 3.8160, Perplexity: 45.42245\n",
      "Epoch [2/3], Step [57300/138038], Loss: 2.0207, Perplexity: 7.54341\n",
      "Epoch [2/3], Step [57400/138038], Loss: 2.3414, Perplexity: 10.3958\n",
      "Epoch [2/3], Step [57500/138038], Loss: 2.3295, Perplexity: 10.2725\n",
      "Epoch [2/3], Step [57600/138038], Loss: 1.9190, Perplexity: 6.81440\n",
      "Epoch [2/3], Step [57700/138038], Loss: 3.0544, Perplexity: 21.2086\n",
      "Epoch [2/3], Step [57800/138038], Loss: 2.9495, Perplexity: 19.0962\n",
      "Epoch [2/3], Step [57900/138038], Loss: 2.0005, Perplexity: 7.392714\n",
      "Epoch [2/3], Step [58000/138038], Loss: 2.8586, Perplexity: 17.4372\n",
      "Epoch [2/3], Step [58100/138038], Loss: 2.4038, Perplexity: 11.0653\n",
      "Epoch [2/3], Step [58200/138038], Loss: 2.0428, Perplexity: 7.71216\n",
      "Epoch [2/3], Step [58300/138038], Loss: 2.2983, Perplexity: 9.95680\n",
      "Epoch [2/3], Step [58400/138038], Loss: 2.2214, Perplexity: 9.22041\n",
      "Epoch [2/3], Step [58500/138038], Loss: 2.0423, Perplexity: 7.708108\n",
      "Epoch [2/3], Step [58600/138038], Loss: 2.4729, Perplexity: 11.8569\n",
      "Epoch [2/3], Step [58700/138038], Loss: 4.4899, Perplexity: 89.11398\n",
      "Epoch [2/3], Step [58800/138038], Loss: 1.9826, Perplexity: 7.261857\n",
      "Epoch [2/3], Step [58900/138038], Loss: 3.7074, Perplexity: 40.74944\n",
      "Epoch [2/3], Step [59000/138038], Loss: 3.2948, Perplexity: 26.9730\n",
      "Epoch [2/3], Step [59100/138038], Loss: 2.1886, Perplexity: 8.92282\n",
      "Epoch [2/3], Step [59200/138038], Loss: 2.2668, Perplexity: 9.648976\n",
      "Epoch [2/3], Step [59300/138038], Loss: 4.1369, Perplexity: 62.60959\n",
      "Epoch [2/3], Step [59400/138038], Loss: 2.6644, Perplexity: 14.35994\n",
      "Epoch [2/3], Step [59500/138038], Loss: 2.9888, Perplexity: 19.8624\n",
      "Epoch [2/3], Step [59600/138038], Loss: 3.4562, Perplexity: 31.69518\n",
      "Epoch [2/3], Step [59700/138038], Loss: 2.9979, Perplexity: 20.04370\n",
      "Epoch [2/3], Step [59800/138038], Loss: 3.0486, Perplexity: 21.0864\n",
      "Epoch [2/3], Step [59900/138038], Loss: 2.9744, Perplexity: 19.5782\n",
      "Epoch [2/3], Step [60000/138038], Loss: 1.5777, Perplexity: 4.84375\n",
      "Epoch [2/3], Step [60100/138038], Loss: 3.5591, Perplexity: 35.1322\n",
      "Epoch [2/3], Step [60200/138038], Loss: 2.4181, Perplexity: 11.2241\n",
      "Epoch [2/3], Step [60300/138038], Loss: 1.8987, Perplexity: 6.677184\n",
      "Epoch [2/3], Step [60400/138038], Loss: 3.6237, Perplexity: 37.4765\n",
      "Epoch [2/3], Step [60500/138038], Loss: 2.1004, Perplexity: 8.16944\n",
      "Epoch [2/3], Step [60600/138038], Loss: 2.3429, Perplexity: 10.41189\n",
      "Epoch [2/3], Step [60700/138038], Loss: 1.9514, Perplexity: 7.038775\n",
      "Epoch [2/3], Step [60800/138038], Loss: 2.4002, Perplexity: 11.02577\n",
      "Epoch [2/3], Step [60900/138038], Loss: 3.1409, Perplexity: 23.12380\n",
      "Epoch [2/3], Step [61000/138038], Loss: 1.5434, Perplexity: 4.68045\n",
      "Epoch [2/3], Step [61100/138038], Loss: 3.2006, Perplexity: 24.5464\n",
      "Epoch [2/3], Step [61200/138038], Loss: 2.1936, Perplexity: 8.967270\n",
      "Epoch [2/3], Step [61300/138038], Loss: 2.7898, Perplexity: 16.2781\n",
      "Epoch [2/3], Step [61400/138038], Loss: 2.0595, Perplexity: 7.842068\n",
      "Epoch [2/3], Step [61500/138038], Loss: 1.8989, Perplexity: 6.67879\n",
      "Epoch [2/3], Step [61600/138038], Loss: 2.9899, Perplexity: 19.88460\n",
      "Epoch [2/3], Step [61700/138038], Loss: 3.1992, Perplexity: 24.5117\n",
      "Epoch [2/3], Step [61800/138038], Loss: 3.7001, Perplexity: 40.4518\n",
      "Epoch [2/3], Step [61900/138038], Loss: 3.3112, Perplexity: 27.4172\n",
      "Epoch [2/3], Step [62000/138038], Loss: 3.7251, Perplexity: 41.4754\n",
      "Epoch [2/3], Step [62100/138038], Loss: 3.5441, Perplexity: 34.6101\n",
      "Epoch [2/3], Step [62200/138038], Loss: 2.5422, Perplexity: 12.70701\n",
      "Epoch [2/3], Step [62300/138038], Loss: 2.9052, Perplexity: 18.2687\n",
      "Epoch [2/3], Step [62400/138038], Loss: 2.8366, Perplexity: 17.0578\n",
      "Epoch [2/3], Step [62500/138038], Loss: 2.4714, Perplexity: 11.83865\n",
      "Epoch [2/3], Step [62600/138038], Loss: 2.8001, Perplexity: 16.44578\n",
      "Epoch [2/3], Step [62700/138038], Loss: 2.6657, Perplexity: 14.3778\n",
      "Epoch [2/3], Step [62800/138038], Loss: 4.1277, Perplexity: 62.0361\n",
      "Epoch [2/3], Step [62900/138038], Loss: 2.6915, Perplexity: 14.7533\n",
      "Epoch [2/3], Step [63000/138038], Loss: 1.8525, Perplexity: 6.37572\n",
      "Epoch [2/3], Step [63100/138038], Loss: 2.2173, Perplexity: 9.18258\n",
      "Epoch [2/3], Step [63200/138038], Loss: 3.9914, Perplexity: 54.1326\n",
      "Epoch [2/3], Step [63300/138038], Loss: 2.8992, Perplexity: 18.1594\n",
      "Epoch [2/3], Step [63400/138038], Loss: 2.6301, Perplexity: 13.87499\n",
      "Epoch [2/3], Step [63500/138038], Loss: 3.6923, Perplexity: 40.1386\n",
      "Epoch [2/3], Step [63600/138038], Loss: 2.4820, Perplexity: 11.9646\n",
      "Epoch [2/3], Step [63700/138038], Loss: 3.0765, Perplexity: 21.6830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [63800/138038], Loss: 3.3243, Perplexity: 27.7783\n",
      "Epoch [2/3], Step [63900/138038], Loss: 2.9044, Perplexity: 18.2535\n",
      "Epoch [2/3], Step [64000/138038], Loss: 2.7361, Perplexity: 15.4271\n",
      "Epoch [2/3], Step [64100/138038], Loss: 1.8469, Perplexity: 6.34045\n",
      "Epoch [2/3], Step [64200/138038], Loss: 1.9841, Perplexity: 7.27284\n",
      "Epoch [2/3], Step [64300/138038], Loss: 2.4224, Perplexity: 11.2728\n",
      "Epoch [2/3], Step [64400/138038], Loss: 1.6498, Perplexity: 5.20607\n",
      "Epoch [2/3], Step [64500/138038], Loss: 2.6510, Perplexity: 14.16830\n",
      "Epoch [2/3], Step [64600/138038], Loss: 1.3881, Perplexity: 4.007339\n",
      "Epoch [2/3], Step [64700/138038], Loss: 2.7427, Perplexity: 15.5281\n",
      "Epoch [2/3], Step [64800/138038], Loss: 4.4128, Perplexity: 82.50020\n",
      "Epoch [2/3], Step [64900/138038], Loss: 3.3759, Perplexity: 29.2513\n",
      "Epoch [2/3], Step [65000/138038], Loss: 2.4905, Perplexity: 12.06700\n",
      "Epoch [2/3], Step [65100/138038], Loss: 1.9391, Perplexity: 6.95276\n",
      "Epoch [2/3], Step [65200/138038], Loss: 3.1675, Perplexity: 23.7488\n",
      "Epoch [2/3], Step [65300/138038], Loss: 2.3879, Perplexity: 10.89089\n",
      "Epoch [2/3], Step [65400/138038], Loss: 1.7272, Perplexity: 5.62517\n",
      "Epoch [2/3], Step [65500/138038], Loss: 3.5534, Perplexity: 34.9309\n",
      "Epoch [2/3], Step [65600/138038], Loss: 2.5068, Perplexity: 12.2660\n",
      "Epoch [2/3], Step [65700/138038], Loss: 1.7399, Perplexity: 5.697024\n",
      "Epoch [2/3], Step [65800/138038], Loss: 2.1631, Perplexity: 8.69809\n",
      "Epoch [2/3], Step [65900/138038], Loss: 2.7680, Perplexity: 15.9272\n",
      "Epoch [2/3], Step [66000/138038], Loss: 2.7176, Perplexity: 15.1439\n",
      "Epoch [2/3], Step [66100/138038], Loss: 2.1263, Perplexity: 8.383793\n",
      "Epoch [2/3], Step [66200/138038], Loss: 3.0774, Perplexity: 21.70107\n",
      "Epoch [2/3], Step [66300/138038], Loss: 1.4833, Perplexity: 4.40773\n",
      "Epoch [2/3], Step [66400/138038], Loss: 2.7648, Perplexity: 15.8755\n",
      "Epoch [2/3], Step [66500/138038], Loss: 2.6763, Perplexity: 14.53095\n",
      "Epoch [2/3], Step [66600/138038], Loss: 2.3764, Perplexity: 10.76647\n",
      "Epoch [2/3], Step [66700/138038], Loss: 2.6626, Perplexity: 14.33301\n",
      "Epoch [2/3], Step [66800/138038], Loss: 2.9935, Perplexity: 19.95634\n",
      "Epoch [2/3], Step [66900/138038], Loss: 2.6042, Perplexity: 13.52117\n",
      "Epoch [2/3], Step [67000/138038], Loss: 3.2063, Perplexity: 24.6885\n",
      "Epoch [2/3], Step [67100/138038], Loss: 2.5357, Perplexity: 12.62480\n",
      "Epoch [2/3], Step [67200/138038], Loss: 4.1793, Perplexity: 65.32291\n",
      "Epoch [2/3], Step [67300/138038], Loss: 2.3172, Perplexity: 10.14707\n",
      "Epoch [2/3], Step [67400/138038], Loss: 2.4831, Perplexity: 11.97829\n",
      "Epoch [2/3], Step [67500/138038], Loss: 2.4589, Perplexity: 11.69158\n",
      "Epoch [2/3], Step [67600/138038], Loss: 2.3208, Perplexity: 10.18377\n",
      "Epoch [2/3], Step [67700/138038], Loss: 3.3831, Perplexity: 29.4622\n",
      "Epoch [2/3], Step [67800/138038], Loss: 2.6640, Perplexity: 14.3529\n",
      "Epoch [2/3], Step [67900/138038], Loss: 2.7266, Perplexity: 15.2806\n",
      "Epoch [2/3], Step [68000/138038], Loss: 3.8019, Perplexity: 44.7847\n",
      "Epoch [2/3], Step [68100/138038], Loss: 2.7970, Perplexity: 16.3947\n",
      "Epoch [2/3], Step [68200/138038], Loss: 2.8679, Perplexity: 17.6001\n",
      "Epoch [2/3], Step [68300/138038], Loss: 3.8999, Perplexity: 49.39521\n",
      "Epoch [2/3], Step [68400/138038], Loss: 2.6311, Perplexity: 13.8888\n",
      "Epoch [2/3], Step [68500/138038], Loss: 2.7982, Perplexity: 16.4150\n",
      "Epoch [2/3], Step [68600/138038], Loss: 2.1862, Perplexity: 8.90131\n",
      "Epoch [2/3], Step [68700/138038], Loss: 3.1108, Perplexity: 22.4381\n",
      "Epoch [2/3], Step [68800/138038], Loss: 2.8297, Perplexity: 16.9409\n",
      "Epoch [2/3], Step [68900/138038], Loss: 2.3971, Perplexity: 10.9914\n",
      "Epoch [2/3], Step [69000/138038], Loss: 3.0324, Perplexity: 20.74809\n",
      "Epoch [2/3], Step [69100/138038], Loss: 2.2608, Perplexity: 9.59047\n",
      "Epoch [2/3], Step [69200/138038], Loss: 2.4669, Perplexity: 11.78532\n",
      "Epoch [2/3], Step [69300/138038], Loss: 2.9023, Perplexity: 18.2160\n",
      "Epoch [2/3], Step [69400/138038], Loss: 1.8311, Perplexity: 6.240596\n",
      "Epoch [2/3], Step [69500/138038], Loss: 2.7630, Perplexity: 15.8471\n",
      "Epoch [2/3], Step [69600/138038], Loss: 2.2650, Perplexity: 9.631335\n",
      "Epoch [2/3], Step [69700/138038], Loss: 2.2465, Perplexity: 9.45496\n",
      "Epoch [2/3], Step [69800/138038], Loss: 3.1115, Perplexity: 22.4536\n",
      "Epoch [2/3], Step [69900/138038], Loss: 2.3278, Perplexity: 10.2555\n",
      "Epoch [2/3], Step [70000/138038], Loss: 2.7339, Perplexity: 15.3934\n",
      "Epoch [2/3], Step [70100/138038], Loss: 2.3234, Perplexity: 10.2103\n",
      "Epoch [2/3], Step [70200/138038], Loss: 2.2363, Perplexity: 9.358574\n",
      "Epoch [2/3], Step [70300/138038], Loss: 2.5898, Perplexity: 13.3269\n",
      "Epoch [2/3], Step [70400/138038], Loss: 2.2777, Perplexity: 9.754355\n",
      "Epoch [2/3], Step [70500/138038], Loss: 2.3724, Perplexity: 10.7232\n",
      "Epoch [2/3], Step [70600/138038], Loss: 3.2310, Perplexity: 25.30393\n",
      "Epoch [2/3], Step [70700/138038], Loss: 2.8487, Perplexity: 17.2655\n",
      "Epoch [2/3], Step [70800/138038], Loss: 2.5503, Perplexity: 12.8111\n",
      "Epoch [2/3], Step [70900/138038], Loss: 2.7455, Perplexity: 15.5720\n",
      "Epoch [2/3], Step [71000/138038], Loss: 3.1628, Perplexity: 23.6378\n",
      "Epoch [2/3], Step [71100/138038], Loss: 2.8252, Perplexity: 16.8640\n",
      "Epoch [2/3], Step [71200/138038], Loss: 1.5343, Perplexity: 4.638258\n",
      "Epoch [2/3], Step [71300/138038], Loss: 3.1124, Perplexity: 22.47459\n",
      "Epoch [2/3], Step [71400/138038], Loss: 3.8913, Perplexity: 48.9752\n",
      "Epoch [2/3], Step [71500/138038], Loss: 2.4899, Perplexity: 12.0605\n",
      "Epoch [2/3], Step [71600/138038], Loss: 2.4473, Perplexity: 11.5572\n",
      "Epoch [2/3], Step [71700/138038], Loss: 2.6149, Perplexity: 13.66605\n",
      "Epoch [2/3], Step [71800/138038], Loss: 3.8750, Perplexity: 48.18126\n",
      "Epoch [2/3], Step [71900/138038], Loss: 2.1866, Perplexity: 8.90457\n",
      "Epoch [2/3], Step [72000/138038], Loss: 2.9669, Perplexity: 19.43188\n",
      "Epoch [2/3], Step [72100/138038], Loss: 2.1754, Perplexity: 8.80567\n",
      "Epoch [2/3], Step [72200/138038], Loss: 2.8125, Perplexity: 16.6518\n",
      "Epoch [2/3], Step [72300/138038], Loss: 2.4663, Perplexity: 11.77919\n",
      "Epoch [2/3], Step [72400/138038], Loss: 3.7569, Perplexity: 42.8175\n",
      "Epoch [2/3], Step [72500/138038], Loss: 2.3945, Perplexity: 10.9630\n",
      "Epoch [2/3], Step [72600/138038], Loss: 2.8646, Perplexity: 17.54125\n",
      "Epoch [2/3], Step [72700/138038], Loss: 3.0697, Perplexity: 21.53596\n",
      "Epoch [2/3], Step [72800/138038], Loss: 2.4885, Perplexity: 12.04297\n",
      "Epoch [2/3], Step [72900/138038], Loss: 2.7971, Perplexity: 16.39643\n",
      "Epoch [2/3], Step [73000/138038], Loss: 1.8707, Perplexity: 6.493041\n",
      "Epoch [2/3], Step [73100/138038], Loss: 3.1526, Perplexity: 23.39702\n",
      "Epoch [2/3], Step [73200/138038], Loss: 2.7330, Perplexity: 15.3796\n",
      "Epoch [2/3], Step [73300/138038], Loss: 3.5039, Perplexity: 33.2435\n",
      "Epoch [2/3], Step [73400/138038], Loss: 2.2551, Perplexity: 9.53669\n",
      "Epoch [2/3], Step [73500/138038], Loss: 2.1598, Perplexity: 8.66930\n",
      "Epoch [2/3], Step [73600/138038], Loss: 2.9094, Perplexity: 18.34640\n",
      "Epoch [2/3], Step [73700/138038], Loss: 2.2821, Perplexity: 9.79721\n",
      "Epoch [2/3], Step [73800/138038], Loss: 1.6752, Perplexity: 5.34010\n",
      "Epoch [2/3], Step [73900/138038], Loss: 2.5190, Perplexity: 12.41599\n",
      "Epoch [2/3], Step [74000/138038], Loss: 1.8867, Perplexity: 6.597348\n",
      "Epoch [2/3], Step [74100/138038], Loss: 2.3836, Perplexity: 10.8442\n",
      "Epoch [2/3], Step [74200/138038], Loss: 2.9926, Perplexity: 19.93837\n",
      "Epoch [2/3], Step [74300/138038], Loss: 3.2478, Perplexity: 25.7337\n",
      "Epoch [2/3], Step [74400/138038], Loss: 3.8157, Perplexity: 45.4089\n",
      "Epoch [2/3], Step [74500/138038], Loss: 3.1960, Perplexity: 24.43433\n",
      "Epoch [2/3], Step [74600/138038], Loss: 2.0011, Perplexity: 7.39721\n",
      "Epoch [2/3], Step [74700/138038], Loss: 3.2264, Perplexity: 25.1886\n",
      "Epoch [2/3], Step [74800/138038], Loss: 3.2959, Perplexity: 27.00186\n",
      "Epoch [2/3], Step [74900/138038], Loss: 2.9767, Perplexity: 19.6237\n",
      "Epoch [2/3], Step [75000/138038], Loss: 1.6602, Perplexity: 5.26049\n",
      "Epoch [2/3], Step [75100/138038], Loss: 3.0047, Perplexity: 20.18027\n",
      "Epoch [2/3], Step [75200/138038], Loss: 3.2132, Perplexity: 24.85956\n",
      "Epoch [2/3], Step [75300/138038], Loss: 2.2933, Perplexity: 9.90719\n",
      "Epoch [2/3], Step [75400/138038], Loss: 2.3517, Perplexity: 10.5034\n",
      "Epoch [2/3], Step [75500/138038], Loss: 3.0405, Perplexity: 20.9154\n",
      "Epoch [2/3], Step [75600/138038], Loss: 3.0571, Perplexity: 21.2666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [75700/138038], Loss: 2.7706, Perplexity: 15.9680\n",
      "Epoch [2/3], Step [75800/138038], Loss: 2.6734, Perplexity: 14.4897\n",
      "Epoch [2/3], Step [75900/138038], Loss: 2.6888, Perplexity: 14.71384\n",
      "Epoch [2/3], Step [76000/138038], Loss: 2.8849, Perplexity: 17.90179\n",
      "Epoch [2/3], Step [76100/138038], Loss: 3.8090, Perplexity: 45.10560\n",
      "Epoch [2/3], Step [76200/138038], Loss: 2.7534, Perplexity: 15.6964\n",
      "Epoch [2/3], Step [76300/138038], Loss: 2.2824, Perplexity: 9.79978\n",
      "Epoch [2/3], Step [76400/138038], Loss: 3.4624, Perplexity: 31.8929\n",
      "Epoch [2/3], Step [76500/138038], Loss: 1.7757, Perplexity: 5.90457\n",
      "Epoch [2/3], Step [76600/138038], Loss: 2.9664, Perplexity: 19.4226\n",
      "Epoch [2/3], Step [76700/138038], Loss: 2.5998, Perplexity: 13.46102\n",
      "Epoch [2/3], Step [76800/138038], Loss: 3.8585, Perplexity: 47.3940\n",
      "Epoch [2/3], Step [76900/138038], Loss: 2.8424, Perplexity: 17.1568\n",
      "Epoch [2/3], Step [77000/138038], Loss: 3.9868, Perplexity: 53.8802\n",
      "Epoch [2/3], Step [77100/138038], Loss: 2.1739, Perplexity: 8.79296\n",
      "Epoch [2/3], Step [77200/138038], Loss: 2.5446, Perplexity: 12.7385\n",
      "Epoch [2/3], Step [77300/138038], Loss: 2.3943, Perplexity: 10.9600\n",
      "Epoch [2/3], Step [77400/138038], Loss: 3.8890, Perplexity: 48.8626\n",
      "Epoch [2/3], Step [77500/138038], Loss: 3.2371, Perplexity: 25.4606\n",
      "Epoch [2/3], Step [77600/138038], Loss: 1.9221, Perplexity: 6.83568\n",
      "Epoch [2/3], Step [77700/138038], Loss: 2.8308, Perplexity: 16.9594\n",
      "Epoch [2/3], Step [77800/138038], Loss: 2.2795, Perplexity: 9.77216\n",
      "Epoch [2/3], Step [77900/138038], Loss: 2.7580, Perplexity: 15.76837\n",
      "Epoch [2/3], Step [78000/138038], Loss: 3.3683, Perplexity: 29.0306\n",
      "Epoch [2/3], Step [78100/138038], Loss: 2.8781, Perplexity: 17.78096\n",
      "Epoch [2/3], Step [78200/138038], Loss: 3.1121, Perplexity: 22.4692\n",
      "Epoch [2/3], Step [78300/138038], Loss: 3.3618, Perplexity: 28.8422\n",
      "Epoch [2/3], Step [78400/138038], Loss: 3.2796, Perplexity: 26.56425\n",
      "Epoch [2/3], Step [78500/138038], Loss: 2.1761, Perplexity: 8.81206\n",
      "Epoch [2/3], Step [78600/138038], Loss: 2.5916, Perplexity: 13.3508\n",
      "Epoch [2/3], Step [78700/138038], Loss: 2.0490, Perplexity: 7.76019\n",
      "Epoch [2/3], Step [78800/138038], Loss: 3.3896, Perplexity: 29.65377\n",
      "Epoch [2/3], Step [78900/138038], Loss: 3.6726, Perplexity: 39.3527\n",
      "Epoch [2/3], Step [79000/138038], Loss: 2.2465, Perplexity: 9.454805\n",
      "Epoch [2/3], Step [79100/138038], Loss: 3.9163, Perplexity: 50.21595\n",
      "Epoch [2/3], Step [79200/138038], Loss: 2.4251, Perplexity: 11.30340\n",
      "Epoch [2/3], Step [79300/138038], Loss: 3.2623, Perplexity: 26.10966\n",
      "Epoch [2/3], Step [79400/138038], Loss: 2.2419, Perplexity: 9.41166\n",
      "Epoch [2/3], Step [79500/138038], Loss: 1.7438, Perplexity: 5.719322\n",
      "Epoch [2/3], Step [79600/138038], Loss: 3.4476, Perplexity: 31.42549\n",
      "Epoch [2/3], Step [79700/138038], Loss: 2.6800, Perplexity: 14.5849\n",
      "Epoch [2/3], Step [79800/138038], Loss: 2.2883, Perplexity: 9.85818\n",
      "Epoch [2/3], Step [79900/138038], Loss: 2.3650, Perplexity: 10.6443\n",
      "Epoch [2/3], Step [80000/138038], Loss: 2.7593, Perplexity: 15.78879\n",
      "Epoch [2/3], Step [80100/138038], Loss: 3.1761, Perplexity: 23.9537\n",
      "Epoch [2/3], Step [80200/138038], Loss: 1.4492, Perplexity: 4.25983\n",
      "Epoch [2/3], Step [80300/138038], Loss: 1.9351, Perplexity: 6.924961\n",
      "Epoch [2/3], Step [80400/138038], Loss: 3.2595, Perplexity: 26.0370\n",
      "Epoch [2/3], Step [80500/138038], Loss: 2.3334, Perplexity: 10.31271\n",
      "Epoch [2/3], Step [80600/138038], Loss: 3.3808, Perplexity: 29.39520\n",
      "Epoch [2/3], Step [80700/138038], Loss: 2.5982, Perplexity: 13.4392\n",
      "Epoch [2/3], Step [80800/138038], Loss: 3.0398, Perplexity: 20.90012\n",
      "Epoch [2/3], Step [80900/138038], Loss: 2.1573, Perplexity: 8.647858\n",
      "Epoch [2/3], Step [81000/138038], Loss: 3.1552, Perplexity: 23.4587\n",
      "Epoch [2/3], Step [81100/138038], Loss: 3.0082, Perplexity: 20.25162\n",
      "Epoch [2/3], Step [81200/138038], Loss: 4.2311, Perplexity: 68.7912\n",
      "Epoch [2/3], Step [81300/138038], Loss: 2.7350, Perplexity: 15.4102\n",
      "Epoch [2/3], Step [81400/138038], Loss: 2.8138, Perplexity: 16.67360\n",
      "Epoch [2/3], Step [81500/138038], Loss: 1.5524, Perplexity: 4.72286\n",
      "Epoch [2/3], Step [81600/138038], Loss: 1.8197, Perplexity: 6.170229\n",
      "Epoch [2/3], Step [81700/138038], Loss: 3.1499, Perplexity: 23.33427\n",
      "Epoch [2/3], Step [81800/138038], Loss: 2.1146, Perplexity: 8.286558\n",
      "Epoch [2/3], Step [81900/138038], Loss: 3.0439, Perplexity: 20.98773\n",
      "Epoch [2/3], Step [82000/138038], Loss: 1.6740, Perplexity: 5.33323\n",
      "Epoch [2/3], Step [82100/138038], Loss: 3.0038, Perplexity: 20.1621\n",
      "Epoch [2/3], Step [82200/138038], Loss: 1.3378, Perplexity: 3.81069\n",
      "Epoch [2/3], Step [82300/138038], Loss: 2.7345, Perplexity: 15.4022\n",
      "Epoch [2/3], Step [82400/138038], Loss: 2.7014, Perplexity: 14.9002\n",
      "Epoch [2/3], Step [82500/138038], Loss: 2.9024, Perplexity: 18.2179\n",
      "Epoch [2/3], Step [82600/138038], Loss: 3.4943, Perplexity: 32.9266\n",
      "Epoch [2/3], Step [82700/138038], Loss: 2.0345, Perplexity: 7.64862\n",
      "Epoch [2/3], Step [82800/138038], Loss: 3.7946, Perplexity: 44.4615\n",
      "Epoch [2/3], Step [82900/138038], Loss: 3.3569, Perplexity: 28.6987\n",
      "Epoch [2/3], Step [83000/138038], Loss: 2.0545, Perplexity: 7.80319\n",
      "Epoch [2/3], Step [83100/138038], Loss: 1.3839, Perplexity: 3.99040\n",
      "Epoch [2/3], Step [83200/138038], Loss: 3.3942, Perplexity: 29.79075\n",
      "Epoch [2/3], Step [83300/138038], Loss: 3.5587, Perplexity: 35.1175\n",
      "Epoch [2/3], Step [83400/138038], Loss: 2.3333, Perplexity: 10.31166\n",
      "Epoch [2/3], Step [83500/138038], Loss: 2.1448, Perplexity: 8.540331\n",
      "Epoch [2/3], Step [83600/138038], Loss: 2.5509, Perplexity: 12.8182\n",
      "Epoch [2/3], Step [83700/138038], Loss: 1.6527, Perplexity: 5.22127\n",
      "Epoch [2/3], Step [83800/138038], Loss: 2.5314, Perplexity: 12.57137\n",
      "Epoch [2/3], Step [83900/138038], Loss: 3.1602, Perplexity: 23.5758\n",
      "Epoch [2/3], Step [84000/138038], Loss: 2.5583, Perplexity: 12.9132\n",
      "Epoch [2/3], Step [84100/138038], Loss: 2.2626, Perplexity: 9.60852\n",
      "Epoch [2/3], Step [84200/138038], Loss: 2.3613, Perplexity: 10.60476\n",
      "Epoch [2/3], Step [84300/138038], Loss: 3.0167, Perplexity: 20.4241\n",
      "Epoch [2/3], Step [84400/138038], Loss: 2.9939, Perplexity: 19.9644\n",
      "Epoch [2/3], Step [84500/138038], Loss: 3.3222, Perplexity: 27.72087\n",
      "Epoch [2/3], Step [84600/138038], Loss: 2.5607, Perplexity: 12.9448\n",
      "Epoch [2/3], Step [84700/138038], Loss: 2.9258, Perplexity: 18.64975\n",
      "Epoch [2/3], Step [84800/138038], Loss: 2.2261, Perplexity: 9.264019\n",
      "Epoch [2/3], Step [84900/138038], Loss: 1.8313, Perplexity: 6.242223\n",
      "Epoch [2/3], Step [85000/138038], Loss: 2.1176, Perplexity: 8.310929\n",
      "Epoch [2/3], Step [85100/138038], Loss: 2.2929, Perplexity: 9.90336\n",
      "Epoch [2/3], Step [85200/138038], Loss: 3.2954, Perplexity: 26.9879\n",
      "Epoch [2/3], Step [85300/138038], Loss: 3.5516, Perplexity: 34.8679\n",
      "Epoch [2/3], Step [85400/138038], Loss: 3.2605, Perplexity: 26.06370\n",
      "Epoch [2/3], Step [85500/138038], Loss: 3.5130, Perplexity: 33.5493\n",
      "Epoch [2/3], Step [85600/138038], Loss: 2.0123, Perplexity: 7.48030\n",
      "Epoch [2/3], Step [85700/138038], Loss: 2.4731, Perplexity: 11.85886\n",
      "Epoch [2/3], Step [85800/138038], Loss: 3.7445, Perplexity: 42.2873\n",
      "Epoch [2/3], Step [85900/138038], Loss: 4.3511, Perplexity: 77.5650\n",
      "Epoch [2/3], Step [86000/138038], Loss: 2.0548, Perplexity: 7.805142\n",
      "Epoch [2/3], Step [86100/138038], Loss: 2.9870, Perplexity: 19.8258\n",
      "Epoch [2/3], Step [86200/138038], Loss: 2.5583, Perplexity: 12.91351\n",
      "Epoch [2/3], Step [86300/138038], Loss: 3.3909, Perplexity: 29.6936\n",
      "Epoch [2/3], Step [86400/138038], Loss: 2.3215, Perplexity: 10.19143\n",
      "Epoch [2/3], Step [86500/138038], Loss: 2.8659, Perplexity: 17.56480\n",
      "Epoch [2/3], Step [86600/138038], Loss: 3.0785, Perplexity: 21.7249\n",
      "Epoch [2/3], Step [86700/138038], Loss: 2.3736, Perplexity: 10.7363\n",
      "Epoch [2/3], Step [86800/138038], Loss: 3.5702, Perplexity: 35.5234\n",
      "Epoch [2/3], Step [86900/138038], Loss: 2.9505, Perplexity: 19.11481\n",
      "Epoch [2/3], Step [87000/138038], Loss: 3.6382, Perplexity: 38.0215\n",
      "Epoch [2/3], Step [87100/138038], Loss: 2.3680, Perplexity: 10.67575\n",
      "Epoch [2/3], Step [87200/138038], Loss: 3.5409, Perplexity: 34.4972\n",
      "Epoch [2/3], Step [87300/138038], Loss: 2.6511, Perplexity: 14.17029\n",
      "Epoch [2/3], Step [87400/138038], Loss: 2.0469, Perplexity: 7.74404\n",
      "Epoch [2/3], Step [87500/138038], Loss: 1.9884, Perplexity: 7.303715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [87600/138038], Loss: 2.0122, Perplexity: 7.479826\n",
      "Epoch [2/3], Step [87700/138038], Loss: 2.5861, Perplexity: 13.27791\n",
      "Epoch [2/3], Step [87800/138038], Loss: 2.8218, Perplexity: 16.8074\n",
      "Epoch [2/3], Step [87900/138038], Loss: 1.9725, Perplexity: 7.188429\n",
      "Epoch [2/3], Step [88000/138038], Loss: 2.6794, Perplexity: 14.5770\n",
      "Epoch [2/3], Step [88100/138038], Loss: 2.0735, Perplexity: 7.95245\n",
      "Epoch [2/3], Step [88200/138038], Loss: 2.8321, Perplexity: 16.9811\n",
      "Epoch [2/3], Step [88300/138038], Loss: 1.9434, Perplexity: 6.982874\n",
      "Epoch [2/3], Step [88400/138038], Loss: 2.8355, Perplexity: 17.0385\n",
      "Epoch [2/3], Step [88500/138038], Loss: 1.9221, Perplexity: 6.83549\n",
      "Epoch [2/3], Step [88600/138038], Loss: 1.8228, Perplexity: 6.189182\n",
      "Epoch [2/3], Step [88700/138038], Loss: 3.7791, Perplexity: 43.77486\n",
      "Epoch [2/3], Step [88800/138038], Loss: 3.1052, Perplexity: 22.3132\n",
      "Epoch [2/3], Step [88900/138038], Loss: 2.6354, Perplexity: 13.9483\n",
      "Epoch [2/3], Step [89000/138038], Loss: 1.9771, Perplexity: 7.221918\n",
      "Epoch [2/3], Step [89100/138038], Loss: 2.7385, Perplexity: 15.4631\n",
      "Epoch [2/3], Step [89200/138038], Loss: 1.8043, Perplexity: 6.07548\n",
      "Epoch [2/3], Step [89300/138038], Loss: 2.6399, Perplexity: 14.01244\n",
      "Epoch [2/3], Step [89400/138038], Loss: 3.0539, Perplexity: 21.19832\n",
      "Epoch [2/3], Step [89500/138038], Loss: 2.9074, Perplexity: 18.3088\n",
      "Epoch [2/3], Step [89600/138038], Loss: 2.1835, Perplexity: 8.877758\n",
      "Epoch [2/3], Step [89700/138038], Loss: 2.3160, Perplexity: 10.1353\n",
      "Epoch [2/3], Step [89800/138038], Loss: 2.5103, Perplexity: 12.3085\n",
      "Epoch [2/3], Step [89900/138038], Loss: 2.5559, Perplexity: 12.8823\n",
      "Epoch [2/3], Step [90000/138038], Loss: 3.2082, Perplexity: 24.73524\n",
      "Epoch [2/3], Step [90100/138038], Loss: 1.6488, Perplexity: 5.20060\n",
      "Epoch [2/3], Step [90200/138038], Loss: 3.7356, Perplexity: 41.91343\n",
      "Epoch [2/3], Step [90300/138038], Loss: 1.5079, Perplexity: 4.51701\n",
      "Epoch [2/3], Step [90400/138038], Loss: 2.1744, Perplexity: 8.79681\n",
      "Epoch [2/3], Step [90500/138038], Loss: 2.2382, Perplexity: 9.376891\n",
      "Epoch [2/3], Step [90600/138038], Loss: 1.9996, Perplexity: 7.38637\n",
      "Epoch [2/3], Step [90700/138038], Loss: 2.3641, Perplexity: 10.6348\n",
      "Epoch [2/3], Step [90800/138038], Loss: 3.7734, Perplexity: 43.5262\n",
      "Epoch [2/3], Step [90900/138038], Loss: 2.5070, Perplexity: 12.2679\n",
      "Epoch [2/3], Step [91000/138038], Loss: 2.2303, Perplexity: 9.30238\n",
      "Epoch [2/3], Step [91100/138038], Loss: 3.5212, Perplexity: 33.82461\n",
      "Epoch [2/3], Step [91200/138038], Loss: 2.5995, Perplexity: 13.45685\n",
      "Epoch [2/3], Step [91300/138038], Loss: 2.6591, Perplexity: 14.2835\n",
      "Epoch [2/3], Step [91400/138038], Loss: 2.9622, Perplexity: 19.34070\n",
      "Epoch [2/3], Step [91500/138038], Loss: 2.6254, Perplexity: 13.8096\n",
      "Epoch [2/3], Step [91600/138038], Loss: 2.4449, Perplexity: 11.5289\n",
      "Epoch [2/3], Step [91700/138038], Loss: 2.4866, Perplexity: 12.0209\n",
      "Epoch [2/3], Step [91800/138038], Loss: 2.6962, Perplexity: 14.8238\n",
      "Epoch [2/3], Step [91900/138038], Loss: 2.2550, Perplexity: 9.534992\n",
      "Epoch [2/3], Step [92000/138038], Loss: 3.6905, Perplexity: 40.0631\n",
      "Epoch [2/3], Step [92100/138038], Loss: 3.3681, Perplexity: 29.0243\n",
      "Epoch [2/3], Step [92200/138038], Loss: 3.0305, Perplexity: 20.7079\n",
      "Epoch [2/3], Step [92300/138038], Loss: 2.2667, Perplexity: 9.647125\n",
      "Epoch [2/3], Step [92400/138038], Loss: 2.5394, Perplexity: 12.67245\n",
      "Epoch [2/3], Step [92500/138038], Loss: 2.8548, Perplexity: 17.3709\n",
      "Epoch [2/3], Step [92600/138038], Loss: 2.6012, Perplexity: 13.47934\n",
      "Epoch [2/3], Step [92700/138038], Loss: 2.1800, Perplexity: 8.84648\n",
      "Epoch [2/3], Step [92800/138038], Loss: 3.2974, Perplexity: 27.0413\n",
      "Epoch [2/3], Step [92900/138038], Loss: 3.1798, Perplexity: 24.04127\n",
      "Epoch [2/3], Step [93000/138038], Loss: 2.5098, Perplexity: 12.30286\n",
      "Epoch [2/3], Step [93100/138038], Loss: 2.1455, Perplexity: 8.54605\n",
      "Epoch [2/3], Step [93200/138038], Loss: 2.5399, Perplexity: 12.67831\n",
      "Epoch [2/3], Step [93300/138038], Loss: 1.6315, Perplexity: 5.11135\n",
      "Epoch [2/3], Step [93400/138038], Loss: 3.1263, Perplexity: 22.7884\n",
      "Epoch [2/3], Step [93500/138038], Loss: 2.3932, Perplexity: 10.9485\n",
      "Epoch [2/3], Step [93600/138038], Loss: 1.8911, Perplexity: 6.62686\n",
      "Epoch [2/3], Step [93700/138038], Loss: 3.7186, Perplexity: 41.2062\n",
      "Epoch [2/3], Step [93800/138038], Loss: 3.0757, Perplexity: 21.6649\n",
      "Epoch [2/3], Step [93900/138038], Loss: 3.1642, Perplexity: 23.66875\n",
      "Epoch [2/3], Step [94000/138038], Loss: 1.8643, Perplexity: 6.45118\n",
      "Epoch [2/3], Step [94100/138038], Loss: 2.7140, Perplexity: 15.0896\n",
      "Epoch [2/3], Step [94200/138038], Loss: 3.2659, Perplexity: 26.2046\n",
      "Epoch [2/3], Step [94300/138038], Loss: 3.6957, Perplexity: 40.27548\n",
      "Epoch [2/3], Step [94400/138038], Loss: 2.9331, Perplexity: 18.7857\n",
      "Epoch [2/3], Step [94500/138038], Loss: 2.0636, Perplexity: 7.874176\n",
      "Epoch [2/3], Step [94600/138038], Loss: 2.2785, Perplexity: 9.762574\n",
      "Epoch [2/3], Step [94700/138038], Loss: 2.5978, Perplexity: 13.4342\n",
      "Epoch [2/3], Step [94800/138038], Loss: 2.7624, Perplexity: 15.83848\n",
      "Epoch [2/3], Step [94900/138038], Loss: 2.9535, Perplexity: 19.17247\n",
      "Epoch [2/3], Step [95000/138038], Loss: 1.9505, Perplexity: 7.032121\n",
      "Epoch [2/3], Step [95100/138038], Loss: 2.2878, Perplexity: 9.853644\n",
      "Epoch [2/3], Step [95200/138038], Loss: 2.3517, Perplexity: 10.5031\n",
      "Epoch [2/3], Step [95300/138038], Loss: 2.5433, Perplexity: 12.72132\n",
      "Epoch [2/3], Step [95400/138038], Loss: 2.1331, Perplexity: 8.44098\n",
      "Epoch [2/3], Step [95500/138038], Loss: 2.9847, Perplexity: 19.7814\n",
      "Epoch [2/3], Step [95600/138038], Loss: 3.5230, Perplexity: 33.88479\n",
      "Epoch [2/3], Step [95700/138038], Loss: 2.5405, Perplexity: 12.6858\n",
      "Epoch [2/3], Step [95800/138038], Loss: 3.5724, Perplexity: 35.6033\n",
      "Epoch [2/3], Step [95900/138038], Loss: 2.4340, Perplexity: 11.4045\n",
      "Epoch [2/3], Step [96000/138038], Loss: 2.4062, Perplexity: 11.0919\n",
      "Epoch [2/3], Step [96100/138038], Loss: 3.4387, Perplexity: 31.14589\n",
      "Epoch [2/3], Step [96200/138038], Loss: 2.6748, Perplexity: 14.5092\n",
      "Epoch [2/3], Step [96300/138038], Loss: 3.3700, Perplexity: 29.0773\n",
      "Epoch [2/3], Step [96400/138038], Loss: 2.4367, Perplexity: 11.4354\n",
      "Epoch [2/3], Step [96500/138038], Loss: 2.1755, Perplexity: 8.80675\n",
      "Epoch [2/3], Step [96600/138038], Loss: 2.8263, Perplexity: 16.8829\n",
      "Epoch [2/3], Step [96700/138038], Loss: 2.2509, Perplexity: 9.49622\n",
      "Epoch [2/3], Step [96800/138038], Loss: 3.8132, Perplexity: 45.29362\n",
      "Epoch [2/3], Step [96900/138038], Loss: 2.4930, Perplexity: 12.0971\n",
      "Epoch [2/3], Step [97000/138038], Loss: 2.0436, Perplexity: 7.71841\n",
      "Epoch [2/3], Step [97100/138038], Loss: 2.5359, Perplexity: 12.6283\n",
      "Epoch [2/3], Step [97200/138038], Loss: 2.7731, Perplexity: 16.0088\n",
      "Epoch [2/3], Step [97300/138038], Loss: 2.5983, Perplexity: 13.4407\n",
      "Epoch [2/3], Step [97400/138038], Loss: 3.6397, Perplexity: 38.0802\n",
      "Epoch [2/3], Step [97500/138038], Loss: 1.9044, Perplexity: 6.71554\n",
      "Epoch [2/3], Step [97600/138038], Loss: 2.4389, Perplexity: 11.45994\n",
      "Epoch [2/3], Step [97700/138038], Loss: 4.0300, Perplexity: 56.2627\n",
      "Epoch [2/3], Step [97800/138038], Loss: 2.0624, Perplexity: 7.864546\n",
      "Epoch [2/3], Step [97900/138038], Loss: 1.8115, Perplexity: 6.11956\n",
      "Epoch [2/3], Step [98000/138038], Loss: 2.0480, Perplexity: 7.752252\n",
      "Epoch [2/3], Step [98100/138038], Loss: 2.4175, Perplexity: 11.21811\n",
      "Epoch [2/3], Step [98200/138038], Loss: 2.1484, Perplexity: 8.571518\n",
      "Epoch [2/3], Step [98300/138038], Loss: 2.0855, Perplexity: 8.048542\n",
      "Epoch [2/3], Step [98400/138038], Loss: 2.1043, Perplexity: 8.201251\n",
      "Epoch [2/3], Step [98500/138038], Loss: 3.1725, Perplexity: 23.8680\n",
      "Epoch [2/3], Step [98600/138038], Loss: 4.0962, Perplexity: 60.1111\n",
      "Epoch [2/3], Step [98700/138038], Loss: 2.3772, Perplexity: 10.7748\n",
      "Epoch [2/3], Step [98800/138038], Loss: 2.1974, Perplexity: 9.001988\n",
      "Epoch [2/3], Step [98900/138038], Loss: 1.9292, Perplexity: 6.88398\n",
      "Epoch [2/3], Step [99000/138038], Loss: 2.2609, Perplexity: 9.59174\n",
      "Epoch [2/3], Step [99100/138038], Loss: 2.4333, Perplexity: 11.3961\n",
      "Epoch [2/3], Step [99200/138038], Loss: 2.4938, Perplexity: 12.1074\n",
      "Epoch [2/3], Step [99300/138038], Loss: 3.3604, Perplexity: 28.8003\n",
      "Epoch [2/3], Step [99400/138038], Loss: 1.7538, Perplexity: 5.77641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [99500/138038], Loss: 3.1853, Perplexity: 24.1750\n",
      "Epoch [2/3], Step [99600/138038], Loss: 2.1018, Perplexity: 8.181200\n",
      "Epoch [2/3], Step [99700/138038], Loss: 4.3739, Perplexity: 79.3512\n",
      "Epoch [2/3], Step [99800/138038], Loss: 2.1165, Perplexity: 8.30212\n",
      "Epoch [2/3], Step [99900/138038], Loss: 2.6738, Perplexity: 14.4955\n",
      "Epoch [2/3], Step [100000/138038], Loss: 2.0919, Perplexity: 8.0999\n",
      "Epoch [2/3], Step [100100/138038], Loss: 2.5637, Perplexity: 12.9832\n",
      "Epoch [2/3], Step [100200/138038], Loss: 2.6143, Perplexity: 13.6577\n",
      "Epoch [2/3], Step [100300/138038], Loss: 3.3018, Perplexity: 27.1606\n",
      "Epoch [2/3], Step [100400/138038], Loss: 2.5444, Perplexity: 12.7350\n",
      "Epoch [2/3], Step [100500/138038], Loss: 3.3057, Perplexity: 27.2684\n",
      "Epoch [2/3], Step [100600/138038], Loss: 1.9831, Perplexity: 7.265655\n",
      "Epoch [2/3], Step [100700/138038], Loss: 1.9607, Perplexity: 7.10409\n",
      "Epoch [2/3], Step [100800/138038], Loss: 2.4382, Perplexity: 11.45216\n",
      "Epoch [2/3], Step [100900/138038], Loss: 3.1886, Perplexity: 24.2535\n",
      "Epoch [2/3], Step [101000/138038], Loss: 3.5723, Perplexity: 35.59835\n",
      "Epoch [2/3], Step [101100/138038], Loss: 2.8554, Perplexity: 17.38189\n",
      "Epoch [2/3], Step [101200/138038], Loss: 2.8438, Perplexity: 17.1811\n",
      "Epoch [2/3], Step [101300/138038], Loss: 2.9032, Perplexity: 18.2316\n",
      "Epoch [2/3], Step [101400/138038], Loss: 3.6012, Perplexity: 36.6417\n",
      "Epoch [2/3], Step [101500/138038], Loss: 2.2217, Perplexity: 9.22322\n",
      "Epoch [2/3], Step [101600/138038], Loss: 2.6979, Perplexity: 14.8491\n",
      "Epoch [2/3], Step [101700/138038], Loss: 2.3712, Perplexity: 10.7101\n",
      "Epoch [2/3], Step [101800/138038], Loss: 2.7602, Perplexity: 15.8030\n",
      "Epoch [2/3], Step [101900/138038], Loss: 1.9598, Perplexity: 7.098155\n",
      "Epoch [2/3], Step [102000/138038], Loss: 2.9857, Perplexity: 19.80069\n",
      "Epoch [2/3], Step [102100/138038], Loss: 2.6193, Perplexity: 13.7260\n",
      "Epoch [2/3], Step [102200/138038], Loss: 1.8517, Perplexity: 6.37043\n",
      "Epoch [2/3], Step [102300/138038], Loss: 3.0946, Perplexity: 22.0788\n",
      "Epoch [2/3], Step [102400/138038], Loss: 2.6773, Perplexity: 14.5462\n",
      "Epoch [2/3], Step [102500/138038], Loss: 3.0437, Perplexity: 20.9834\n",
      "Epoch [2/3], Step [102600/138038], Loss: 3.5393, Perplexity: 34.44368\n",
      "Epoch [2/3], Step [102700/138038], Loss: 2.6144, Perplexity: 13.6589\n",
      "Epoch [2/3], Step [102800/138038], Loss: 2.7046, Perplexity: 14.9479\n",
      "Epoch [2/3], Step [102900/138038], Loss: 3.2837, Perplexity: 26.67369\n",
      "Epoch [2/3], Step [103000/138038], Loss: 2.8766, Perplexity: 17.75347\n",
      "Epoch [2/3], Step [103100/138038], Loss: 1.9665, Perplexity: 7.14543\n",
      "Epoch [2/3], Step [103200/138038], Loss: 1.6165, Perplexity: 5.03563\n",
      "Epoch [2/3], Step [103300/138038], Loss: 2.6336, Perplexity: 13.9237\n",
      "Epoch [2/3], Step [103400/138038], Loss: 3.2892, Perplexity: 26.82177\n",
      "Epoch [2/3], Step [103500/138038], Loss: 2.1544, Perplexity: 8.622456\n",
      "Epoch [2/3], Step [103600/138038], Loss: 2.8106, Perplexity: 16.61942\n",
      "Epoch [2/3], Step [103700/138038], Loss: 2.9112, Perplexity: 18.3782\n",
      "Epoch [2/3], Step [103800/138038], Loss: 3.4713, Perplexity: 32.17776\n",
      "Epoch [2/3], Step [103900/138038], Loss: 2.6207, Perplexity: 13.7451\n",
      "Epoch [2/3], Step [104000/138038], Loss: 3.8609, Perplexity: 47.50571\n",
      "Epoch [2/3], Step [104100/138038], Loss: 2.5921, Perplexity: 13.35727\n",
      "Epoch [2/3], Step [104200/138038], Loss: 4.4990, Perplexity: 89.92292\n",
      "Epoch [2/3], Step [104300/138038], Loss: 3.5026, Perplexity: 33.2030\n",
      "Epoch [2/3], Step [104400/138038], Loss: 2.4701, Perplexity: 11.8241\n",
      "Epoch [2/3], Step [104500/138038], Loss: 2.3005, Perplexity: 9.979275\n",
      "Epoch [2/3], Step [104600/138038], Loss: 2.3933, Perplexity: 10.94948\n",
      "Epoch [2/3], Step [104700/138038], Loss: 3.1571, Perplexity: 23.50313\n",
      "Epoch [2/3], Step [104800/138038], Loss: 2.7006, Perplexity: 14.8889\n",
      "Epoch [2/3], Step [104900/138038], Loss: 2.9370, Perplexity: 18.85866\n",
      "Epoch [2/3], Step [105000/138038], Loss: 2.7076, Perplexity: 14.9926\n",
      "Epoch [2/3], Step [105100/138038], Loss: 2.7366, Perplexity: 15.4351\n",
      "Epoch [2/3], Step [105200/138038], Loss: 1.9960, Perplexity: 7.35938\n",
      "Epoch [2/3], Step [105300/138038], Loss: 2.1312, Perplexity: 8.424853\n",
      "Epoch [2/3], Step [105400/138038], Loss: 2.8687, Perplexity: 17.61364\n",
      "Epoch [2/3], Step [105500/138038], Loss: 2.6138, Perplexity: 13.65021\n",
      "Epoch [2/3], Step [105600/138038], Loss: 2.3101, Perplexity: 10.07560\n",
      "Epoch [2/3], Step [105700/138038], Loss: 3.0733, Perplexity: 21.6124\n",
      "Epoch [2/3], Step [105800/138038], Loss: 2.5021, Perplexity: 12.2084\n",
      "Epoch [2/3], Step [105900/138038], Loss: 2.4116, Perplexity: 11.1521\n",
      "Epoch [2/3], Step [106000/138038], Loss: 2.9492, Perplexity: 19.08987\n",
      "Epoch [2/3], Step [106100/138038], Loss: 3.2592, Perplexity: 26.0292\n",
      "Epoch [2/3], Step [106200/138038], Loss: 5.7460, Perplexity: 312.9258\n",
      "Epoch [2/3], Step [106300/138038], Loss: 1.8725, Perplexity: 6.50449\n",
      "Epoch [2/3], Step [106400/138038], Loss: 2.5304, Perplexity: 12.5590\n",
      "Epoch [2/3], Step [106500/138038], Loss: 2.9737, Perplexity: 19.56353\n",
      "Epoch [2/3], Step [106600/138038], Loss: 2.8844, Perplexity: 17.89358\n",
      "Epoch [2/3], Step [106700/138038], Loss: 2.3370, Perplexity: 10.3498\n",
      "Epoch [2/3], Step [106800/138038], Loss: 2.4245, Perplexity: 11.2970\n",
      "Epoch [2/3], Step [106900/138038], Loss: 2.3939, Perplexity: 10.95652\n",
      "Epoch [2/3], Step [107000/138038], Loss: 1.7064, Perplexity: 5.50906\n",
      "Epoch [2/3], Step [107100/138038], Loss: 2.6387, Perplexity: 13.9947\n",
      "Epoch [2/3], Step [107200/138038], Loss: 3.0488, Perplexity: 21.09068\n",
      "Epoch [2/3], Step [107300/138038], Loss: 2.3475, Perplexity: 10.45927\n",
      "Epoch [2/3], Step [107400/138038], Loss: 3.0008, Perplexity: 20.1009\n",
      "Epoch [2/3], Step [107500/138038], Loss: 2.0648, Perplexity: 7.883695\n",
      "Epoch [2/3], Step [107600/138038], Loss: 4.1086, Perplexity: 60.8626\n",
      "Epoch [2/3], Step [107700/138038], Loss: 2.8817, Perplexity: 17.8441\n",
      "Epoch [2/3], Step [107800/138038], Loss: 3.3216, Perplexity: 27.7045\n",
      "Epoch [2/3], Step [107900/138038], Loss: 2.6976, Perplexity: 14.8434\n",
      "Epoch [2/3], Step [108000/138038], Loss: 2.6436, Perplexity: 14.0643\n",
      "Epoch [2/3], Step [108100/138038], Loss: 2.7400, Perplexity: 15.4874\n",
      "Epoch [2/3], Step [108200/138038], Loss: 3.5316, Perplexity: 34.17948\n",
      "Epoch [2/3], Step [108300/138038], Loss: 3.4928, Perplexity: 32.8782\n",
      "Epoch [2/3], Step [108400/138038], Loss: 2.7955, Perplexity: 16.37079\n",
      "Epoch [2/3], Step [108500/138038], Loss: 1.6332, Perplexity: 5.120455\n",
      "Epoch [2/3], Step [108600/138038], Loss: 2.9568, Perplexity: 19.2370\n",
      "Epoch [2/3], Step [108700/138038], Loss: 2.4739, Perplexity: 11.8689\n",
      "Epoch [2/3], Step [108800/138038], Loss: 2.0735, Perplexity: 7.95237\n",
      "Epoch [2/3], Step [108900/138038], Loss: 1.8547, Perplexity: 6.39018\n",
      "Epoch [2/3], Step [109000/138038], Loss: 2.1863, Perplexity: 8.90225\n",
      "Epoch [2/3], Step [109100/138038], Loss: 2.1242, Perplexity: 8.366217\n",
      "Epoch [2/3], Step [109200/138038], Loss: 2.6956, Perplexity: 14.8138\n",
      "Epoch [2/3], Step [109300/138038], Loss: 3.4280, Perplexity: 30.8138\n",
      "Epoch [2/3], Step [109400/138038], Loss: 2.6112, Perplexity: 13.61529\n",
      "Epoch [2/3], Step [109500/138038], Loss: 2.9011, Perplexity: 18.19334\n",
      "Epoch [2/3], Step [109600/138038], Loss: 3.5851, Perplexity: 36.0583\n",
      "Epoch [2/3], Step [109700/138038], Loss: 2.2030, Perplexity: 9.052198\n",
      "Epoch [2/3], Step [109800/138038], Loss: 3.5980, Perplexity: 36.5269\n",
      "Epoch [2/3], Step [109900/138038], Loss: 1.8846, Perplexity: 6.58399\n",
      "Epoch [2/3], Step [110000/138038], Loss: 2.1782, Perplexity: 8.83036\n",
      "Epoch [2/3], Step [110100/138038], Loss: 2.6472, Perplexity: 14.1147\n",
      "Epoch [2/3], Step [110200/138038], Loss: 2.5670, Perplexity: 13.0270\n",
      "Epoch [2/3], Step [110300/138038], Loss: 3.0090, Perplexity: 20.2677\n",
      "Epoch [2/3], Step [110400/138038], Loss: 2.0066, Perplexity: 7.43809\n",
      "Epoch [2/3], Step [110500/138038], Loss: 1.6019, Perplexity: 4.962383\n",
      "Epoch [2/3], Step [110600/138038], Loss: 2.3497, Perplexity: 10.48282\n",
      "Epoch [2/3], Step [110700/138038], Loss: 2.4151, Perplexity: 11.1908\n",
      "Epoch [2/3], Step [110800/138038], Loss: 1.9979, Perplexity: 7.373750\n",
      "Epoch [2/3], Step [110900/138038], Loss: 3.2403, Perplexity: 25.5414\n",
      "Epoch [2/3], Step [111000/138038], Loss: 2.7425, Perplexity: 15.52600\n",
      "Epoch [2/3], Step [111100/138038], Loss: 2.5446, Perplexity: 12.7382\n",
      "Epoch [2/3], Step [111200/138038], Loss: 2.3873, Perplexity: 10.8836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [111300/138038], Loss: 3.5440, Perplexity: 34.6039\n",
      "Epoch [2/3], Step [111400/138038], Loss: 3.3560, Perplexity: 28.67335\n",
      "Epoch [2/3], Step [111500/138038], Loss: 2.8552, Perplexity: 17.3783\n",
      "Epoch [2/3], Step [111600/138038], Loss: 3.2158, Perplexity: 24.9236\n",
      "Epoch [2/3], Step [111700/138038], Loss: 2.4342, Perplexity: 11.4070\n",
      "Epoch [2/3], Step [111800/138038], Loss: 2.1793, Perplexity: 8.84016\n",
      "Epoch [2/3], Step [111900/138038], Loss: 2.4447, Perplexity: 11.5276\n",
      "Epoch [2/3], Step [112000/138038], Loss: 2.7244, Perplexity: 15.24719\n",
      "Epoch [2/3], Step [112100/138038], Loss: 2.9195, Perplexity: 18.53196\n",
      "Epoch [2/3], Step [112200/138038], Loss: 1.5088, Perplexity: 4.52151\n",
      "Epoch [2/3], Step [112300/138038], Loss: 2.5904, Perplexity: 13.3355\n",
      "Epoch [2/3], Step [112400/138038], Loss: 2.2774, Perplexity: 9.750905\n",
      "Epoch [2/3], Step [112500/138038], Loss: 2.1540, Perplexity: 8.61978\n",
      "Epoch [2/3], Step [112600/138038], Loss: 2.9498, Perplexity: 19.1028\n",
      "Epoch [2/3], Step [112700/138038], Loss: 2.7389, Perplexity: 15.47042\n",
      "Epoch [2/3], Step [112800/138038], Loss: 2.6794, Perplexity: 14.5761\n",
      "Epoch [2/3], Step [112900/138038], Loss: 1.4453, Perplexity: 4.243294\n",
      "Epoch [2/3], Step [113000/138038], Loss: 2.6070, Perplexity: 13.5587\n",
      "Epoch [2/3], Step [113100/138038], Loss: 2.6740, Perplexity: 14.4981\n",
      "Epoch [2/3], Step [113200/138038], Loss: 3.6564, Perplexity: 38.72041\n",
      "Epoch [2/3], Step [113300/138038], Loss: 2.6303, Perplexity: 13.8782\n",
      "Epoch [2/3], Step [113400/138038], Loss: 2.3695, Perplexity: 10.69245\n",
      "Epoch [2/3], Step [113500/138038], Loss: 2.6909, Perplexity: 14.7455\n",
      "Epoch [2/3], Step [113600/138038], Loss: 2.7601, Perplexity: 15.80103\n",
      "Epoch [2/3], Step [113700/138038], Loss: 3.4977, Perplexity: 33.0382\n",
      "Epoch [2/3], Step [113800/138038], Loss: 3.0402, Perplexity: 20.9092\n",
      "Epoch [2/3], Step [113900/138038], Loss: 2.6965, Perplexity: 14.82807\n",
      "Epoch [2/3], Step [114000/138038], Loss: 3.8627, Perplexity: 47.5930\n",
      "Epoch [2/3], Step [114100/138038], Loss: 2.1909, Perplexity: 8.943380\n",
      "Epoch [2/3], Step [114200/138038], Loss: 3.7832, Perplexity: 43.9553\n",
      "Epoch [2/3], Step [114300/138038], Loss: 2.9620, Perplexity: 19.33605\n",
      "Epoch [2/3], Step [114400/138038], Loss: 2.7598, Perplexity: 15.79723\n",
      "Epoch [2/3], Step [114500/138038], Loss: 3.3977, Perplexity: 29.89544\n",
      "Epoch [2/3], Step [114600/138038], Loss: 3.0845, Perplexity: 21.85713\n",
      "Epoch [2/3], Step [114700/138038], Loss: 2.3636, Perplexity: 10.62875\n",
      "Epoch [2/3], Step [114800/138038], Loss: 1.8500, Perplexity: 6.35982\n",
      "Epoch [2/3], Step [114900/138038], Loss: 3.3657, Perplexity: 28.95366\n",
      "Epoch [2/3], Step [115000/138038], Loss: 1.9825, Perplexity: 7.26067\n",
      "Epoch [2/3], Step [115100/138038], Loss: 3.5745, Perplexity: 35.6762\n",
      "Epoch [2/3], Step [115200/138038], Loss: 3.1658, Perplexity: 23.7073\n",
      "Epoch [2/3], Step [115300/138038], Loss: 2.4466, Perplexity: 11.5494\n",
      "Epoch [2/3], Step [115400/138038], Loss: 2.1006, Perplexity: 8.170789\n",
      "Epoch [2/3], Step [115500/138038], Loss: 3.0905, Perplexity: 21.9889\n",
      "Epoch [2/3], Step [115600/138038], Loss: 3.2589, Perplexity: 26.02079\n",
      "Epoch [2/3], Step [115700/138038], Loss: 2.5714, Perplexity: 13.08434\n",
      "Epoch [2/3], Step [115800/138038], Loss: 2.2633, Perplexity: 9.61482\n",
      "Epoch [2/3], Step [115900/138038], Loss: 2.8758, Perplexity: 17.7395\n",
      "Epoch [2/3], Step [116000/138038], Loss: 3.0960, Perplexity: 22.1083\n",
      "Epoch [2/3], Step [116100/138038], Loss: 1.9502, Perplexity: 7.03009\n",
      "Epoch [2/3], Step [116200/138038], Loss: 2.3694, Perplexity: 10.6906\n",
      "Epoch [2/3], Step [116300/138038], Loss: 3.3050, Perplexity: 27.2473\n",
      "Epoch [2/3], Step [116400/138038], Loss: 2.7941, Perplexity: 16.3484\n",
      "Epoch [2/3], Step [116500/138038], Loss: 3.4146, Perplexity: 30.4051\n",
      "Epoch [2/3], Step [116600/138038], Loss: 2.5900, Perplexity: 13.32952\n",
      "Epoch [2/3], Step [116700/138038], Loss: 2.6795, Perplexity: 14.5772\n",
      "Epoch [2/3], Step [116800/138038], Loss: 3.5146, Perplexity: 33.6023\n",
      "Epoch [2/3], Step [116900/138038], Loss: 2.8192, Perplexity: 16.7638\n",
      "Epoch [2/3], Step [117000/138038], Loss: 2.5160, Perplexity: 12.3785\n",
      "Epoch [2/3], Step [117100/138038], Loss: 3.7670, Perplexity: 43.25177\n",
      "Epoch [2/3], Step [117200/138038], Loss: 1.9220, Perplexity: 6.834581\n",
      "Epoch [2/3], Step [117300/138038], Loss: 2.1968, Perplexity: 8.99630\n",
      "Epoch [2/3], Step [117400/138038], Loss: 2.8449, Perplexity: 17.2005\n",
      "Epoch [2/3], Step [117500/138038], Loss: 2.9348, Perplexity: 18.8169\n",
      "Epoch [2/3], Step [117600/138038], Loss: 2.2242, Perplexity: 9.24580\n",
      "Epoch [2/3], Step [117700/138038], Loss: 3.3935, Perplexity: 29.77110\n",
      "Epoch [2/3], Step [117800/138038], Loss: 2.9760, Perplexity: 19.6096\n",
      "Epoch [2/3], Step [117900/138038], Loss: 2.3335, Perplexity: 10.31433\n",
      "Epoch [2/3], Step [118000/138038], Loss: 2.0990, Perplexity: 8.158300\n",
      "Epoch [2/3], Step [118100/138038], Loss: 3.7667, Perplexity: 43.2375\n",
      "Epoch [2/3], Step [118200/138038], Loss: 3.3055, Perplexity: 27.26096\n",
      "Epoch [2/3], Step [118300/138038], Loss: 2.3085, Perplexity: 10.05979\n",
      "Epoch [2/3], Step [118400/138038], Loss: 1.9908, Perplexity: 7.32137\n",
      "Epoch [2/3], Step [118500/138038], Loss: 1.7267, Perplexity: 5.622027\n",
      "Epoch [2/3], Step [118600/138038], Loss: 2.1222, Perplexity: 8.34961\n",
      "Epoch [2/3], Step [118700/138038], Loss: 2.6013, Perplexity: 13.4807\n",
      "Epoch [2/3], Step [118800/138038], Loss: 4.4078, Perplexity: 82.0906\n",
      "Epoch [2/3], Step [118900/138038], Loss: 2.2279, Perplexity: 9.28070\n",
      "Epoch [2/3], Step [119000/138038], Loss: 2.3630, Perplexity: 10.6225\n",
      "Epoch [2/3], Step [119100/138038], Loss: 3.1348, Perplexity: 22.98393\n",
      "Epoch [2/3], Step [119200/138038], Loss: 2.6109, Perplexity: 13.6111\n",
      "Epoch [2/3], Step [119300/138038], Loss: 3.0805, Perplexity: 21.7683\n",
      "Epoch [2/3], Step [119400/138038], Loss: 3.0343, Perplexity: 20.7865\n",
      "Epoch [2/3], Step [119500/138038], Loss: 2.8983, Perplexity: 18.1432\n",
      "Epoch [2/3], Step [119600/138038], Loss: 2.3393, Perplexity: 10.3740\n",
      "Epoch [2/3], Step [119700/138038], Loss: 2.2955, Perplexity: 9.929690\n",
      "Epoch [2/3], Step [119800/138038], Loss: 2.0872, Perplexity: 8.06239\n",
      "Epoch [2/3], Step [119900/138038], Loss: 2.6359, Perplexity: 13.9561\n",
      "Epoch [2/3], Step [120000/138038], Loss: 3.0092, Perplexity: 20.2713\n",
      "Epoch [2/3], Step [120100/138038], Loss: 3.3240, Perplexity: 27.77241\n",
      "Epoch [2/3], Step [120200/138038], Loss: 3.4166, Perplexity: 30.46652\n",
      "Epoch [2/3], Step [120300/138038], Loss: 3.0948, Perplexity: 22.08342\n",
      "Epoch [2/3], Step [120400/138038], Loss: 2.3483, Perplexity: 10.4679\n",
      "Epoch [2/3], Step [120500/138038], Loss: 2.1680, Perplexity: 8.740467\n",
      "Epoch [2/3], Step [120600/138038], Loss: 1.7404, Perplexity: 5.699430\n",
      "Epoch [2/3], Step [120700/138038], Loss: 3.9661, Perplexity: 52.7806\n",
      "Epoch [2/3], Step [120800/138038], Loss: 2.4694, Perplexity: 11.8150\n",
      "Epoch [2/3], Step [120900/138038], Loss: 1.7528, Perplexity: 5.770552\n",
      "Epoch [2/3], Step [121000/138038], Loss: 3.0264, Perplexity: 20.6233\n",
      "Epoch [2/3], Step [121100/138038], Loss: 1.9689, Perplexity: 7.16288\n",
      "Epoch [2/3], Step [121200/138038], Loss: 4.1125, Perplexity: 61.09851\n",
      "Epoch [2/3], Step [121300/138038], Loss: 3.4467, Perplexity: 31.3979\n",
      "Epoch [2/3], Step [121400/138038], Loss: 2.7307, Perplexity: 15.34357\n",
      "Epoch [2/3], Step [121500/138038], Loss: 3.8286, Perplexity: 45.9966\n",
      "Epoch [2/3], Step [121600/138038], Loss: 3.4215, Perplexity: 30.6141\n",
      "Epoch [2/3], Step [121700/138038], Loss: 3.3219, Perplexity: 27.7131\n",
      "Epoch [2/3], Step [121800/138038], Loss: 2.5415, Perplexity: 12.6984\n",
      "Epoch [2/3], Step [121900/138038], Loss: 2.1824, Perplexity: 8.867146\n",
      "Epoch [2/3], Step [122000/138038], Loss: 3.7931, Perplexity: 44.39495\n",
      "Epoch [2/3], Step [122100/138038], Loss: 2.3109, Perplexity: 10.08354\n",
      "Epoch [2/3], Step [122200/138038], Loss: 3.2765, Perplexity: 26.4836\n",
      "Epoch [2/3], Step [122300/138038], Loss: 2.5841, Perplexity: 13.2517\n",
      "Epoch [2/3], Step [122400/138038], Loss: 2.2177, Perplexity: 9.18605\n",
      "Epoch [2/3], Step [122500/138038], Loss: 3.2421, Perplexity: 25.5877\n",
      "Epoch [2/3], Step [122600/138038], Loss: 3.2214, Perplexity: 25.06343\n",
      "Epoch [2/3], Step [122700/138038], Loss: 3.2079, Perplexity: 24.7262\n",
      "Epoch [2/3], Step [122800/138038], Loss: 3.0743, Perplexity: 21.6339\n",
      "Epoch [2/3], Step [122900/138038], Loss: 2.6013, Perplexity: 13.4815\n",
      "Epoch [2/3], Step [123000/138038], Loss: 1.8952, Perplexity: 6.65382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [123100/138038], Loss: 2.3346, Perplexity: 10.32539\n",
      "Epoch [2/3], Step [123200/138038], Loss: 3.4257, Perplexity: 30.74275\n",
      "Epoch [2/3], Step [123300/138038], Loss: 4.0307, Perplexity: 56.2994\n",
      "Epoch [2/3], Step [123400/138038], Loss: 3.1849, Perplexity: 24.16479\n",
      "Epoch [2/3], Step [123500/138038], Loss: 2.9408, Perplexity: 18.9302\n",
      "Epoch [2/3], Step [123600/138038], Loss: 4.1598, Perplexity: 64.06136\n",
      "Epoch [2/3], Step [123700/138038], Loss: 2.3697, Perplexity: 10.69422\n",
      "Epoch [2/3], Step [123800/138038], Loss: 3.7614, Perplexity: 43.00917\n",
      "Epoch [2/3], Step [123900/138038], Loss: 1.7113, Perplexity: 5.53637\n",
      "Epoch [2/3], Step [124000/138038], Loss: 1.4631, Perplexity: 4.31926\n",
      "Epoch [2/3], Step [124100/138038], Loss: 1.9771, Perplexity: 7.221833\n",
      "Epoch [2/3], Step [124200/138038], Loss: 1.7046, Perplexity: 5.49900\n",
      "Epoch [2/3], Step [124300/138038], Loss: 2.2748, Perplexity: 9.72611\n",
      "Epoch [2/3], Step [124400/138038], Loss: 2.9422, Perplexity: 18.9571\n",
      "Epoch [2/3], Step [124500/138038], Loss: 2.6046, Perplexity: 13.5265\n",
      "Epoch [2/3], Step [124600/138038], Loss: 2.4781, Perplexity: 11.9186\n",
      "Epoch [2/3], Step [124700/138038], Loss: 3.1445, Perplexity: 23.20764\n",
      "Epoch [2/3], Step [124800/138038], Loss: 2.7623, Perplexity: 15.83658\n",
      "Epoch [2/3], Step [124900/138038], Loss: 2.6342, Perplexity: 13.93188\n",
      "Epoch [2/3], Step [125000/138038], Loss: 2.4916, Perplexity: 12.08043\n",
      "Epoch [2/3], Step [125100/138038], Loss: 3.6280, Perplexity: 37.6372\n",
      "Epoch [2/3], Step [125200/138038], Loss: 3.0491, Perplexity: 21.0966\n",
      "Epoch [2/3], Step [125300/138038], Loss: 1.9204, Perplexity: 6.823580\n",
      "Epoch [2/3], Step [125400/138038], Loss: 2.0849, Perplexity: 8.04389\n",
      "Epoch [2/3], Step [125500/138038], Loss: 3.3770, Perplexity: 29.2823\n",
      "Epoch [2/3], Step [125600/138038], Loss: 3.0279, Perplexity: 20.65332\n",
      "Epoch [2/3], Step [125700/138038], Loss: 3.0313, Perplexity: 20.7242\n",
      "Epoch [2/3], Step [125800/138038], Loss: 3.8581, Perplexity: 47.37724\n",
      "Epoch [2/3], Step [125900/138038], Loss: 4.3368, Perplexity: 76.4621\n",
      "Epoch [2/3], Step [126000/138038], Loss: 2.6213, Perplexity: 13.7535\n",
      "Epoch [2/3], Step [126100/138038], Loss: 2.8138, Perplexity: 16.67359\n",
      "Epoch [2/3], Step [126200/138038], Loss: 2.4057, Perplexity: 11.0857\n",
      "Epoch [2/3], Step [126300/138038], Loss: 2.0807, Perplexity: 8.010314\n",
      "Epoch [2/3], Step [126400/138038], Loss: 2.7758, Perplexity: 16.0512\n",
      "Epoch [2/3], Step [126500/138038], Loss: 3.4654, Perplexity: 31.99076\n",
      "Epoch [2/3], Step [126600/138038], Loss: 3.2343, Perplexity: 25.38988\n",
      "Epoch [2/3], Step [126700/138038], Loss: 2.9515, Perplexity: 19.13371\n",
      "Epoch [2/3], Step [126800/138038], Loss: 3.3353, Perplexity: 28.0855\n",
      "Epoch [2/3], Step [126900/138038], Loss: 3.2531, Perplexity: 25.86984\n",
      "Epoch [2/3], Step [127000/138038], Loss: 2.4282, Perplexity: 11.33840\n",
      "Epoch [2/3], Step [127100/138038], Loss: 2.3182, Perplexity: 10.15773\n",
      "Epoch [2/3], Step [127200/138038], Loss: 2.2470, Perplexity: 9.459070\n",
      "Epoch [2/3], Step [127300/138038], Loss: 3.2735, Perplexity: 26.4023\n",
      "Epoch [2/3], Step [127400/138038], Loss: 2.9283, Perplexity: 18.6955\n",
      "Epoch [2/3], Step [127500/138038], Loss: 2.1285, Perplexity: 8.40231\n",
      "Epoch [2/3], Step [127600/138038], Loss: 2.7636, Perplexity: 15.8567\n",
      "Epoch [2/3], Step [127700/138038], Loss: 3.9484, Perplexity: 51.8524\n",
      "Epoch [2/3], Step [127800/138038], Loss: 2.3944, Perplexity: 10.9617\n",
      "Epoch [2/3], Step [127900/138038], Loss: 3.7296, Perplexity: 41.66121\n",
      "Epoch [2/3], Step [128000/138038], Loss: 2.1715, Perplexity: 8.77183\n",
      "Epoch [2/3], Step [128100/138038], Loss: 2.3431, Perplexity: 10.4134\n",
      "Epoch [2/3], Step [128200/138038], Loss: 2.0438, Perplexity: 7.72020\n",
      "Epoch [2/3], Step [128300/138038], Loss: 3.3373, Perplexity: 28.14359\n",
      "Epoch [2/3], Step [128400/138038], Loss: 2.6964, Perplexity: 14.8262\n",
      "Epoch [2/3], Step [128500/138038], Loss: 2.3025, Perplexity: 9.99929\n",
      "Epoch [2/3], Step [128600/138038], Loss: 3.0015, Perplexity: 20.1165\n",
      "Epoch [2/3], Step [128700/138038], Loss: 1.7153, Perplexity: 5.55827\n",
      "Epoch [2/3], Step [128800/138038], Loss: 2.7998, Perplexity: 16.4408\n",
      "Epoch [2/3], Step [128900/138038], Loss: 2.3707, Perplexity: 10.7045\n",
      "Epoch [2/3], Step [129000/138038], Loss: 2.3841, Perplexity: 10.8490\n",
      "Epoch [2/3], Step [129100/138038], Loss: 2.8253, Perplexity: 16.8658\n",
      "Epoch [2/3], Step [129200/138038], Loss: 2.7484, Perplexity: 15.6170\n",
      "Epoch [2/3], Step [129300/138038], Loss: 3.4632, Perplexity: 31.92041\n",
      "Epoch [2/3], Step [129400/138038], Loss: 2.9292, Perplexity: 18.71232\n",
      "Epoch [2/3], Step [129500/138038], Loss: 2.8699, Perplexity: 17.63607\n",
      "Epoch [2/3], Step [129600/138038], Loss: 2.4015, Perplexity: 11.0395\n",
      "Epoch [2/3], Step [129700/138038], Loss: 2.3289, Perplexity: 10.2671\n",
      "Epoch [2/3], Step [129800/138038], Loss: 2.6351, Perplexity: 13.94471\n",
      "Epoch [2/3], Step [129900/138038], Loss: 3.4767, Perplexity: 32.3534\n",
      "Epoch [2/3], Step [130000/138038], Loss: 2.0529, Perplexity: 7.79087\n",
      "Epoch [2/3], Step [130100/138038], Loss: 1.8811, Perplexity: 6.560575\n",
      "Epoch [2/3], Step [130200/138038], Loss: 2.3228, Perplexity: 10.20370\n",
      "Epoch [2/3], Step [130300/138038], Loss: 2.9216, Perplexity: 18.5702\n",
      "Epoch [2/3], Step [130400/138038], Loss: 3.5904, Perplexity: 36.24789\n",
      "Epoch [2/3], Step [130500/138038], Loss: 2.5077, Perplexity: 12.27709\n",
      "Epoch [2/3], Step [130600/138038], Loss: 2.7358, Perplexity: 15.4225\n",
      "Epoch [2/3], Step [130700/138038], Loss: 2.1107, Perplexity: 8.25402\n",
      "Epoch [2/3], Step [130800/138038], Loss: 3.3364, Perplexity: 28.1188\n",
      "Epoch [2/3], Step [130900/138038], Loss: 1.9215, Perplexity: 6.830968\n",
      "Epoch [2/3], Step [131000/138038], Loss: 2.6854, Perplexity: 14.66428\n",
      "Epoch [2/3], Step [131100/138038], Loss: 3.7135, Perplexity: 40.99559\n",
      "Epoch [2/3], Step [131200/138038], Loss: 3.0227, Perplexity: 20.54758\n",
      "Epoch [2/3], Step [131300/138038], Loss: 3.3184, Perplexity: 27.6153\n",
      "Epoch [2/3], Step [131400/138038], Loss: 2.7181, Perplexity: 15.1510\n",
      "Epoch [2/3], Step [131500/138038], Loss: 2.2503, Perplexity: 9.491024\n",
      "Epoch [2/3], Step [131600/138038], Loss: 3.3966, Perplexity: 29.8616\n",
      "Epoch [2/3], Step [131700/138038], Loss: 3.5850, Perplexity: 36.0525\n",
      "Epoch [2/3], Step [131800/138038], Loss: 2.9772, Perplexity: 19.6322\n",
      "Epoch [2/3], Step [131900/138038], Loss: 2.8362, Perplexity: 17.0504\n",
      "Epoch [2/3], Step [132000/138038], Loss: 3.5633, Perplexity: 35.2780\n",
      "Epoch [2/3], Step [132100/138038], Loss: 2.7190, Perplexity: 15.1648\n",
      "Epoch [2/3], Step [132200/138038], Loss: 2.6868, Perplexity: 14.6849\n",
      "Epoch [2/3], Step [132300/138038], Loss: 3.0291, Perplexity: 20.6777\n",
      "Epoch [2/3], Step [132400/138038], Loss: 3.2139, Perplexity: 24.87675\n",
      "Epoch [2/3], Step [132500/138038], Loss: 1.7488, Perplexity: 5.74778\n",
      "Epoch [2/3], Step [132600/138038], Loss: 2.3130, Perplexity: 10.10421\n",
      "Epoch [2/3], Step [132700/138038], Loss: 2.6195, Perplexity: 13.7284\n",
      "Epoch [2/3], Step [132800/138038], Loss: 2.8477, Perplexity: 17.2488\n",
      "Epoch [2/3], Step [132900/138038], Loss: 2.7331, Perplexity: 15.38084\n",
      "Epoch [2/3], Step [133000/138038], Loss: 2.4857, Perplexity: 12.0091\n",
      "Epoch [2/3], Step [133100/138038], Loss: 2.6996, Perplexity: 14.8737\n",
      "Epoch [2/3], Step [133200/138038], Loss: 3.0995, Perplexity: 22.18722\n",
      "Epoch [2/3], Step [133300/138038], Loss: 2.2458, Perplexity: 9.448024\n",
      "Epoch [2/3], Step [133400/138038], Loss: 3.2006, Perplexity: 24.54664\n",
      "Epoch [2/3], Step [133500/138038], Loss: 3.2352, Perplexity: 25.4116\n",
      "Epoch [2/3], Step [133600/138038], Loss: 2.0319, Perplexity: 7.62886\n",
      "Epoch [2/3], Step [133700/138038], Loss: 2.7168, Perplexity: 15.1312\n",
      "Epoch [2/3], Step [133800/138038], Loss: 2.7433, Perplexity: 15.5376\n",
      "Epoch [2/3], Step [133900/138038], Loss: 2.9424, Perplexity: 18.96126\n",
      "Epoch [2/3], Step [134000/138038], Loss: 2.9816, Perplexity: 19.7187\n",
      "Epoch [2/3], Step [134100/138038], Loss: 2.2014, Perplexity: 9.03721\n",
      "Epoch [2/3], Step [134200/138038], Loss: 3.8335, Perplexity: 46.2235\n",
      "Epoch [2/3], Step [134300/138038], Loss: 3.2344, Perplexity: 25.39162\n",
      "Epoch [2/3], Step [134400/138038], Loss: 2.2224, Perplexity: 9.22985\n",
      "Epoch [2/3], Step [134500/138038], Loss: 2.6794, Perplexity: 14.5768\n",
      "Epoch [2/3], Step [134600/138038], Loss: 1.4390, Perplexity: 4.21638\n",
      "Epoch [2/3], Step [134700/138038], Loss: 1.4033, Perplexity: 4.06880\n",
      "Epoch [2/3], Step [134800/138038], Loss: 1.8661, Perplexity: 6.463300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [134900/138038], Loss: 2.4692, Perplexity: 11.81256\n",
      "Epoch [2/3], Step [135000/138038], Loss: 2.2354, Perplexity: 9.35000\n",
      "Epoch [2/3], Step [135100/138038], Loss: 2.5316, Perplexity: 12.5737\n",
      "Epoch [2/3], Step [135200/138038], Loss: 1.8327, Perplexity: 6.25097\n",
      "Epoch [2/3], Step [135300/138038], Loss: 2.3523, Perplexity: 10.50995\n",
      "Epoch [2/3], Step [135400/138038], Loss: 2.2593, Perplexity: 9.57647\n",
      "Epoch [2/3], Step [135500/138038], Loss: 4.4491, Perplexity: 85.55093\n",
      "Epoch [2/3], Step [135600/138038], Loss: 2.4891, Perplexity: 12.0510\n",
      "Epoch [2/3], Step [135700/138038], Loss: 3.4401, Perplexity: 31.1908\n",
      "Epoch [2/3], Step [135800/138038], Loss: 3.3047, Perplexity: 27.2416\n",
      "Epoch [2/3], Step [135900/138038], Loss: 2.0227, Perplexity: 7.558916\n",
      "Epoch [2/3], Step [136000/138038], Loss: 2.3814, Perplexity: 10.8196\n",
      "Epoch [2/3], Step [136100/138038], Loss: 2.5270, Perplexity: 12.51585\n",
      "Epoch [2/3], Step [136200/138038], Loss: 4.1630, Perplexity: 64.2615\n",
      "Epoch [2/3], Step [136300/138038], Loss: 3.1597, Perplexity: 23.5629\n",
      "Epoch [2/3], Step [136400/138038], Loss: 3.6840, Perplexity: 39.80470\n",
      "Epoch [2/3], Step [136500/138038], Loss: 3.0705, Perplexity: 21.5525\n",
      "Epoch [2/3], Step [136600/138038], Loss: 2.9439, Perplexity: 18.9898\n",
      "Epoch [2/3], Step [136700/138038], Loss: 2.2947, Perplexity: 9.92163\n",
      "Epoch [2/3], Step [136800/138038], Loss: 3.3925, Perplexity: 29.73924\n",
      "Epoch [2/3], Step [136900/138038], Loss: 2.5691, Perplexity: 13.0537\n",
      "Epoch [2/3], Step [137000/138038], Loss: 2.6969, Perplexity: 14.8333\n",
      "Epoch [2/3], Step [137100/138038], Loss: 2.0586, Perplexity: 7.83540\n",
      "Epoch [2/3], Step [137200/138038], Loss: 2.4471, Perplexity: 11.55512\n",
      "Epoch [2/3], Step [137300/138038], Loss: 2.1798, Perplexity: 8.84478\n",
      "Epoch [2/3], Step [137400/138038], Loss: 1.7713, Perplexity: 5.87880\n",
      "Epoch [2/3], Step [137500/138038], Loss: 2.0931, Perplexity: 8.10995\n",
      "Epoch [2/3], Step [137600/138038], Loss: 2.2053, Perplexity: 9.07333\n",
      "Epoch [2/3], Step [137700/138038], Loss: 2.6370, Perplexity: 13.9712\n",
      "Epoch [2/3], Step [137800/138038], Loss: 2.2656, Perplexity: 9.636908\n",
      "Epoch [2/3], Step [137900/138038], Loss: 2.9805, Perplexity: 19.6969\n",
      "Epoch [2/3], Step [138000/138038], Loss: 1.7341, Perplexity: 5.664015\n",
      "Epoch [3/3], Step [100/138038], Loss: 2.7542, Perplexity: 15.7079115\n",
      "Epoch [3/3], Step [200/138038], Loss: 2.4125, Perplexity: 11.1618\n",
      "Epoch [3/3], Step [300/138038], Loss: 3.3435, Perplexity: 28.3169\n",
      "Epoch [3/3], Step [400/138038], Loss: 1.9805, Perplexity: 7.246771\n",
      "Epoch [3/3], Step [500/138038], Loss: 2.2301, Perplexity: 9.30043\n",
      "Epoch [3/3], Step [600/138038], Loss: 3.0903, Perplexity: 21.98391\n",
      "Epoch [3/3], Step [700/138038], Loss: 2.5360, Perplexity: 12.62906\n",
      "Epoch [3/3], Step [800/138038], Loss: 2.3391, Perplexity: 10.3720\n",
      "Epoch [3/3], Step [900/138038], Loss: 3.3035, Perplexity: 27.20798\n",
      "Epoch [3/3], Step [1000/138038], Loss: 2.5922, Perplexity: 13.3597\n",
      "Epoch [3/3], Step [1100/138038], Loss: 2.6926, Perplexity: 14.7698\n",
      "Epoch [3/3], Step [1200/138038], Loss: 3.2861, Perplexity: 26.73728\n",
      "Epoch [3/3], Step [1300/138038], Loss: 3.9772, Perplexity: 53.3686\n",
      "Epoch [3/3], Step [1400/138038], Loss: 2.6644, Perplexity: 14.35901\n",
      "Epoch [3/3], Step [1500/138038], Loss: 3.2938, Perplexity: 26.94401\n",
      "Epoch [3/3], Step [1600/138038], Loss: 2.7111, Perplexity: 15.0455\n",
      "Epoch [3/3], Step [1700/138038], Loss: 4.0168, Perplexity: 55.5208\n",
      "Epoch [3/3], Step [1800/138038], Loss: 1.8069, Perplexity: 6.091630\n",
      "Epoch [3/3], Step [1900/138038], Loss: 2.4216, Perplexity: 11.26421\n",
      "Epoch [3/3], Step [2000/138038], Loss: 2.3193, Perplexity: 10.1689\n",
      "Epoch [3/3], Step [2100/138038], Loss: 1.7957, Perplexity: 6.02399\n",
      "Epoch [3/3], Step [2200/138038], Loss: 3.5447, Perplexity: 34.6299\n",
      "Epoch [3/3], Step [2300/138038], Loss: 2.4703, Perplexity: 11.8263\n",
      "Epoch [3/3], Step [2400/138038], Loss: 3.4698, Perplexity: 32.1304\n",
      "Epoch [3/3], Step [2500/138038], Loss: 2.0765, Perplexity: 7.97637\n",
      "Epoch [3/3], Step [2600/138038], Loss: 2.3104, Perplexity: 10.07897\n",
      "Epoch [3/3], Step [2700/138038], Loss: 3.1809, Perplexity: 24.0675\n",
      "Epoch [3/3], Step [2800/138038], Loss: 3.4828, Perplexity: 32.5508\n",
      "Epoch [3/3], Step [2900/138038], Loss: 2.7329, Perplexity: 15.37698\n",
      "Epoch [3/3], Step [3000/138038], Loss: 2.5311, Perplexity: 12.5673\n",
      "Epoch [3/3], Step [3100/138038], Loss: 1.6058, Perplexity: 4.98189\n",
      "Epoch [3/3], Step [3200/138038], Loss: 1.8210, Perplexity: 6.178081\n",
      "Epoch [3/3], Step [3300/138038], Loss: 3.1619, Perplexity: 23.61607\n",
      "Epoch [3/3], Step [3400/138038], Loss: 2.8067, Perplexity: 16.55551\n",
      "Epoch [3/3], Step [3500/138038], Loss: 2.1861, Perplexity: 8.90081\n",
      "Epoch [3/3], Step [3600/138038], Loss: 2.8387, Perplexity: 17.09303\n",
      "Epoch [3/3], Step [3700/138038], Loss: 2.6289, Perplexity: 13.8582\n",
      "Epoch [3/3], Step [3800/138038], Loss: 2.6322, Perplexity: 13.9049\n",
      "Epoch [3/3], Step [3900/138038], Loss: 2.2922, Perplexity: 9.89712\n",
      "Epoch [3/3], Step [4000/138038], Loss: 2.9794, Perplexity: 19.6766\n",
      "Epoch [3/3], Step [4100/138038], Loss: 3.0972, Perplexity: 22.1367\n",
      "Epoch [3/3], Step [4200/138038], Loss: 3.2750, Perplexity: 26.4419\n",
      "Epoch [3/3], Step [4300/138038], Loss: 2.4766, Perplexity: 11.90076\n",
      "Epoch [3/3], Step [4400/138038], Loss: 2.4376, Perplexity: 11.4458\n",
      "Epoch [3/3], Step [4500/138038], Loss: 2.2314, Perplexity: 9.31331\n",
      "Epoch [3/3], Step [4600/138038], Loss: 2.6645, Perplexity: 14.3613\n",
      "Epoch [3/3], Step [4700/138038], Loss: 2.6511, Perplexity: 14.16919\n",
      "Epoch [3/3], Step [4800/138038], Loss: 1.3821, Perplexity: 3.98351\n",
      "Epoch [3/3], Step [4900/138038], Loss: 2.9436, Perplexity: 18.9841\n",
      "Epoch [3/3], Step [5000/138038], Loss: 2.3060, Perplexity: 10.03451\n",
      "Epoch [3/3], Step [5100/138038], Loss: 2.1112, Perplexity: 8.258138\n",
      "Epoch [3/3], Step [5200/138038], Loss: 3.3050, Perplexity: 27.2488\n",
      "Epoch [3/3], Step [5300/138038], Loss: 2.2278, Perplexity: 9.27921\n",
      "Epoch [3/3], Step [5400/138038], Loss: 3.5156, Perplexity: 33.6375\n",
      "Epoch [3/3], Step [5500/138038], Loss: 2.8102, Perplexity: 16.61281\n",
      "Epoch [3/3], Step [5600/138038], Loss: 3.1652, Perplexity: 23.6946\n",
      "Epoch [3/3], Step [5700/138038], Loss: 3.1002, Perplexity: 22.2013\n",
      "Epoch [3/3], Step [5800/138038], Loss: 2.3281, Perplexity: 10.25872\n",
      "Epoch [3/3], Step [5900/138038], Loss: 3.3393, Perplexity: 28.19996\n",
      "Epoch [3/3], Step [6000/138038], Loss: 3.0913, Perplexity: 22.0054\n",
      "Epoch [3/3], Step [6100/138038], Loss: 2.8202, Perplexity: 16.78001\n",
      "Epoch [3/3], Step [6200/138038], Loss: 1.3012, Perplexity: 3.67373\n",
      "Epoch [3/3], Step [6300/138038], Loss: 1.9463, Perplexity: 7.00302\n",
      "Epoch [3/3], Step [6400/138038], Loss: 2.5082, Perplexity: 12.2832\n",
      "Epoch [3/3], Step [6500/138038], Loss: 2.7387, Perplexity: 15.4675\n",
      "Epoch [3/3], Step [6600/138038], Loss: 2.1555, Perplexity: 8.63190\n",
      "Epoch [3/3], Step [6700/138038], Loss: 3.1421, Perplexity: 23.15205\n",
      "Epoch [3/3], Step [6800/138038], Loss: 2.9004, Perplexity: 18.18220\n",
      "Epoch [3/3], Step [6900/138038], Loss: 2.2731, Perplexity: 9.70975\n",
      "Epoch [3/3], Step [7000/138038], Loss: 2.8306, Perplexity: 16.9559\n",
      "Epoch [3/3], Step [7100/138038], Loss: 2.5925, Perplexity: 13.3627\n",
      "Epoch [3/3], Step [7200/138038], Loss: 2.5628, Perplexity: 12.9718\n",
      "Epoch [3/3], Step [7300/138038], Loss: 1.8713, Perplexity: 6.496742\n",
      "Epoch [3/3], Step [7400/138038], Loss: 2.4911, Perplexity: 12.07438\n",
      "Epoch [3/3], Step [7500/138038], Loss: 3.4230, Perplexity: 30.6612\n",
      "Epoch [3/3], Step [7600/138038], Loss: 3.8202, Perplexity: 45.6151\n",
      "Epoch [3/3], Step [7700/138038], Loss: 2.0003, Perplexity: 7.39153\n",
      "Epoch [3/3], Step [7800/138038], Loss: 3.1354, Perplexity: 22.9978\n",
      "Epoch [3/3], Step [7900/138038], Loss: 2.0992, Perplexity: 8.159469\n",
      "Epoch [3/3], Step [8000/138038], Loss: 2.3507, Perplexity: 10.4930\n",
      "Epoch [3/3], Step [8100/138038], Loss: 3.5482, Perplexity: 34.7524\n",
      "Epoch [3/3], Step [8200/138038], Loss: 2.1043, Perplexity: 8.20147\n",
      "Epoch [3/3], Step [8300/138038], Loss: 1.8643, Perplexity: 6.451291\n",
      "Epoch [3/3], Step [8400/138038], Loss: 2.2301, Perplexity: 9.30081\n",
      "Epoch [3/3], Step [8500/138038], Loss: 3.2866, Perplexity: 26.7513\n",
      "Epoch [3/3], Step [8600/138038], Loss: 2.8339, Perplexity: 17.0110\n",
      "Epoch [3/3], Step [8700/138038], Loss: 2.1995, Perplexity: 9.020460\n",
      "Epoch [3/3], Step [8800/138038], Loss: 2.8606, Perplexity: 17.4716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [8900/138038], Loss: 1.9777, Perplexity: 7.22618\n",
      "Epoch [3/3], Step [9000/138038], Loss: 2.2669, Perplexity: 9.649347\n",
      "Epoch [3/3], Step [9100/138038], Loss: 2.6766, Perplexity: 14.5351\n",
      "Epoch [3/3], Step [9200/138038], Loss: 3.8008, Perplexity: 44.7352\n",
      "Epoch [3/3], Step [9300/138038], Loss: 2.6007, Perplexity: 13.47363\n",
      "Epoch [3/3], Step [9400/138038], Loss: 3.1416, Perplexity: 23.14125\n",
      "Epoch [3/3], Step [9500/138038], Loss: 3.5093, Perplexity: 33.4248\n",
      "Epoch [3/3], Step [9600/138038], Loss: 3.1011, Perplexity: 22.2227\n",
      "Epoch [3/3], Step [9700/138038], Loss: 2.3281, Perplexity: 10.2579\n",
      "Epoch [3/3], Step [9800/138038], Loss: 2.2091, Perplexity: 9.10747\n",
      "Epoch [3/3], Step [9900/138038], Loss: 1.8033, Perplexity: 6.06961\n",
      "Epoch [3/3], Step [10000/138038], Loss: 2.4624, Perplexity: 11.7335\n",
      "Epoch [3/3], Step [10100/138038], Loss: 2.3211, Perplexity: 10.1864\n",
      "Epoch [3/3], Step [10200/138038], Loss: 3.1921, Perplexity: 24.3394\n",
      "Epoch [3/3], Step [10300/138038], Loss: 2.6841, Perplexity: 14.6456\n",
      "Epoch [3/3], Step [10400/138038], Loss: 2.2172, Perplexity: 9.18154\n",
      "Epoch [3/3], Step [10500/138038], Loss: 1.9808, Perplexity: 7.24837\n",
      "Epoch [3/3], Step [10600/138038], Loss: 1.8194, Perplexity: 6.168065\n",
      "Epoch [3/3], Step [10700/138038], Loss: 3.3885, Perplexity: 29.62113\n",
      "Epoch [3/3], Step [10800/138038], Loss: 2.6760, Perplexity: 14.5268\n",
      "Epoch [3/3], Step [10900/138038], Loss: 1.8290, Perplexity: 6.22749\n",
      "Epoch [3/3], Step [11000/138038], Loss: 2.1235, Perplexity: 8.360786\n",
      "Epoch [3/3], Step [11100/138038], Loss: 2.5607, Perplexity: 12.9452\n",
      "Epoch [3/3], Step [11200/138038], Loss: 2.3649, Perplexity: 10.64311\n",
      "Epoch [3/3], Step [11300/138038], Loss: 3.3059, Perplexity: 27.2727\n",
      "Epoch [3/3], Step [11400/138038], Loss: 2.3320, Perplexity: 10.2986\n",
      "Epoch [3/3], Step [11500/138038], Loss: 3.3769, Perplexity: 29.27895\n",
      "Epoch [3/3], Step [11600/138038], Loss: 2.0986, Perplexity: 8.15491\n",
      "Epoch [3/3], Step [11700/138038], Loss: 2.7107, Perplexity: 15.0394\n",
      "Epoch [3/3], Step [11800/138038], Loss: 1.2408, Perplexity: 3.458439\n",
      "Epoch [3/3], Step [11900/138038], Loss: 3.0710, Perplexity: 21.5632\n",
      "Epoch [3/3], Step [12000/138038], Loss: 2.4927, Perplexity: 12.0939\n",
      "Epoch [3/3], Step [12100/138038], Loss: 1.5448, Perplexity: 4.686991\n",
      "Epoch [3/3], Step [12200/138038], Loss: 1.9153, Perplexity: 6.78930\n",
      "Epoch [3/3], Step [12300/138038], Loss: 3.1927, Perplexity: 24.3543\n",
      "Epoch [3/3], Step [12400/138038], Loss: 2.4495, Perplexity: 11.5827\n",
      "Epoch [3/3], Step [12500/138038], Loss: 2.5642, Perplexity: 12.9908\n",
      "Epoch [3/3], Step [12600/138038], Loss: 2.6894, Perplexity: 14.7234\n",
      "Epoch [3/3], Step [12700/138038], Loss: 3.8705, Perplexity: 47.96798\n",
      "Epoch [3/3], Step [12800/138038], Loss: 2.6073, Perplexity: 13.5629\n",
      "Epoch [3/3], Step [12900/138038], Loss: 2.9817, Perplexity: 19.7218\n",
      "Epoch [3/3], Step [13000/138038], Loss: 2.3201, Perplexity: 10.1767\n",
      "Epoch [3/3], Step [13100/138038], Loss: 1.6611, Perplexity: 5.264953\n",
      "Epoch [3/3], Step [13200/138038], Loss: 2.6122, Perplexity: 13.62959\n",
      "Epoch [3/3], Step [13300/138038], Loss: 2.0526, Perplexity: 7.788392\n",
      "Epoch [3/3], Step [13400/138038], Loss: 1.4320, Perplexity: 4.18696\n",
      "Epoch [3/3], Step [13500/138038], Loss: 1.8639, Perplexity: 6.448678\n",
      "Epoch [3/3], Step [13600/138038], Loss: 2.2764, Perplexity: 9.741867\n",
      "Epoch [3/3], Step [13700/138038], Loss: 2.5508, Perplexity: 12.8176\n",
      "Epoch [3/3], Step [13800/138038], Loss: 2.5388, Perplexity: 12.66422\n",
      "Epoch [3/3], Step [13900/138038], Loss: 1.5139, Perplexity: 4.54465\n",
      "Epoch [3/3], Step [14000/138038], Loss: 3.5649, Perplexity: 35.3345\n",
      "Epoch [3/3], Step [14100/138038], Loss: 2.8874, Perplexity: 17.9473\n",
      "Epoch [3/3], Step [14200/138038], Loss: 2.7984, Perplexity: 16.41765\n",
      "Epoch [3/3], Step [14300/138038], Loss: 1.8267, Perplexity: 6.21321\n",
      "Epoch [3/3], Step [14400/138038], Loss: 2.3795, Perplexity: 10.7999\n",
      "Epoch [3/3], Step [14500/138038], Loss: 2.9231, Perplexity: 18.5990\n",
      "Epoch [3/3], Step [14600/138038], Loss: 1.8767, Perplexity: 6.531940\n",
      "Epoch [3/3], Step [14700/138038], Loss: 3.3175, Perplexity: 27.5911\n",
      "Epoch [3/3], Step [14800/138038], Loss: 3.3103, Perplexity: 27.3941\n",
      "Epoch [3/3], Step [14900/138038], Loss: 3.5582, Perplexity: 35.0994\n",
      "Epoch [3/3], Step [15000/138038], Loss: 1.9641, Perplexity: 7.12873\n",
      "Epoch [3/3], Step [15100/138038], Loss: 1.9414, Perplexity: 6.96823\n",
      "Epoch [3/3], Step [15200/138038], Loss: 2.9934, Perplexity: 19.9540\n",
      "Epoch [3/3], Step [15300/138038], Loss: 1.9520, Perplexity: 7.04298\n",
      "Epoch [3/3], Step [15400/138038], Loss: 3.0455, Perplexity: 21.0206\n",
      "Epoch [3/3], Step [15500/138038], Loss: 3.1398, Perplexity: 23.09960\n",
      "Epoch [3/3], Step [15600/138038], Loss: 2.8473, Perplexity: 17.24065\n",
      "Epoch [3/3], Step [15700/138038], Loss: 3.7059, Perplexity: 40.68468\n",
      "Epoch [3/3], Step [15800/138038], Loss: 3.4366, Perplexity: 31.0818\n",
      "Epoch [3/3], Step [15900/138038], Loss: 2.5999, Perplexity: 13.4620\n",
      "Epoch [3/3], Step [16000/138038], Loss: 3.3472, Perplexity: 28.4231\n",
      "Epoch [3/3], Step [16100/138038], Loss: 2.4165, Perplexity: 11.2070\n",
      "Epoch [3/3], Step [16200/138038], Loss: 3.9801, Perplexity: 53.5218\n",
      "Epoch [3/3], Step [16300/138038], Loss: 2.4206, Perplexity: 11.25273\n",
      "Epoch [3/3], Step [16400/138038], Loss: 2.8343, Perplexity: 17.0180\n",
      "Epoch [3/3], Step [16500/138038], Loss: 3.4815, Perplexity: 32.50881\n",
      "Epoch [3/3], Step [16600/138038], Loss: 2.3268, Perplexity: 10.2447\n",
      "Epoch [3/3], Step [16700/138038], Loss: 3.3688, Perplexity: 29.0444\n",
      "Epoch [3/3], Step [16800/138038], Loss: 3.8543, Perplexity: 47.1940\n",
      "Epoch [3/3], Step [16900/138038], Loss: 2.2113, Perplexity: 9.12792\n",
      "Epoch [3/3], Step [17000/138038], Loss: 2.2998, Perplexity: 9.97188\n",
      "Epoch [3/3], Step [17100/138038], Loss: 2.9217, Perplexity: 18.5723\n",
      "Epoch [3/3], Step [17200/138038], Loss: 2.6292, Perplexity: 13.86253\n",
      "Epoch [3/3], Step [17300/138038], Loss: 2.6983, Perplexity: 14.8540\n",
      "Epoch [3/3], Step [17400/138038], Loss: 1.7964, Perplexity: 6.028215\n",
      "Epoch [3/3], Step [17500/138038], Loss: 2.4020, Perplexity: 11.0457\n",
      "Epoch [3/3], Step [17600/138038], Loss: 2.4889, Perplexity: 12.0482\n",
      "Epoch [3/3], Step [17700/138038], Loss: 2.8408, Perplexity: 17.12974\n",
      "Epoch [3/3], Step [17800/138038], Loss: 3.1755, Perplexity: 23.9393\n",
      "Epoch [3/3], Step [17900/138038], Loss: 3.6570, Perplexity: 38.7463\n",
      "Epoch [3/3], Step [18000/138038], Loss: 3.8342, Perplexity: 46.25425\n",
      "Epoch [3/3], Step [18100/138038], Loss: 2.2838, Perplexity: 9.81377\n",
      "Epoch [3/3], Step [18200/138038], Loss: 3.1271, Perplexity: 22.8084\n",
      "Epoch [3/3], Step [18300/138038], Loss: 1.9580, Perplexity: 7.085043\n",
      "Epoch [3/3], Step [18400/138038], Loss: 2.5202, Perplexity: 12.4313\n",
      "Epoch [3/3], Step [18500/138038], Loss: 3.7096, Perplexity: 40.8360\n",
      "Epoch [3/3], Step [18600/138038], Loss: 3.2413, Perplexity: 25.5674\n",
      "Epoch [3/3], Step [18700/138038], Loss: 2.2896, Perplexity: 9.87077\n",
      "Epoch [3/3], Step [18800/138038], Loss: 2.3641, Perplexity: 10.6347\n",
      "Epoch [3/3], Step [18900/138038], Loss: 2.2869, Perplexity: 9.843980\n",
      "Epoch [3/3], Step [19000/138038], Loss: 2.7590, Perplexity: 15.7840\n",
      "Epoch [3/3], Step [19100/138038], Loss: 2.3328, Perplexity: 10.3068\n",
      "Epoch [3/3], Step [19200/138038], Loss: 2.5067, Perplexity: 12.2643\n",
      "Epoch [3/3], Step [19300/138038], Loss: 2.6717, Perplexity: 14.46414\n",
      "Epoch [3/3], Step [19400/138038], Loss: 2.9280, Perplexity: 18.6898\n",
      "Epoch [3/3], Step [19500/138038], Loss: 1.9249, Perplexity: 6.85469\n",
      "Epoch [3/3], Step [19600/138038], Loss: 1.2466, Perplexity: 3.478459\n",
      "Epoch [3/3], Step [19700/138038], Loss: 2.5552, Perplexity: 12.8735\n",
      "Epoch [3/3], Step [19800/138038], Loss: 3.0050, Perplexity: 20.18628\n",
      "Epoch [3/3], Step [19900/138038], Loss: 3.2109, Perplexity: 24.8011\n",
      "Epoch [3/3], Step [20000/138038], Loss: 2.2089, Perplexity: 9.10603\n",
      "Epoch [3/3], Step [20100/138038], Loss: 2.6600, Perplexity: 14.2965\n",
      "Epoch [3/3], Step [20200/138038], Loss: 2.8705, Perplexity: 17.64546\n",
      "Epoch [3/3], Step [20300/138038], Loss: 1.6756, Perplexity: 5.341847\n",
      "Epoch [3/3], Step [20400/138038], Loss: 2.8313, Perplexity: 16.9674\n",
      "Epoch [3/3], Step [20500/138038], Loss: 2.1901, Perplexity: 8.93570\n",
      "Epoch [3/3], Step [20600/138038], Loss: 4.0095, Perplexity: 55.1190\n",
      "Epoch [3/3], Step [20700/138038], Loss: 2.5575, Perplexity: 12.90299\n",
      "Epoch [3/3], Step [20800/138038], Loss: 2.3966, Perplexity: 10.9858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [20900/138038], Loss: 2.8320, Perplexity: 16.9795\n",
      "Epoch [3/3], Step [21000/138038], Loss: 2.1513, Perplexity: 8.59561\n",
      "Epoch [3/3], Step [21100/138038], Loss: 1.8689, Perplexity: 6.480872\n",
      "Epoch [3/3], Step [21200/138038], Loss: 2.2352, Perplexity: 9.34866\n",
      "Epoch [3/3], Step [21300/138038], Loss: 2.0129, Perplexity: 7.485013\n",
      "Epoch [3/3], Step [21400/138038], Loss: 3.3815, Perplexity: 29.4149\n",
      "Epoch [3/3], Step [21500/138038], Loss: 2.4372, Perplexity: 11.4408\n",
      "Epoch [3/3], Step [21600/138038], Loss: 2.2018, Perplexity: 9.04158\n",
      "Epoch [3/3], Step [21700/138038], Loss: 2.7034, Perplexity: 14.9303\n",
      "Epoch [3/3], Step [21800/138038], Loss: 2.5760, Perplexity: 13.1443\n",
      "Epoch [3/3], Step [21900/138038], Loss: 3.4863, Perplexity: 32.6640\n",
      "Epoch [3/3], Step [22000/138038], Loss: 3.4503, Perplexity: 31.51136\n",
      "Epoch [3/3], Step [22100/138038], Loss: 2.3689, Perplexity: 10.6857\n",
      "Epoch [3/3], Step [22200/138038], Loss: 3.1470, Perplexity: 23.2661\n",
      "Epoch [3/3], Step [22300/138038], Loss: 2.2739, Perplexity: 9.71744\n",
      "Epoch [3/3], Step [22400/138038], Loss: 2.4610, Perplexity: 11.7165\n",
      "Epoch [3/3], Step [22500/138038], Loss: 2.4058, Perplexity: 11.0869\n",
      "Epoch [3/3], Step [22600/138038], Loss: 3.3049, Perplexity: 27.24568\n",
      "Epoch [3/3], Step [22700/138038], Loss: 2.3119, Perplexity: 10.0935\n",
      "Epoch [3/3], Step [22800/138038], Loss: 2.8844, Perplexity: 17.8929\n",
      "Epoch [3/3], Step [22900/138038], Loss: 2.3177, Perplexity: 10.15220\n",
      "Epoch [3/3], Step [23000/138038], Loss: 3.6174, Perplexity: 37.2416\n",
      "Epoch [3/3], Step [23100/138038], Loss: 2.2308, Perplexity: 9.307386\n",
      "Epoch [3/3], Step [23200/138038], Loss: 2.1081, Perplexity: 8.232442\n",
      "Epoch [3/3], Step [23300/138038], Loss: 2.9655, Perplexity: 19.4051\n",
      "Epoch [3/3], Step [23400/138038], Loss: 2.6091, Perplexity: 13.5873\n",
      "Epoch [3/3], Step [23500/138038], Loss: 2.2833, Perplexity: 9.80864\n",
      "Epoch [3/3], Step [23600/138038], Loss: 2.1501, Perplexity: 8.58619\n",
      "Epoch [3/3], Step [23700/138038], Loss: 2.7820, Perplexity: 16.15107\n",
      "Epoch [3/3], Step [23800/138038], Loss: 2.1348, Perplexity: 8.45520\n",
      "Epoch [3/3], Step [23900/138038], Loss: 2.2464, Perplexity: 9.453403\n",
      "Epoch [3/3], Step [24000/138038], Loss: 2.4865, Perplexity: 12.0191\n",
      "Epoch [3/3], Step [24100/138038], Loss: 2.8481, Perplexity: 17.2544\n",
      "Epoch [3/3], Step [24200/138038], Loss: 2.0131, Perplexity: 7.48658\n",
      "Epoch [3/3], Step [24300/138038], Loss: 2.0074, Perplexity: 7.44415\n",
      "Epoch [3/3], Step [24400/138038], Loss: 1.7813, Perplexity: 5.93792\n",
      "Epoch [3/3], Step [24500/138038], Loss: 2.8583, Perplexity: 17.43193\n",
      "Epoch [3/3], Step [24600/138038], Loss: 2.6033, Perplexity: 13.5085\n",
      "Epoch [3/3], Step [24700/138038], Loss: 2.4127, Perplexity: 11.1641\n",
      "Epoch [3/3], Step [24800/138038], Loss: 2.9600, Perplexity: 19.29751\n",
      "Epoch [3/3], Step [24900/138038], Loss: 2.8720, Perplexity: 17.6723\n",
      "Epoch [3/3], Step [25000/138038], Loss: 3.1896, Perplexity: 24.2783\n",
      "Epoch [3/3], Step [25100/138038], Loss: 2.7688, Perplexity: 15.93901\n",
      "Epoch [3/3], Step [25200/138038], Loss: 2.2854, Perplexity: 9.829841\n",
      "Epoch [3/3], Step [25300/138038], Loss: 2.8899, Perplexity: 17.9909\n",
      "Epoch [3/3], Step [25400/138038], Loss: 2.7314, Perplexity: 15.3541\n",
      "Epoch [3/3], Step [25500/138038], Loss: 2.2213, Perplexity: 9.21983\n",
      "Epoch [3/3], Step [25600/138038], Loss: 1.8297, Perplexity: 6.231843\n",
      "Epoch [3/3], Step [25700/138038], Loss: 1.4217, Perplexity: 4.14422\n",
      "Epoch [3/3], Step [25800/138038], Loss: 2.7928, Perplexity: 16.32712\n",
      "Epoch [3/3], Step [25900/138038], Loss: 3.3531, Perplexity: 28.5913\n",
      "Epoch [3/3], Step [26000/138038], Loss: 2.7060, Perplexity: 14.9697\n",
      "Epoch [3/3], Step [26100/138038], Loss: 3.6668, Perplexity: 39.1266\n",
      "Epoch [3/3], Step [26200/138038], Loss: 3.2218, Perplexity: 25.07289\n",
      "Epoch [3/3], Step [26300/138038], Loss: 1.6903, Perplexity: 5.42125\n",
      "Epoch [3/3], Step [26400/138038], Loss: 3.1616, Perplexity: 23.6092\n",
      "Epoch [3/3], Step [26500/138038], Loss: 2.6399, Perplexity: 14.01116\n",
      "Epoch [3/3], Step [26600/138038], Loss: 1.8360, Perplexity: 6.27166\n",
      "Epoch [3/3], Step [26700/138038], Loss: 3.5583, Perplexity: 35.10479\n",
      "Epoch [3/3], Step [26800/138038], Loss: 2.5118, Perplexity: 12.32723\n",
      "Epoch [3/3], Step [26900/138038], Loss: 2.0663, Perplexity: 7.89564\n",
      "Epoch [3/3], Step [27000/138038], Loss: 2.3648, Perplexity: 10.64161\n",
      "Epoch [3/3], Step [27100/138038], Loss: 3.3674, Perplexity: 29.00314\n",
      "Epoch [3/3], Step [27200/138038], Loss: 2.7600, Perplexity: 15.7996\n",
      "Epoch [3/3], Step [27300/138038], Loss: 2.9908, Perplexity: 19.9024\n",
      "Epoch [3/3], Step [27400/138038], Loss: 2.9633, Perplexity: 19.3616\n",
      "Epoch [3/3], Step [27500/138038], Loss: 1.7707, Perplexity: 5.87505\n",
      "Epoch [3/3], Step [27600/138038], Loss: 1.9021, Perplexity: 6.70018\n",
      "Epoch [3/3], Step [27700/138038], Loss: 1.9590, Perplexity: 7.09222\n",
      "Epoch [3/3], Step [27800/138038], Loss: 2.7116, Perplexity: 15.0529\n",
      "Epoch [3/3], Step [27900/138038], Loss: 3.4563, Perplexity: 31.7002\n",
      "Epoch [3/3], Step [28000/138038], Loss: 2.9611, Perplexity: 19.3199\n",
      "Epoch [3/3], Step [28100/138038], Loss: 1.4492, Perplexity: 4.25966\n",
      "Epoch [3/3], Step [28200/138038], Loss: 1.8027, Perplexity: 6.06588\n",
      "Epoch [3/3], Step [28300/138038], Loss: 3.0022, Perplexity: 20.1289\n",
      "Epoch [3/3], Step [28400/138038], Loss: 2.8703, Perplexity: 17.6418\n",
      "Epoch [3/3], Step [28500/138038], Loss: 3.1422, Perplexity: 23.1540\n",
      "Epoch [3/3], Step [28600/138038], Loss: 2.6088, Perplexity: 13.58256\n",
      "Epoch [3/3], Step [28700/138038], Loss: 2.5240, Perplexity: 12.47804\n",
      "Epoch [3/3], Step [28800/138038], Loss: 2.1592, Perplexity: 8.66398\n",
      "Epoch [3/3], Step [28900/138038], Loss: 4.1798, Perplexity: 65.3504\n",
      "Epoch [3/3], Step [29000/138038], Loss: 3.2068, Perplexity: 24.6993\n",
      "Epoch [3/3], Step [29100/138038], Loss: 2.4537, Perplexity: 11.6312\n",
      "Epoch [3/3], Step [29200/138038], Loss: 2.9816, Perplexity: 19.7201\n",
      "Epoch [3/3], Step [29300/138038], Loss: 2.6454, Perplexity: 14.0888\n",
      "Epoch [3/3], Step [29400/138038], Loss: 2.9741, Perplexity: 19.5717\n",
      "Epoch [3/3], Step [29500/138038], Loss: 4.6639, Perplexity: 106.0515\n",
      "Epoch [3/3], Step [29600/138038], Loss: 2.1404, Perplexity: 8.502913\n",
      "Epoch [3/3], Step [29700/138038], Loss: 3.9565, Perplexity: 52.2732\n",
      "Epoch [3/3], Step [29800/138038], Loss: 3.0841, Perplexity: 21.8484\n",
      "Epoch [3/3], Step [29900/138038], Loss: 1.9137, Perplexity: 6.77824\n",
      "Epoch [3/3], Step [30000/138038], Loss: 2.4870, Perplexity: 12.0256\n",
      "Epoch [3/3], Step [30100/138038], Loss: 2.6317, Perplexity: 13.8978\n",
      "Epoch [3/3], Step [30200/138038], Loss: 1.4911, Perplexity: 4.44214\n",
      "Epoch [3/3], Step [30300/138038], Loss: 2.1754, Perplexity: 8.80611\n",
      "Epoch [3/3], Step [30400/138038], Loss: 2.4790, Perplexity: 11.9298\n",
      "Epoch [3/3], Step [30500/138038], Loss: 2.7118, Perplexity: 15.05570\n",
      "Epoch [3/3], Step [30600/138038], Loss: 4.0656, Perplexity: 58.2983\n",
      "Epoch [3/3], Step [30700/138038], Loss: 2.8449, Perplexity: 17.2003\n",
      "Epoch [3/3], Step [30800/138038], Loss: 2.5543, Perplexity: 12.86229\n",
      "Epoch [3/3], Step [30900/138038], Loss: 2.4915, Perplexity: 12.07917\n",
      "Epoch [3/3], Step [31000/138038], Loss: 2.7751, Perplexity: 16.0396\n",
      "Epoch [3/3], Step [31100/138038], Loss: 2.0953, Perplexity: 8.12815\n",
      "Epoch [3/3], Step [31200/138038], Loss: 2.0770, Perplexity: 7.980317\n",
      "Epoch [3/3], Step [31300/138038], Loss: 2.9336, Perplexity: 18.7949\n",
      "Epoch [3/3], Step [31400/138038], Loss: 2.6508, Perplexity: 14.16551\n",
      "Epoch [3/3], Step [31500/138038], Loss: 2.0337, Perplexity: 7.64224\n",
      "Epoch [3/3], Step [31600/138038], Loss: 2.0277, Perplexity: 7.59672\n",
      "Epoch [3/3], Step [31700/138038], Loss: 2.5178, Perplexity: 12.4011\n",
      "Epoch [3/3], Step [31800/138038], Loss: 2.4699, Perplexity: 11.82148\n",
      "Epoch [3/3], Step [31900/138038], Loss: 3.1710, Perplexity: 23.8322\n",
      "Epoch [3/3], Step [32000/138038], Loss: 1.9030, Perplexity: 6.70629\n",
      "Epoch [3/3], Step [32100/138038], Loss: 2.4413, Perplexity: 11.4878\n",
      "Epoch [3/3], Step [32200/138038], Loss: 2.5757, Perplexity: 13.1405\n",
      "Epoch [3/3], Step [32300/138038], Loss: 3.1454, Perplexity: 23.2290\n",
      "Epoch [3/3], Step [32400/138038], Loss: 2.6927, Perplexity: 14.77096\n",
      "Epoch [3/3], Step [32500/138038], Loss: 2.3382, Perplexity: 10.36216\n",
      "Epoch [3/3], Step [32600/138038], Loss: 1.7490, Perplexity: 5.74862\n",
      "Epoch [3/3], Step [32700/138038], Loss: 1.9298, Perplexity: 6.887936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [32800/138038], Loss: 2.0560, Perplexity: 7.81441\n",
      "Epoch [3/3], Step [32900/138038], Loss: 1.9933, Perplexity: 7.33961\n",
      "Epoch [3/3], Step [33000/138038], Loss: 2.4178, Perplexity: 11.22099\n",
      "Epoch [3/3], Step [33100/138038], Loss: 2.3285, Perplexity: 10.26269\n",
      "Epoch [3/3], Step [33200/138038], Loss: 2.2336, Perplexity: 9.33324\n",
      "Epoch [3/3], Step [33300/138038], Loss: 2.3821, Perplexity: 10.8277\n",
      "Epoch [3/3], Step [33400/138038], Loss: 2.1233, Perplexity: 8.35886\n",
      "Epoch [3/3], Step [33500/138038], Loss: 4.1000, Perplexity: 60.34331\n",
      "Epoch [3/3], Step [33600/138038], Loss: 2.8444, Perplexity: 17.1914\n",
      "Epoch [3/3], Step [33700/138038], Loss: 2.4116, Perplexity: 11.1513\n",
      "Epoch [3/3], Step [33800/138038], Loss: 2.4320, Perplexity: 11.38171\n",
      "Epoch [3/3], Step [33900/138038], Loss: 1.8222, Perplexity: 6.18527\n",
      "Epoch [3/3], Step [34000/138038], Loss: 3.1101, Perplexity: 22.4232\n",
      "Epoch [3/3], Step [34100/138038], Loss: 5.1113, Perplexity: 165.8897\n",
      "Epoch [3/3], Step [34200/138038], Loss: 2.1314, Perplexity: 8.426409\n",
      "Epoch [3/3], Step [34300/138038], Loss: 4.0190, Perplexity: 55.6452\n",
      "Epoch [3/3], Step [34400/138038], Loss: 2.2346, Perplexity: 9.343078\n",
      "Epoch [3/3], Step [34500/138038], Loss: 3.6310, Perplexity: 37.7500\n",
      "Epoch [3/3], Step [34600/138038], Loss: 3.4200, Perplexity: 30.57069\n",
      "Epoch [3/3], Step [34700/138038], Loss: 2.5169, Perplexity: 12.3900\n",
      "Epoch [3/3], Step [34800/138038], Loss: 3.4405, Perplexity: 31.2020\n",
      "Epoch [3/3], Step [34900/138038], Loss: 2.5777, Perplexity: 13.1674\n",
      "Epoch [3/3], Step [35000/138038], Loss: 3.2213, Perplexity: 25.0604\n",
      "Epoch [3/3], Step [35100/138038], Loss: 2.8766, Perplexity: 17.7543\n",
      "Epoch [3/3], Step [35200/138038], Loss: 2.2673, Perplexity: 9.65291\n",
      "Epoch [3/3], Step [35300/138038], Loss: 2.4571, Perplexity: 11.67133\n",
      "Epoch [3/3], Step [35400/138038], Loss: 1.8851, Perplexity: 6.58722\n",
      "Epoch [3/3], Step [35500/138038], Loss: 1.7241, Perplexity: 5.60723\n",
      "Epoch [3/3], Step [35600/138038], Loss: 2.4810, Perplexity: 11.9531\n",
      "Epoch [3/3], Step [35700/138038], Loss: 3.9283, Perplexity: 50.8214\n",
      "Epoch [3/3], Step [35800/138038], Loss: 2.5552, Perplexity: 12.8744\n",
      "Epoch [3/3], Step [35900/138038], Loss: 2.0878, Perplexity: 8.06748\n",
      "Epoch [3/3], Step [36000/138038], Loss: 1.6594, Perplexity: 5.25648\n",
      "Epoch [3/3], Step [36100/138038], Loss: 2.5213, Perplexity: 12.4445\n",
      "Epoch [3/3], Step [36200/138038], Loss: 3.1609, Perplexity: 23.5907\n",
      "Epoch [3/3], Step [36300/138038], Loss: 2.3685, Perplexity: 10.6816\n",
      "Epoch [3/3], Step [36400/138038], Loss: 2.6626, Perplexity: 14.33317\n",
      "Epoch [3/3], Step [36500/138038], Loss: 2.6393, Perplexity: 14.0036\n",
      "Epoch [3/3], Step [36600/138038], Loss: 2.1507, Perplexity: 8.59078\n",
      "Epoch [3/3], Step [36700/138038], Loss: 2.5624, Perplexity: 12.96758\n",
      "Epoch [3/3], Step [36800/138038], Loss: 1.8859, Perplexity: 6.592217\n",
      "Epoch [3/3], Step [36900/138038], Loss: 3.1250, Perplexity: 22.75980\n",
      "Epoch [3/3], Step [37000/138038], Loss: 2.3391, Perplexity: 10.37157\n",
      "Epoch [3/3], Step [37100/138038], Loss: 2.2673, Perplexity: 9.653108\n",
      "Epoch [3/3], Step [37200/138038], Loss: 2.5370, Perplexity: 12.6420\n",
      "Epoch [3/3], Step [37300/138038], Loss: 2.5266, Perplexity: 12.5108\n",
      "Epoch [3/3], Step [37400/138038], Loss: 2.1250, Perplexity: 8.37287\n",
      "Epoch [3/3], Step [37500/138038], Loss: 4.4915, Perplexity: 89.2514\n",
      "Epoch [3/3], Step [37600/138038], Loss: 2.8262, Perplexity: 16.8804\n",
      "Epoch [3/3], Step [37700/138038], Loss: 2.4856, Perplexity: 12.0089\n",
      "Epoch [3/3], Step [37800/138038], Loss: 2.2575, Perplexity: 9.558745\n",
      "Epoch [3/3], Step [37900/138038], Loss: 2.8088, Perplexity: 16.5900\n",
      "Epoch [3/3], Step [38000/138038], Loss: 2.6346, Perplexity: 13.93832\n",
      "Epoch [3/3], Step [38100/138038], Loss: 3.6582, Perplexity: 38.7923\n",
      "Epoch [3/3], Step [38200/138038], Loss: 2.0608, Perplexity: 7.85257\n",
      "Epoch [3/3], Step [38300/138038], Loss: 2.3035, Perplexity: 10.00919\n",
      "Epoch [3/3], Step [38400/138038], Loss: 2.8703, Perplexity: 17.6429\n",
      "Epoch [3/3], Step [38500/138038], Loss: 3.0997, Perplexity: 22.1920\n",
      "Epoch [3/3], Step [38600/138038], Loss: 2.7121, Perplexity: 15.0603\n",
      "Epoch [3/3], Step [38700/138038], Loss: 2.7711, Perplexity: 15.97557\n",
      "Epoch [3/3], Step [38800/138038], Loss: 3.1930, Perplexity: 24.36208\n",
      "Epoch [3/3], Step [38900/138038], Loss: 3.7820, Perplexity: 43.9059\n",
      "Epoch [3/3], Step [39000/138038], Loss: 3.1829, Perplexity: 24.11682\n",
      "Epoch [3/3], Step [39100/138038], Loss: 3.3309, Perplexity: 27.9644\n",
      "Epoch [3/3], Step [39200/138038], Loss: 4.1489, Perplexity: 63.36238\n",
      "Epoch [3/3], Step [39300/138038], Loss: 2.4917, Perplexity: 12.0822\n",
      "Epoch [3/3], Step [39400/138038], Loss: 2.6473, Perplexity: 14.11549\n",
      "Epoch [3/3], Step [39500/138038], Loss: 2.2772, Perplexity: 9.749702\n",
      "Epoch [3/3], Step [39600/138038], Loss: 2.7330, Perplexity: 15.3793\n",
      "Epoch [3/3], Step [39700/138038], Loss: 3.0040, Perplexity: 20.16584\n",
      "Epoch [3/3], Step [39800/138038], Loss: 3.3053, Perplexity: 27.25782\n",
      "Epoch [3/3], Step [39900/138038], Loss: 2.6287, Perplexity: 13.85621\n",
      "Epoch [3/3], Step [40000/138038], Loss: 2.3526, Perplexity: 10.51291\n",
      "Epoch [3/3], Step [40100/138038], Loss: 2.4260, Perplexity: 11.3137\n",
      "Epoch [3/3], Step [40200/138038], Loss: 1.4994, Perplexity: 4.47926\n",
      "Epoch [3/3], Step [40300/138038], Loss: 3.4946, Perplexity: 32.9372\n",
      "Epoch [3/3], Step [40400/138038], Loss: 2.4093, Perplexity: 11.1265\n",
      "Epoch [3/3], Step [40500/138038], Loss: 2.0089, Perplexity: 7.455332\n",
      "Epoch [3/3], Step [40600/138038], Loss: 2.7006, Perplexity: 14.88900\n",
      "Epoch [3/3], Step [40700/138038], Loss: 1.9030, Perplexity: 6.70584\n",
      "Epoch [3/3], Step [40800/138038], Loss: 2.8303, Perplexity: 16.9509\n",
      "Epoch [3/3], Step [40900/138038], Loss: 1.5695, Perplexity: 4.80413\n",
      "Epoch [3/3], Step [41000/138038], Loss: 2.0352, Perplexity: 7.65388\n",
      "Epoch [3/3], Step [41100/138038], Loss: 2.7255, Perplexity: 15.26416\n",
      "Epoch [3/3], Step [41200/138038], Loss: 3.3705, Perplexity: 29.09429\n",
      "Epoch [3/3], Step [41300/138038], Loss: 2.7095, Perplexity: 15.0223\n",
      "Epoch [3/3], Step [41400/138038], Loss: 2.1635, Perplexity: 8.70146\n",
      "Epoch [3/3], Step [41500/138038], Loss: 1.7278, Perplexity: 5.62839\n",
      "Epoch [3/3], Step [41600/138038], Loss: 2.8016, Perplexity: 16.4710\n",
      "Epoch [3/3], Step [41700/138038], Loss: 3.4902, Perplexity: 32.7931\n",
      "Epoch [3/3], Step [41800/138038], Loss: 2.7514, Perplexity: 15.66511\n",
      "Epoch [3/3], Step [41900/138038], Loss: 5.0209, Perplexity: 151.5443\n",
      "Epoch [3/3], Step [42000/138038], Loss: 1.7653, Perplexity: 5.84351\n",
      "Epoch [3/3], Step [42100/138038], Loss: 2.0337, Perplexity: 7.64263\n",
      "Epoch [3/3], Step [42200/138038], Loss: 2.9601, Perplexity: 19.3008\n",
      "Epoch [3/3], Step [42300/138038], Loss: 1.5802, Perplexity: 4.85613\n",
      "Epoch [3/3], Step [42400/138038], Loss: 2.5788, Perplexity: 13.18177\n",
      "Epoch [3/3], Step [42500/138038], Loss: 2.4492, Perplexity: 11.5792\n",
      "Epoch [3/3], Step [42600/138038], Loss: 2.6977, Perplexity: 14.8450\n",
      "Epoch [3/3], Step [42700/138038], Loss: 3.9486, Perplexity: 51.8637\n",
      "Epoch [3/3], Step [42800/138038], Loss: 3.5518, Perplexity: 34.87522\n",
      "Epoch [3/3], Step [42900/138038], Loss: 2.6422, Perplexity: 14.0434\n",
      "Epoch [3/3], Step [43000/138038], Loss: 2.6786, Perplexity: 14.5647\n",
      "Epoch [3/3], Step [43100/138038], Loss: 3.6413, Perplexity: 38.14285\n",
      "Epoch [3/3], Step [43200/138038], Loss: 1.4656, Perplexity: 4.33004\n",
      "Epoch [3/3], Step [43300/138038], Loss: 2.4122, Perplexity: 11.1587\n",
      "Epoch [3/3], Step [43400/138038], Loss: 2.0749, Perplexity: 7.96370\n",
      "Epoch [3/3], Step [43500/138038], Loss: 2.9879, Perplexity: 19.8433\n",
      "Epoch [3/3], Step [43600/138038], Loss: 3.5246, Perplexity: 33.94136\n",
      "Epoch [3/3], Step [43700/138038], Loss: 2.0656, Perplexity: 7.89011\n",
      "Epoch [3/3], Step [43800/138038], Loss: 2.4210, Perplexity: 11.25729\n",
      "Epoch [3/3], Step [43900/138038], Loss: 3.7332, Perplexity: 41.8127\n",
      "Epoch [3/3], Step [44000/138038], Loss: 3.2026, Perplexity: 24.5953\n",
      "Epoch [3/3], Step [44100/138038], Loss: 2.9668, Perplexity: 19.4289\n",
      "Epoch [3/3], Step [44200/138038], Loss: 1.9915, Perplexity: 7.32627\n",
      "Epoch [3/3], Step [44300/138038], Loss: 3.2544, Perplexity: 25.90322\n",
      "Epoch [3/3], Step [44400/138038], Loss: 3.1433, Perplexity: 23.1801\n",
      "Epoch [3/3], Step [44500/138038], Loss: 3.1480, Perplexity: 23.28966\n",
      "Epoch [3/3], Step [44600/138038], Loss: 2.6737, Perplexity: 14.4930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [44700/138038], Loss: 2.0753, Perplexity: 7.967375\n",
      "Epoch [3/3], Step [44800/138038], Loss: 2.2745, Perplexity: 9.72285\n",
      "Epoch [3/3], Step [44900/138038], Loss: 1.6817, Perplexity: 5.37502\n",
      "Epoch [3/3], Step [45000/138038], Loss: 2.6777, Perplexity: 14.5517\n",
      "Epoch [3/3], Step [45100/138038], Loss: 1.9640, Perplexity: 7.12817\n",
      "Epoch [3/3], Step [45200/138038], Loss: 1.7959, Perplexity: 6.02502\n",
      "Epoch [3/3], Step [45300/138038], Loss: 2.3102, Perplexity: 10.07613\n",
      "Epoch [3/3], Step [45400/138038], Loss: 2.6508, Perplexity: 14.1659\n",
      "Epoch [3/3], Step [45500/138038], Loss: 2.9498, Perplexity: 19.1016\n",
      "Epoch [3/3], Step [45600/138038], Loss: 2.5667, Perplexity: 13.0233\n",
      "Epoch [3/3], Step [45700/138038], Loss: 1.9132, Perplexity: 6.77491\n",
      "Epoch [3/3], Step [45800/138038], Loss: 1.9510, Perplexity: 7.03578\n",
      "Epoch [3/3], Step [45900/138038], Loss: 2.5138, Perplexity: 12.3522\n",
      "Epoch [3/3], Step [46000/138038], Loss: 1.5065, Perplexity: 4.51085\n",
      "Epoch [3/3], Step [46100/138038], Loss: 3.4021, Perplexity: 30.0272\n",
      "Epoch [3/3], Step [46200/138038], Loss: 3.1946, Perplexity: 24.4008\n",
      "Epoch [3/3], Step [46300/138038], Loss: 2.9962, Perplexity: 20.0092\n",
      "Epoch [3/3], Step [46400/138038], Loss: 3.3567, Perplexity: 28.69580\n",
      "Epoch [3/3], Step [46500/138038], Loss: 1.9936, Perplexity: 7.34169\n",
      "Epoch [3/3], Step [46600/138038], Loss: 1.9142, Perplexity: 6.781432\n",
      "Epoch [3/3], Step [46700/138038], Loss: 2.5568, Perplexity: 12.89449\n",
      "Epoch [3/3], Step [46800/138038], Loss: 2.2630, Perplexity: 9.61226\n",
      "Epoch [3/3], Step [46900/138038], Loss: 2.8835, Perplexity: 17.87632\n",
      "Epoch [3/3], Step [47000/138038], Loss: 3.5555, Perplexity: 35.00708\n",
      "Epoch [3/3], Step [47100/138038], Loss: 2.5320, Perplexity: 12.57910\n",
      "Epoch [3/3], Step [47200/138038], Loss: 3.7886, Perplexity: 44.1962\n",
      "Epoch [3/3], Step [47300/138038], Loss: 2.9246, Perplexity: 18.6260\n",
      "Epoch [3/3], Step [47400/138038], Loss: 2.4353, Perplexity: 11.41913\n",
      "Epoch [3/3], Step [47500/138038], Loss: 1.9671, Perplexity: 7.14994\n",
      "Epoch [3/3], Step [47600/138038], Loss: 1.9846, Perplexity: 7.27628\n",
      "Epoch [3/3], Step [47700/138038], Loss: 2.7675, Perplexity: 15.9186\n",
      "Epoch [3/3], Step [47800/138038], Loss: 2.8131, Perplexity: 16.6612\n",
      "Epoch [3/3], Step [47900/138038], Loss: 2.8570, Perplexity: 17.4086\n",
      "Epoch [3/3], Step [48000/138038], Loss: 2.3724, Perplexity: 10.7226\n",
      "Epoch [3/3], Step [48100/138038], Loss: 1.6485, Perplexity: 5.19892\n",
      "Epoch [3/3], Step [48200/138038], Loss: 2.0239, Perplexity: 7.56789\n",
      "Epoch [3/3], Step [48300/138038], Loss: 1.5755, Perplexity: 4.83347\n",
      "Epoch [3/3], Step [48400/138038], Loss: 1.7547, Perplexity: 5.78199\n",
      "Epoch [3/3], Step [48500/138038], Loss: 3.0183, Perplexity: 20.4563\n",
      "Epoch [3/3], Step [48600/138038], Loss: 2.2197, Perplexity: 9.20437\n",
      "Epoch [3/3], Step [48700/138038], Loss: 3.4678, Perplexity: 32.0663\n",
      "Epoch [3/3], Step [48800/138038], Loss: 1.6013, Perplexity: 4.95960\n",
      "Epoch [3/3], Step [48900/138038], Loss: 2.2686, Perplexity: 9.66629\n",
      "Epoch [3/3], Step [49000/138038], Loss: 4.5918, Perplexity: 98.66711\n",
      "Epoch [3/3], Step [49100/138038], Loss: 3.0051, Perplexity: 20.18782\n",
      "Epoch [3/3], Step [49200/138038], Loss: 3.5854, Perplexity: 36.0673\n",
      "Epoch [3/3], Step [49300/138038], Loss: 4.2473, Perplexity: 69.91509\n",
      "Epoch [3/3], Step [49400/138038], Loss: 3.4605, Perplexity: 31.83218\n",
      "Epoch [3/3], Step [49500/138038], Loss: 1.5211, Perplexity: 4.577374\n",
      "Epoch [3/3], Step [49600/138038], Loss: 1.8859, Perplexity: 6.59232\n",
      "Epoch [3/3], Step [49700/138038], Loss: 1.9121, Perplexity: 6.76738\n",
      "Epoch [3/3], Step [49800/138038], Loss: 2.3855, Perplexity: 10.8649\n",
      "Epoch [3/3], Step [49900/138038], Loss: 3.4496, Perplexity: 31.48834\n",
      "Epoch [3/3], Step [50000/138038], Loss: 3.1970, Perplexity: 24.4590\n",
      "Epoch [3/3], Step [50100/138038], Loss: 2.9866, Perplexity: 19.8180\n",
      "Epoch [3/3], Step [50200/138038], Loss: 1.8401, Perplexity: 6.29748\n",
      "Epoch [3/3], Step [50300/138038], Loss: 2.5675, Perplexity: 13.03286\n",
      "Epoch [3/3], Step [50400/138038], Loss: 3.7281, Perplexity: 41.5992\n",
      "Epoch [3/3], Step [50500/138038], Loss: 1.5819, Perplexity: 4.864281\n",
      "Epoch [3/3], Step [50600/138038], Loss: 1.8129, Perplexity: 6.12803\n",
      "Epoch [3/3], Step [50700/138038], Loss: 2.6899, Perplexity: 14.7305\n",
      "Epoch [3/3], Step [50800/138038], Loss: 2.3125, Perplexity: 10.1000\n",
      "Epoch [3/3], Step [50900/138038], Loss: 1.9650, Perplexity: 7.13476\n",
      "Epoch [3/3], Step [51000/138038], Loss: 2.4759, Perplexity: 11.8929\n",
      "Epoch [3/3], Step [51100/138038], Loss: 2.4526, Perplexity: 11.61875\n",
      "Epoch [3/3], Step [51200/138038], Loss: 2.6203, Perplexity: 13.7395\n",
      "Epoch [3/3], Step [51300/138038], Loss: 2.0766, Perplexity: 7.97757\n",
      "Epoch [3/3], Step [51400/138038], Loss: 3.0998, Perplexity: 22.19438\n",
      "Epoch [3/3], Step [51500/138038], Loss: 1.9208, Perplexity: 6.82675\n",
      "Epoch [3/3], Step [51600/138038], Loss: 2.9021, Perplexity: 18.2129\n",
      "Epoch [3/3], Step [51700/138038], Loss: 2.9548, Perplexity: 19.1975\n",
      "Epoch [3/3], Step [51800/138038], Loss: 2.1985, Perplexity: 9.01173\n",
      "Epoch [3/3], Step [51900/138038], Loss: 2.2772, Perplexity: 9.74977\n",
      "Epoch [3/3], Step [52000/138038], Loss: 2.6454, Perplexity: 14.0885\n",
      "Epoch [3/3], Step [52100/138038], Loss: 2.4744, Perplexity: 11.8749\n",
      "Epoch [3/3], Step [52200/138038], Loss: 2.9223, Perplexity: 18.5840\n",
      "Epoch [3/3], Step [52300/138038], Loss: 3.5969, Perplexity: 36.48447\n",
      "Epoch [3/3], Step [52400/138038], Loss: 2.6198, Perplexity: 13.7335\n",
      "Epoch [3/3], Step [52500/138038], Loss: 2.6673, Perplexity: 14.4011\n",
      "Epoch [3/3], Step [52600/138038], Loss: 2.8130, Perplexity: 16.6603\n",
      "Epoch [3/3], Step [52700/138038], Loss: 2.7551, Perplexity: 15.7231\n",
      "Epoch [3/3], Step [52800/138038], Loss: 2.3716, Perplexity: 10.7148\n",
      "Epoch [3/3], Step [52900/138038], Loss: 3.3639, Perplexity: 28.9004\n",
      "Epoch [3/3], Step [53000/138038], Loss: 2.7655, Perplexity: 15.88761\n",
      "Epoch [3/3], Step [53100/138038], Loss: 2.3181, Perplexity: 10.1559\n",
      "Epoch [3/3], Step [53200/138038], Loss: 3.2577, Perplexity: 25.9884\n",
      "Epoch [3/3], Step [53300/138038], Loss: 1.6790, Perplexity: 5.36006\n",
      "Epoch [3/3], Step [53400/138038], Loss: 1.8576, Perplexity: 6.40841\n",
      "Epoch [3/3], Step [53500/138038], Loss: 2.0265, Perplexity: 7.58726\n",
      "Epoch [3/3], Step [53600/138038], Loss: 2.9543, Perplexity: 19.1880\n",
      "Epoch [3/3], Step [53700/138038], Loss: 2.5969, Perplexity: 13.4220\n",
      "Epoch [3/3], Step [53800/138038], Loss: 1.2983, Perplexity: 3.66317\n",
      "Epoch [3/3], Step [53900/138038], Loss: 3.2024, Perplexity: 24.5925\n",
      "Epoch [3/3], Step [54000/138038], Loss: 1.9821, Perplexity: 7.258177\n",
      "Epoch [3/3], Step [54100/138038], Loss: 1.8327, Perplexity: 6.25095\n",
      "Epoch [3/3], Step [54200/138038], Loss: 2.2902, Perplexity: 9.87698\n",
      "Epoch [3/3], Step [54300/138038], Loss: 2.0167, Perplexity: 7.51368\n",
      "Epoch [3/3], Step [54400/138038], Loss: 3.2626, Perplexity: 26.1185\n",
      "Epoch [3/3], Step [54500/138038], Loss: 2.9747, Perplexity: 19.5833\n",
      "Epoch [3/3], Step [54600/138038], Loss: 2.3781, Perplexity: 10.7842\n",
      "Epoch [3/3], Step [54700/138038], Loss: 3.1490, Perplexity: 23.3122\n",
      "Epoch [3/3], Step [54800/138038], Loss: 3.1432, Perplexity: 23.1776\n",
      "Epoch [3/3], Step [54900/138038], Loss: 1.9638, Perplexity: 7.12625\n",
      "Epoch [3/3], Step [55000/138038], Loss: 2.2213, Perplexity: 9.219723\n",
      "Epoch [3/3], Step [55100/138038], Loss: 1.9468, Perplexity: 7.00632\n",
      "Epoch [3/3], Step [55200/138038], Loss: 2.6116, Perplexity: 13.6203\n",
      "Epoch [3/3], Step [55300/138038], Loss: 1.4955, Perplexity: 4.46168\n",
      "Epoch [3/3], Step [55400/138038], Loss: 2.4092, Perplexity: 11.1250\n",
      "Epoch [3/3], Step [55500/138038], Loss: 2.5198, Perplexity: 12.4262\n",
      "Epoch [3/3], Step [55600/138038], Loss: 2.8876, Perplexity: 17.95048\n",
      "Epoch [3/3], Step [55700/138038], Loss: 2.9584, Perplexity: 19.2662\n",
      "Epoch [3/3], Step [55800/138038], Loss: 2.9094, Perplexity: 18.34668\n",
      "Epoch [3/3], Step [55900/138038], Loss: 2.6170, Perplexity: 13.6948\n",
      "Epoch [3/3], Step [56000/138038], Loss: 2.5150, Perplexity: 12.3664\n",
      "Epoch [3/3], Step [56100/138038], Loss: 3.8976, Perplexity: 49.2849\n",
      "Epoch [3/3], Step [56200/138038], Loss: 3.3020, Perplexity: 27.16770\n",
      "Epoch [3/3], Step [56300/138038], Loss: 2.8419, Perplexity: 17.14809\n",
      "Epoch [3/3], Step [56400/138038], Loss: 2.1966, Perplexity: 8.994400\n",
      "Epoch [3/3], Step [56500/138038], Loss: 2.3841, Perplexity: 10.8497\n",
      "Epoch [3/3], Step [56600/138038], Loss: 3.2787, Perplexity: 26.5421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [56700/138038], Loss: 2.3769, Perplexity: 10.77180\n",
      "Epoch [3/3], Step [56800/138038], Loss: 2.6165, Perplexity: 13.6881\n",
      "Epoch [3/3], Step [56900/138038], Loss: 3.2324, Perplexity: 25.3406\n",
      "Epoch [3/3], Step [57000/138038], Loss: 1.6538, Perplexity: 5.226698\n",
      "Epoch [3/3], Step [57100/138038], Loss: 2.8489, Perplexity: 17.2681\n",
      "Epoch [3/3], Step [57200/138038], Loss: 2.7261, Perplexity: 15.27270\n",
      "Epoch [3/3], Step [57300/138038], Loss: 2.1756, Perplexity: 8.80702\n",
      "Epoch [3/3], Step [57400/138038], Loss: 2.5423, Perplexity: 12.70858\n",
      "Epoch [3/3], Step [57500/138038], Loss: 2.5854, Perplexity: 13.2680\n",
      "Epoch [3/3], Step [57600/138038], Loss: 3.7019, Perplexity: 40.5225\n",
      "Epoch [3/3], Step [57700/138038], Loss: 3.4144, Perplexity: 30.39882\n",
      "Epoch [3/3], Step [57800/138038], Loss: 2.3118, Perplexity: 10.0925\n",
      "Epoch [3/3], Step [57900/138038], Loss: 2.3881, Perplexity: 10.8927\n",
      "Epoch [3/3], Step [58000/138038], Loss: 1.8247, Perplexity: 6.20078\n",
      "Epoch [3/3], Step [58100/138038], Loss: 2.0468, Perplexity: 7.74292\n",
      "Epoch [3/3], Step [58200/138038], Loss: 1.5243, Perplexity: 4.59193\n",
      "Epoch [3/3], Step [58300/138038], Loss: 2.6397, Perplexity: 14.0083\n",
      "Epoch [3/3], Step [58400/138038], Loss: 2.5724, Perplexity: 13.0978\n",
      "Epoch [3/3], Step [58500/138038], Loss: 1.5951, Perplexity: 4.92879\n",
      "Epoch [3/3], Step [58600/138038], Loss: 3.0181, Perplexity: 20.45185\n",
      "Epoch [3/3], Step [58700/138038], Loss: 2.7197, Perplexity: 15.1760\n",
      "Epoch [3/3], Step [58800/138038], Loss: 1.8845, Perplexity: 6.58301\n",
      "Epoch [3/3], Step [58900/138038], Loss: 2.3419, Perplexity: 10.4014\n",
      "Epoch [3/3], Step [59000/138038], Loss: 2.0232, Perplexity: 7.56249\n",
      "Epoch [3/3], Step [59100/138038], Loss: 2.3803, Perplexity: 10.8079\n",
      "Epoch [3/3], Step [59200/138038], Loss: 1.9511, Perplexity: 7.03642\n",
      "Epoch [3/3], Step [59300/138038], Loss: 3.7036, Perplexity: 40.5951\n",
      "Epoch [3/3], Step [59400/138038], Loss: 2.7791, Perplexity: 16.1052\n",
      "Epoch [3/3], Step [59500/138038], Loss: 3.6269, Perplexity: 37.59476\n",
      "Epoch [3/3], Step [59600/138038], Loss: 3.5343, Perplexity: 34.2727\n",
      "Epoch [3/3], Step [59700/138038], Loss: 2.1428, Perplexity: 8.52351\n",
      "Epoch [3/3], Step [59800/138038], Loss: 2.3796, Perplexity: 10.8003\n",
      "Epoch [3/3], Step [59900/138038], Loss: 2.4557, Perplexity: 11.65518\n",
      "Epoch [3/3], Step [60000/138038], Loss: 1.7752, Perplexity: 5.901448\n",
      "Epoch [3/3], Step [60100/138038], Loss: 3.2022, Perplexity: 24.58716\n",
      "Epoch [3/3], Step [60200/138038], Loss: 2.3977, Perplexity: 10.9982\n",
      "Epoch [3/3], Step [60300/138038], Loss: 3.0246, Perplexity: 20.5855\n",
      "Epoch [3/3], Step [60400/138038], Loss: 2.3516, Perplexity: 10.5022\n",
      "Epoch [3/3], Step [60500/138038], Loss: 2.6873, Perplexity: 14.69237\n",
      "Epoch [3/3], Step [60600/138038], Loss: 2.1509, Perplexity: 8.59225\n",
      "Epoch [3/3], Step [60700/138038], Loss: 4.5246, Perplexity: 92.2545\n",
      "Epoch [3/3], Step [60800/138038], Loss: 2.2745, Perplexity: 9.722776\n",
      "Epoch [3/3], Step [60900/138038], Loss: 1.7418, Perplexity: 5.707651\n",
      "Epoch [3/3], Step [61000/138038], Loss: 1.5522, Perplexity: 4.72191\n",
      "Epoch [3/3], Step [61100/138038], Loss: 2.1830, Perplexity: 8.87329\n",
      "Epoch [3/3], Step [61200/138038], Loss: 1.8833, Perplexity: 6.57507\n",
      "Epoch [3/3], Step [61300/138038], Loss: 3.4994, Perplexity: 33.0947\n",
      "Epoch [3/3], Step [61400/138038], Loss: 2.0548, Perplexity: 7.80563\n",
      "Epoch [3/3], Step [61500/138038], Loss: 2.8286, Perplexity: 16.9222\n",
      "Epoch [3/3], Step [61600/138038], Loss: 2.9981, Perplexity: 20.0470\n",
      "Epoch [3/3], Step [61700/138038], Loss: 2.3092, Perplexity: 10.0664\n",
      "Epoch [3/3], Step [61800/138038], Loss: 2.6530, Perplexity: 14.19607\n",
      "Epoch [3/3], Step [61900/138038], Loss: 2.2795, Perplexity: 9.771748\n",
      "Epoch [3/3], Step [62000/138038], Loss: 3.4663, Perplexity: 32.0167\n",
      "Epoch [3/3], Step [62100/138038], Loss: 1.5891, Perplexity: 4.89953\n",
      "Epoch [3/3], Step [62200/138038], Loss: 2.7861, Perplexity: 16.21803\n",
      "Epoch [3/3], Step [62300/138038], Loss: 2.5907, Perplexity: 13.3396\n",
      "Epoch [3/3], Step [62400/138038], Loss: 2.2525, Perplexity: 9.511513\n",
      "Epoch [3/3], Step [62500/138038], Loss: 2.7966, Perplexity: 16.38849\n",
      "Epoch [3/3], Step [62600/138038], Loss: 2.5547, Perplexity: 12.8669\n",
      "Epoch [3/3], Step [62700/138038], Loss: 3.2320, Perplexity: 25.3306\n",
      "Epoch [3/3], Step [62800/138038], Loss: 1.9784, Perplexity: 7.23135\n",
      "Epoch [3/3], Step [62900/138038], Loss: 3.0239, Perplexity: 20.57070\n",
      "Epoch [3/3], Step [63000/138038], Loss: 1.8447, Perplexity: 6.32601\n",
      "Epoch [3/3], Step [63100/138038], Loss: 2.1066, Perplexity: 8.220424\n",
      "Epoch [3/3], Step [63200/138038], Loss: 3.6899, Perplexity: 40.03959\n",
      "Epoch [3/3], Step [63300/138038], Loss: 2.6032, Perplexity: 13.50763\n",
      "Epoch [3/3], Step [63400/138038], Loss: 2.8401, Perplexity: 17.11812\n",
      "Epoch [3/3], Step [63500/138038], Loss: 2.5471, Perplexity: 12.76963\n",
      "Epoch [3/3], Step [63600/138038], Loss: 2.7797, Perplexity: 16.11372\n",
      "Epoch [3/3], Step [63700/138038], Loss: 2.6666, Perplexity: 14.3905\n",
      "Epoch [3/3], Step [63800/138038], Loss: 2.4545, Perplexity: 11.64024\n",
      "Epoch [3/3], Step [63900/138038], Loss: 3.3410, Perplexity: 28.24831\n",
      "Epoch [3/3], Step [64000/138038], Loss: 2.8225, Perplexity: 16.8183\n",
      "Epoch [3/3], Step [64100/138038], Loss: 2.2447, Perplexity: 9.437323\n",
      "Epoch [3/3], Step [64200/138038], Loss: 3.0402, Perplexity: 20.9092\n",
      "Epoch [3/3], Step [64300/138038], Loss: 4.3584, Perplexity: 78.13455\n",
      "Epoch [3/3], Step [64400/138038], Loss: 2.2685, Perplexity: 9.66521\n",
      "Epoch [3/3], Step [64500/138038], Loss: 2.6543, Perplexity: 14.2152\n",
      "Epoch [3/3], Step [64600/138038], Loss: 2.2219, Perplexity: 9.22463\n",
      "Epoch [3/3], Step [64700/138038], Loss: 2.2187, Perplexity: 9.19508\n",
      "Epoch [3/3], Step [64800/138038], Loss: 2.3577, Perplexity: 10.5666\n",
      "Epoch [3/3], Step [64900/138038], Loss: 3.4221, Perplexity: 30.6340\n",
      "Epoch [3/3], Step [65000/138038], Loss: 2.2466, Perplexity: 9.45571\n",
      "Epoch [3/3], Step [65100/138038], Loss: 2.0889, Perplexity: 8.07635\n",
      "Epoch [3/3], Step [65200/138038], Loss: 2.6852, Perplexity: 14.6618\n",
      "Epoch [3/3], Step [65300/138038], Loss: 2.5622, Perplexity: 12.96465\n",
      "Epoch [3/3], Step [65400/138038], Loss: 2.3459, Perplexity: 10.4422\n",
      "Epoch [3/3], Step [65500/138038], Loss: 2.4326, Perplexity: 11.3889\n",
      "Epoch [3/3], Step [65600/138038], Loss: 2.7963, Perplexity: 16.3831\n",
      "Epoch [3/3], Step [65700/138038], Loss: 2.6811, Perplexity: 14.60149\n",
      "Epoch [3/3], Step [65800/138038], Loss: 1.9645, Perplexity: 7.131633\n",
      "Epoch [3/3], Step [65900/138038], Loss: 2.5592, Perplexity: 12.9257\n",
      "Epoch [3/3], Step [66000/138038], Loss: 2.9396, Perplexity: 18.9091\n",
      "Epoch [3/3], Step [66100/138038], Loss: 1.9118, Perplexity: 6.765190\n",
      "Epoch [3/3], Step [66200/138038], Loss: 2.9418, Perplexity: 18.9501\n",
      "Epoch [3/3], Step [66300/138038], Loss: 1.5381, Perplexity: 4.655550\n",
      "Epoch [3/3], Step [66400/138038], Loss: 3.5724, Perplexity: 35.6010\n",
      "Epoch [3/3], Step [66500/138038], Loss: 2.3317, Perplexity: 10.29524\n",
      "Epoch [3/3], Step [66600/138038], Loss: 1.9247, Perplexity: 6.85319\n",
      "Epoch [3/3], Step [66700/138038], Loss: 3.2112, Perplexity: 24.8088\n",
      "Epoch [3/3], Step [66800/138038], Loss: 2.2304, Perplexity: 9.303736\n",
      "Epoch [3/3], Step [66900/138038], Loss: 1.8742, Perplexity: 6.51561\n",
      "Epoch [3/3], Step [67000/138038], Loss: 1.2180, Perplexity: 3.38047\n",
      "Epoch [3/3], Step [67100/138038], Loss: 2.3447, Perplexity: 10.4305\n",
      "Epoch [3/3], Step [67200/138038], Loss: 4.3010, Perplexity: 73.77227\n",
      "Epoch [3/3], Step [67300/138038], Loss: 2.5114, Perplexity: 12.32239\n",
      "Epoch [3/3], Step [67400/138038], Loss: 1.2872, Perplexity: 3.62261\n",
      "Epoch [3/3], Step [67500/138038], Loss: 4.3686, Perplexity: 78.9350\n",
      "Epoch [3/3], Step [67600/138038], Loss: 2.4447, Perplexity: 11.52686\n",
      "Epoch [3/3], Step [67700/138038], Loss: 1.6371, Perplexity: 5.14033\n",
      "Epoch [3/3], Step [67800/138038], Loss: 2.9534, Perplexity: 19.1707\n",
      "Epoch [3/3], Step [67900/138038], Loss: 2.2692, Perplexity: 9.67139\n",
      "Epoch [3/3], Step [68000/138038], Loss: 2.5233, Perplexity: 12.4702\n",
      "Epoch [3/3], Step [68100/138038], Loss: 3.0861, Perplexity: 21.8920\n",
      "Epoch [3/3], Step [68200/138038], Loss: 3.4933, Perplexity: 32.8944\n",
      "Epoch [3/3], Step [68300/138038], Loss: 3.4736, Perplexity: 32.25360\n",
      "Epoch [3/3], Step [68400/138038], Loss: 1.9435, Perplexity: 6.983154\n",
      "Epoch [3/3], Step [68500/138038], Loss: 2.9311, Perplexity: 18.7476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [68600/138038], Loss: 2.9304, Perplexity: 18.7357\n",
      "Epoch [3/3], Step [68700/138038], Loss: 2.0377, Perplexity: 7.673123\n",
      "Epoch [3/3], Step [68800/138038], Loss: 2.2406, Perplexity: 9.398965\n",
      "Epoch [3/3], Step [68900/138038], Loss: 3.6006, Perplexity: 36.6216\n",
      "Epoch [3/3], Step [69000/138038], Loss: 2.1718, Perplexity: 8.77444\n",
      "Epoch [3/3], Step [69100/138038], Loss: 3.0723, Perplexity: 21.59191\n",
      "Epoch [3/3], Step [69200/138038], Loss: 2.5359, Perplexity: 12.6277\n",
      "Epoch [3/3], Step [69300/138038], Loss: 2.1317, Perplexity: 8.428899\n",
      "Epoch [3/3], Step [69400/138038], Loss: 3.1098, Perplexity: 22.4157\n",
      "Epoch [3/3], Step [69500/138038], Loss: 3.1893, Perplexity: 24.27183\n",
      "Epoch [3/3], Step [69600/138038], Loss: 2.4886, Perplexity: 12.0448\n",
      "Epoch [3/3], Step [69700/138038], Loss: 3.6769, Perplexity: 39.5221\n",
      "Epoch [3/3], Step [69800/138038], Loss: 2.8845, Perplexity: 17.8943\n",
      "Epoch [3/3], Step [69900/138038], Loss: 2.0976, Perplexity: 8.146363\n",
      "Epoch [3/3], Step [70000/138038], Loss: 2.1295, Perplexity: 8.41068\n",
      "Epoch [3/3], Step [70100/138038], Loss: 2.7986, Perplexity: 16.42093\n",
      "Epoch [3/3], Step [70200/138038], Loss: 3.4201, Perplexity: 30.5710\n",
      "Epoch [3/3], Step [70300/138038], Loss: 2.4445, Perplexity: 11.5242\n",
      "Epoch [3/3], Step [70400/138038], Loss: 5.0977, Perplexity: 163.6505\n",
      "Epoch [3/3], Step [70500/138038], Loss: 1.8094, Perplexity: 6.106887\n",
      "Epoch [3/3], Step [70600/138038], Loss: 2.8732, Perplexity: 17.6941\n",
      "Epoch [3/3], Step [70700/138038], Loss: 2.7142, Perplexity: 15.0928\n",
      "Epoch [3/3], Step [70800/138038], Loss: 3.4923, Perplexity: 32.8630\n",
      "Epoch [3/3], Step [70900/138038], Loss: 3.6336, Perplexity: 37.8494\n",
      "Epoch [3/3], Step [71000/138038], Loss: 2.5951, Perplexity: 13.39774\n",
      "Epoch [3/3], Step [71100/138038], Loss: 2.9520, Perplexity: 19.1439\n",
      "Epoch [3/3], Step [71200/138038], Loss: 2.8329, Perplexity: 16.9944\n",
      "Epoch [3/3], Step [71300/138038], Loss: 2.9041, Perplexity: 18.24842\n",
      "Epoch [3/3], Step [71400/138038], Loss: 2.5320, Perplexity: 12.57894\n",
      "Epoch [3/3], Step [71500/138038], Loss: 2.8457, Perplexity: 17.2136\n",
      "Epoch [3/3], Step [71600/138038], Loss: 2.9707, Perplexity: 19.5057\n",
      "Epoch [3/3], Step [71700/138038], Loss: 2.0572, Perplexity: 7.82439\n",
      "Epoch [3/3], Step [71800/138038], Loss: 2.5242, Perplexity: 12.48064\n",
      "Epoch [3/3], Step [71900/138038], Loss: 2.1358, Perplexity: 8.464042\n",
      "Epoch [3/3], Step [72000/138038], Loss: 3.0327, Perplexity: 20.7526\n",
      "Epoch [3/3], Step [72100/138038], Loss: 2.3426, Perplexity: 10.4086\n",
      "Epoch [3/3], Step [72200/138038], Loss: 2.4278, Perplexity: 11.3338\n",
      "Epoch [3/3], Step [72300/138038], Loss: 2.9857, Perplexity: 19.8002\n",
      "Epoch [3/3], Step [72400/138038], Loss: 2.4582, Perplexity: 11.68393\n",
      "Epoch [3/3], Step [72500/138038], Loss: 3.0626, Perplexity: 21.3841\n",
      "Epoch [3/3], Step [72600/138038], Loss: 3.3120, Perplexity: 27.4387\n",
      "Epoch [3/3], Step [72700/138038], Loss: 3.2442, Perplexity: 25.64054\n",
      "Epoch [3/3], Step [72800/138038], Loss: 1.9033, Perplexity: 6.708063\n",
      "Epoch [3/3], Step [72900/138038], Loss: 1.8438, Perplexity: 6.32080\n",
      "Epoch [3/3], Step [73000/138038], Loss: 3.1794, Perplexity: 24.03291\n",
      "Epoch [3/3], Step [73100/138038], Loss: 2.8279, Perplexity: 16.9097\n",
      "Epoch [3/3], Step [73200/138038], Loss: 1.7541, Perplexity: 5.77807\n",
      "Epoch [3/3], Step [73300/138038], Loss: 2.1737, Perplexity: 8.791064\n",
      "Epoch [3/3], Step [73400/138038], Loss: 2.0219, Perplexity: 7.55299\n",
      "Epoch [3/3], Step [73500/138038], Loss: 2.0303, Perplexity: 7.61679\n",
      "Epoch [3/3], Step [73600/138038], Loss: 2.2368, Perplexity: 9.36302\n",
      "Epoch [3/3], Step [73700/138038], Loss: 2.3815, Perplexity: 10.8212\n",
      "Epoch [3/3], Step [73800/138038], Loss: 2.5810, Perplexity: 13.2102\n",
      "Epoch [3/3], Step [73900/138038], Loss: 3.0833, Perplexity: 21.8313\n",
      "Epoch [3/3], Step [74000/138038], Loss: 2.6586, Perplexity: 14.2758\n",
      "Epoch [3/3], Step [74100/138038], Loss: 2.4449, Perplexity: 11.5292\n",
      "Epoch [3/3], Step [74200/138038], Loss: 3.3154, Perplexity: 27.5333\n",
      "Epoch [3/3], Step [74300/138038], Loss: 2.2999, Perplexity: 9.97333\n",
      "Epoch [3/3], Step [74400/138038], Loss: 2.5405, Perplexity: 12.6855\n",
      "Epoch [3/3], Step [74500/138038], Loss: 2.0599, Perplexity: 7.844928\n",
      "Epoch [3/3], Step [74600/138038], Loss: 2.6801, Perplexity: 14.5873\n",
      "Epoch [3/3], Step [74700/138038], Loss: 3.3067, Perplexity: 27.2940\n",
      "Epoch [3/3], Step [74800/138038], Loss: 2.2135, Perplexity: 9.14792\n",
      "Epoch [3/3], Step [74900/138038], Loss: 2.8257, Perplexity: 16.8723\n",
      "Epoch [3/3], Step [75000/138038], Loss: 2.1722, Perplexity: 8.77742\n",
      "Epoch [3/3], Step [75100/138038], Loss: 1.6124, Perplexity: 5.01474\n",
      "Epoch [3/3], Step [75200/138038], Loss: 2.4989, Perplexity: 12.1692\n",
      "Epoch [3/3], Step [75300/138038], Loss: 2.5130, Perplexity: 12.34247\n",
      "Epoch [3/3], Step [75400/138038], Loss: 2.8770, Perplexity: 17.76114\n",
      "Epoch [3/3], Step [75500/138038], Loss: 1.8957, Perplexity: 6.65693\n",
      "Epoch [3/3], Step [75600/138038], Loss: 3.0742, Perplexity: 21.6323\n",
      "Epoch [3/3], Step [75700/138038], Loss: 3.0471, Perplexity: 21.0543\n",
      "Epoch [3/3], Step [75800/138038], Loss: 2.6259, Perplexity: 13.8165\n",
      "Epoch [3/3], Step [75900/138038], Loss: 2.4664, Perplexity: 11.7798\n",
      "Epoch [3/3], Step [76000/138038], Loss: 1.8800, Perplexity: 6.55374\n",
      "Epoch [3/3], Step [76100/138038], Loss: 2.1855, Perplexity: 8.89538\n",
      "Epoch [3/3], Step [76200/138038], Loss: 3.0173, Perplexity: 20.4364\n",
      "Epoch [3/3], Step [76300/138038], Loss: 3.1573, Perplexity: 23.5063\n",
      "Epoch [3/3], Step [76400/138038], Loss: 2.6469, Perplexity: 14.10995\n",
      "Epoch [3/3], Step [76500/138038], Loss: 2.2920, Perplexity: 9.894532\n",
      "Epoch [3/3], Step [76600/138038], Loss: 2.4560, Perplexity: 11.6579\n",
      "Epoch [3/3], Step [76700/138038], Loss: 3.6090, Perplexity: 36.9292\n",
      "Epoch [3/3], Step [76800/138038], Loss: 3.1157, Perplexity: 22.5501\n",
      "Epoch [3/3], Step [76900/138038], Loss: 2.6925, Perplexity: 14.76805\n",
      "Epoch [3/3], Step [77000/138038], Loss: 2.1655, Perplexity: 8.71924\n",
      "Epoch [3/3], Step [77100/138038], Loss: 2.3440, Perplexity: 10.4230\n",
      "Epoch [3/3], Step [77200/138038], Loss: 3.7303, Perplexity: 41.69199\n",
      "Epoch [3/3], Step [77300/138038], Loss: 3.2575, Perplexity: 25.9841\n",
      "Epoch [3/3], Step [77400/138038], Loss: 2.8032, Perplexity: 16.49708\n",
      "Epoch [3/3], Step [77500/138038], Loss: 2.7421, Perplexity: 15.5200\n",
      "Epoch [3/3], Step [77600/138038], Loss: 1.8117, Perplexity: 6.121068\n",
      "Epoch [3/3], Step [77700/138038], Loss: 2.9044, Perplexity: 18.2550\n",
      "Epoch [3/3], Step [77800/138038], Loss: 4.3683, Perplexity: 78.91300\n",
      "Epoch [3/3], Step [77900/138038], Loss: 2.2983, Perplexity: 9.95773\n",
      "Epoch [3/3], Step [78000/138038], Loss: 2.5905, Perplexity: 13.3366\n",
      "Epoch [3/3], Step [78100/138038], Loss: 3.4354, Perplexity: 31.04347\n",
      "Epoch [3/3], Step [78200/138038], Loss: 2.9007, Perplexity: 18.18638\n",
      "Epoch [3/3], Step [78300/138038], Loss: 1.9910, Perplexity: 7.32304\n",
      "Epoch [3/3], Step [78400/138038], Loss: 1.9089, Perplexity: 6.74557\n",
      "Epoch [3/3], Step [78500/138038], Loss: 3.7497, Perplexity: 42.5087\n",
      "Epoch [3/3], Step [78600/138038], Loss: 2.5149, Perplexity: 12.36564\n",
      "Epoch [3/3], Step [78700/138038], Loss: 2.1252, Perplexity: 8.37453\n",
      "Epoch [3/3], Step [78800/138038], Loss: 1.8721, Perplexity: 6.50188\n",
      "Epoch [3/3], Step [78900/138038], Loss: 3.1185, Perplexity: 22.61152\n",
      "Epoch [3/3], Step [79000/138038], Loss: 2.4511, Perplexity: 11.6013\n",
      "Epoch [3/3], Step [79100/138038], Loss: 2.8072, Perplexity: 16.5627\n",
      "Epoch [3/3], Step [79200/138038], Loss: 2.9274, Perplexity: 18.6792\n",
      "Epoch [3/3], Step [79300/138038], Loss: 1.9916, Perplexity: 7.327551\n",
      "Epoch [3/3], Step [79400/138038], Loss: 2.6439, Perplexity: 14.0674\n",
      "Epoch [3/3], Step [79500/138038], Loss: 2.1356, Perplexity: 8.462509\n",
      "Epoch [3/3], Step [79600/138038], Loss: 2.6948, Perplexity: 14.8023\n",
      "Epoch [3/3], Step [79700/138038], Loss: 2.7690, Perplexity: 15.9424\n",
      "Epoch [3/3], Step [79800/138038], Loss: 2.0234, Perplexity: 7.56442\n",
      "Epoch [3/3], Step [79900/138038], Loss: 2.0527, Perplexity: 7.789381\n",
      "Epoch [3/3], Step [80000/138038], Loss: 3.3497, Perplexity: 28.49300\n",
      "Epoch [3/3], Step [80100/138038], Loss: 2.0635, Perplexity: 7.87349\n",
      "Epoch [3/3], Step [80200/138038], Loss: 3.5574, Perplexity: 35.0734\n",
      "Epoch [3/3], Step [80300/138038], Loss: 2.5831, Perplexity: 13.2384\n",
      "Epoch [3/3], Step [80400/138038], Loss: 2.6385, Perplexity: 13.9925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [80500/138038], Loss: 2.9602, Perplexity: 19.3013\n",
      "Epoch [3/3], Step [80600/138038], Loss: 3.7757, Perplexity: 43.62936\n",
      "Epoch [3/3], Step [80700/138038], Loss: 2.8486, Perplexity: 17.2634\n",
      "Epoch [3/3], Step [80800/138038], Loss: 2.6020, Perplexity: 13.4913\n",
      "Epoch [3/3], Step [80900/138038], Loss: 3.4396, Perplexity: 31.1739\n",
      "Epoch [3/3], Step [81000/138038], Loss: 2.0341, Perplexity: 7.64552\n",
      "Epoch [3/3], Step [81100/138038], Loss: 2.5351, Perplexity: 12.61790\n",
      "Epoch [3/3], Step [81200/138038], Loss: 2.0818, Perplexity: 8.01933\n",
      "Epoch [3/3], Step [81300/138038], Loss: 1.7353, Perplexity: 5.67094\n",
      "Epoch [3/3], Step [81400/138038], Loss: 2.7408, Perplexity: 15.50016\n",
      "Epoch [3/3], Step [81500/138038], Loss: 2.8762, Perplexity: 17.74742\n",
      "Epoch [3/3], Step [81600/138038], Loss: 2.1991, Perplexity: 9.01652\n",
      "Epoch [3/3], Step [81700/138038], Loss: 1.8617, Perplexity: 6.43495\n",
      "Epoch [3/3], Step [81800/138038], Loss: 1.3929, Perplexity: 4.026698\n",
      "Epoch [3/3], Step [81900/138038], Loss: 2.0939, Perplexity: 8.11686\n",
      "Epoch [3/3], Step [82000/138038], Loss: 2.4172, Perplexity: 11.2139\n",
      "Epoch [3/3], Step [82100/138038], Loss: 2.5101, Perplexity: 12.3066\n",
      "Epoch [3/3], Step [82200/138038], Loss: 2.5424, Perplexity: 12.7105\n",
      "Epoch [3/3], Step [82300/138038], Loss: 2.2557, Perplexity: 9.541948\n",
      "Epoch [3/3], Step [82400/138038], Loss: 2.0729, Perplexity: 7.94802\n",
      "Epoch [3/3], Step [82500/138038], Loss: 3.1848, Perplexity: 24.1627\n",
      "Epoch [3/3], Step [82600/138038], Loss: 2.5817, Perplexity: 13.2201\n",
      "Epoch [3/3], Step [82700/138038], Loss: 2.0449, Perplexity: 7.728023\n",
      "Epoch [3/3], Step [82800/138038], Loss: 3.9945, Perplexity: 54.29693\n",
      "Epoch [3/3], Step [82900/138038], Loss: 3.7262, Perplexity: 41.51993\n",
      "Epoch [3/3], Step [83000/138038], Loss: 2.7555, Perplexity: 15.7290\n",
      "Epoch [3/3], Step [83100/138038], Loss: 3.2703, Perplexity: 26.3201\n",
      "Epoch [3/3], Step [83200/138038], Loss: 2.1210, Perplexity: 8.33917\n",
      "Epoch [3/3], Step [83300/138038], Loss: 2.0640, Perplexity: 7.87711\n",
      "Epoch [3/3], Step [83400/138038], Loss: 3.0991, Perplexity: 22.1782\n",
      "Epoch [3/3], Step [83500/138038], Loss: 1.5683, Perplexity: 4.79865\n",
      "Epoch [3/3], Step [83600/138038], Loss: 1.7284, Perplexity: 5.63164\n",
      "Epoch [3/3], Step [83700/138038], Loss: 2.3215, Perplexity: 10.1914\n",
      "Epoch [3/3], Step [83800/138038], Loss: 2.4712, Perplexity: 11.8363\n",
      "Epoch [3/3], Step [83900/138038], Loss: 2.5553, Perplexity: 12.8754\n",
      "Epoch [3/3], Step [84000/138038], Loss: 3.0883, Perplexity: 21.9402\n",
      "Epoch [3/3], Step [84100/138038], Loss: 1.5986, Perplexity: 4.94630\n",
      "Epoch [3/3], Step [84200/138038], Loss: 3.1293, Perplexity: 22.85703\n",
      "Epoch [3/3], Step [84300/138038], Loss: 1.9431, Perplexity: 6.98008\n",
      "Epoch [3/3], Step [84400/138038], Loss: 3.1821, Perplexity: 24.09694\n",
      "Epoch [3/3], Step [84500/138038], Loss: 2.0982, Perplexity: 8.15143\n",
      "Epoch [3/3], Step [84600/138038], Loss: 2.2551, Perplexity: 9.536545\n",
      "Epoch [3/3], Step [84700/138038], Loss: 3.0147, Perplexity: 20.3826\n",
      "Epoch [3/3], Step [84800/138038], Loss: 2.9761, Perplexity: 19.6106\n",
      "Epoch [3/3], Step [84900/138038], Loss: 2.1632, Perplexity: 8.69936\n",
      "Epoch [3/3], Step [85000/138038], Loss: 3.0177, Perplexity: 20.4448\n",
      "Epoch [3/3], Step [85100/138038], Loss: 2.4317, Perplexity: 11.3788\n",
      "Epoch [3/3], Step [85200/138038], Loss: 1.7163, Perplexity: 5.563618\n",
      "Epoch [3/3], Step [85300/138038], Loss: 3.6648, Perplexity: 39.0480\n",
      "Epoch [3/3], Step [85400/138038], Loss: 2.5248, Perplexity: 12.48805\n",
      "Epoch [3/3], Step [85500/138038], Loss: 1.8250, Perplexity: 6.20296\n",
      "Epoch [3/3], Step [85600/138038], Loss: 2.1787, Perplexity: 8.83495\n",
      "Epoch [3/3], Step [85700/138038], Loss: 3.0972, Perplexity: 22.13613\n",
      "Epoch [3/3], Step [85800/138038], Loss: 2.5415, Perplexity: 12.6986\n",
      "Epoch [3/3], Step [85900/138038], Loss: 2.6126, Perplexity: 13.6340\n",
      "Epoch [3/3], Step [86000/138038], Loss: 3.3587, Perplexity: 28.7522\n",
      "Epoch [3/3], Step [86100/138038], Loss: 2.9139, Perplexity: 18.42779\n",
      "Epoch [3/3], Step [86200/138038], Loss: 2.5737, Perplexity: 13.1142\n",
      "Epoch [3/3], Step [86300/138038], Loss: 4.0067, Perplexity: 54.9643\n",
      "Epoch [3/3], Step [86400/138038], Loss: 1.5688, Perplexity: 4.80079\n",
      "Epoch [3/3], Step [86500/138038], Loss: 2.9055, Perplexity: 18.2748\n",
      "Epoch [3/3], Step [86600/138038], Loss: 2.1896, Perplexity: 8.93188\n",
      "Epoch [3/3], Step [86700/138038], Loss: 3.0244, Perplexity: 20.5809\n",
      "Epoch [3/3], Step [86800/138038], Loss: 2.7217, Perplexity: 15.2065\n",
      "Epoch [3/3], Step [86900/138038], Loss: 2.3203, Perplexity: 10.1785\n",
      "Epoch [3/3], Step [87000/138038], Loss: 3.3283, Perplexity: 27.88977\n",
      "Epoch [3/3], Step [87100/138038], Loss: 4.1303, Perplexity: 62.19739\n",
      "Epoch [3/3], Step [87200/138038], Loss: 2.8182, Perplexity: 16.7468\n",
      "Epoch [3/3], Step [87300/138038], Loss: 4.0666, Perplexity: 58.3560\n",
      "Epoch [3/3], Step [87400/138038], Loss: 2.7297, Perplexity: 15.3287\n",
      "Epoch [3/3], Step [87500/138038], Loss: 3.1735, Perplexity: 23.89191\n",
      "Epoch [3/3], Step [87600/138038], Loss: 2.7802, Perplexity: 16.1220\n",
      "Epoch [3/3], Step [87700/138038], Loss: 2.8789, Perplexity: 17.79472\n",
      "Epoch [3/3], Step [87800/138038], Loss: 3.8732, Perplexity: 48.0951\n",
      "Epoch [3/3], Step [87900/138038], Loss: 1.4296, Perplexity: 4.176810\n",
      "Epoch [3/3], Step [88000/138038], Loss: 2.3098, Perplexity: 10.0721\n",
      "Epoch [3/3], Step [88100/138038], Loss: 3.5164, Perplexity: 33.66338\n",
      "Epoch [3/3], Step [88200/138038], Loss: 2.5523, Perplexity: 12.8366\n",
      "Epoch [3/3], Step [88300/138038], Loss: 1.9343, Perplexity: 6.91941\n",
      "Epoch [3/3], Step [88400/138038], Loss: 2.9704, Perplexity: 19.5004\n",
      "Epoch [3/3], Step [88500/138038], Loss: 2.7319, Perplexity: 15.36150\n",
      "Epoch [3/3], Step [88600/138038], Loss: 2.4661, Perplexity: 11.7764\n",
      "Epoch [3/3], Step [88700/138038], Loss: 2.3873, Perplexity: 10.8840\n",
      "Epoch [3/3], Step [88800/138038], Loss: 2.4733, Perplexity: 11.8614\n",
      "Epoch [3/3], Step [88900/138038], Loss: 2.7340, Perplexity: 15.3947\n",
      "Epoch [3/3], Step [89000/138038], Loss: 4.5107, Perplexity: 90.9827\n",
      "Epoch [3/3], Step [89100/138038], Loss: 1.8568, Perplexity: 6.40316\n",
      "Epoch [3/3], Step [89200/138038], Loss: 2.2939, Perplexity: 9.913631\n",
      "Epoch [3/3], Step [89300/138038], Loss: 2.9308, Perplexity: 18.7419\n",
      "Epoch [3/3], Step [89400/138038], Loss: 2.4290, Perplexity: 11.3474\n",
      "Epoch [3/3], Step [89500/138038], Loss: 2.7715, Perplexity: 15.9826\n",
      "Epoch [3/3], Step [89600/138038], Loss: 3.1290, Perplexity: 22.8512\n",
      "Epoch [3/3], Step [89700/138038], Loss: 2.8552, Perplexity: 17.3771\n",
      "Epoch [3/3], Step [89800/138038], Loss: 2.0102, Perplexity: 7.46467\n",
      "Epoch [3/3], Step [89900/138038], Loss: 1.7508, Perplexity: 5.759258\n",
      "Epoch [3/3], Step [90000/138038], Loss: 1.5053, Perplexity: 4.505605\n",
      "Epoch [3/3], Step [90100/138038], Loss: 3.8189, Perplexity: 45.5520\n",
      "Epoch [3/3], Step [90200/138038], Loss: 2.4985, Perplexity: 12.1643\n",
      "Epoch [3/3], Step [90300/138038], Loss: 2.0431, Perplexity: 7.71446\n",
      "Epoch [3/3], Step [90400/138038], Loss: 3.6959, Perplexity: 40.28015\n",
      "Epoch [3/3], Step [90500/138038], Loss: 3.4653, Perplexity: 31.9847\n",
      "Epoch [3/3], Step [90600/138038], Loss: 3.0135, Perplexity: 20.3585\n",
      "Epoch [3/3], Step [90700/138038], Loss: 3.0237, Perplexity: 20.5666\n",
      "Epoch [3/3], Step [90800/138038], Loss: 2.2897, Perplexity: 9.87232\n",
      "Epoch [3/3], Step [90900/138038], Loss: 2.8393, Perplexity: 17.1045\n",
      "Epoch [3/3], Step [91000/138038], Loss: 2.3504, Perplexity: 10.4902\n",
      "Epoch [3/3], Step [91100/138038], Loss: 2.0412, Perplexity: 7.70005\n",
      "Epoch [3/3], Step [91200/138038], Loss: 2.0442, Perplexity: 7.723354\n",
      "Epoch [3/3], Step [91300/138038], Loss: 2.4024, Perplexity: 11.0495\n",
      "Epoch [3/3], Step [91400/138038], Loss: 3.0994, Perplexity: 22.1857\n",
      "Epoch [3/3], Step [91500/138038], Loss: 1.3817, Perplexity: 3.98161\n",
      "Epoch [3/3], Step [91600/138038], Loss: 3.3302, Perplexity: 27.94485\n",
      "Epoch [3/3], Step [91700/138038], Loss: 2.2338, Perplexity: 9.335784\n",
      "Epoch [3/3], Step [91800/138038], Loss: 1.9721, Perplexity: 7.185472\n",
      "Epoch [3/3], Step [91900/138038], Loss: 2.1639, Perplexity: 8.70493\n",
      "Epoch [3/3], Step [92000/138038], Loss: 3.6360, Perplexity: 37.9414\n",
      "Epoch [3/3], Step [92100/138038], Loss: 2.0061, Perplexity: 7.43446\n",
      "Epoch [3/3], Step [92200/138038], Loss: 2.0032, Perplexity: 7.412409\n",
      "Epoch [3/3], Step [92300/138038], Loss: 2.6238, Perplexity: 13.7885\n",
      "Epoch [3/3], Step [92400/138038], Loss: 3.1649, Perplexity: 23.68658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [92500/138038], Loss: 3.1656, Perplexity: 23.7034\n",
      "Epoch [3/3], Step [92600/138038], Loss: 1.9836, Perplexity: 7.26897\n",
      "Epoch [3/3], Step [92700/138038], Loss: 2.7907, Perplexity: 16.2924\n",
      "Epoch [3/3], Step [92800/138038], Loss: 3.4324, Perplexity: 30.9521\n",
      "Epoch [3/3], Step [92900/138038], Loss: 2.1071, Perplexity: 8.22451\n",
      "Epoch [3/3], Step [93000/138038], Loss: 2.7586, Perplexity: 15.7785\n",
      "Epoch [3/3], Step [93100/138038], Loss: 2.1748, Perplexity: 8.80052\n",
      "Epoch [3/3], Step [93200/138038], Loss: 2.1459, Perplexity: 8.549498\n",
      "Epoch [3/3], Step [93300/138038], Loss: 2.5813, Perplexity: 13.21476\n",
      "Epoch [3/3], Step [93400/138038], Loss: 3.0215, Perplexity: 20.5218\n",
      "Epoch [3/3], Step [93500/138038], Loss: 1.7904, Perplexity: 5.99174\n",
      "Epoch [3/3], Step [93600/138038], Loss: 2.4320, Perplexity: 11.3821\n",
      "Epoch [3/3], Step [93700/138038], Loss: 2.8603, Perplexity: 17.46664\n",
      "Epoch [3/3], Step [93800/138038], Loss: 3.5226, Perplexity: 33.87334\n",
      "Epoch [3/3], Step [93900/138038], Loss: 1.8860, Perplexity: 6.59323\n",
      "Epoch [3/3], Step [94000/138038], Loss: 2.1500, Perplexity: 8.585163\n",
      "Epoch [3/3], Step [94100/138038], Loss: 3.5645, Perplexity: 35.32223\n",
      "Epoch [3/3], Step [94200/138038], Loss: 2.9703, Perplexity: 19.4982\n",
      "Epoch [3/3], Step [94300/138038], Loss: 1.4177, Perplexity: 4.12769\n",
      "Epoch [3/3], Step [94400/138038], Loss: 3.4792, Perplexity: 32.4345\n",
      "Epoch [3/3], Step [94500/138038], Loss: 2.6498, Perplexity: 14.15122\n",
      "Epoch [3/3], Step [94600/138038], Loss: 2.0629, Perplexity: 7.86897\n",
      "Epoch [3/3], Step [94700/138038], Loss: 3.0150, Perplexity: 20.3882\n",
      "Epoch [3/3], Step [94800/138038], Loss: 2.3422, Perplexity: 10.4037\n",
      "Epoch [3/3], Step [94900/138038], Loss: 2.4674, Perplexity: 11.7922\n",
      "Epoch [3/3], Step [95000/138038], Loss: 2.1505, Perplexity: 8.58937\n",
      "Epoch [3/3], Step [95100/138038], Loss: 2.2032, Perplexity: 9.05384\n",
      "Epoch [3/3], Step [95200/138038], Loss: 3.0764, Perplexity: 21.6809\n",
      "Epoch [3/3], Step [95300/138038], Loss: 2.6634, Perplexity: 14.34524\n",
      "Epoch [3/3], Step [95400/138038], Loss: 2.7812, Perplexity: 16.1385\n",
      "Epoch [3/3], Step [95500/138038], Loss: 3.1630, Perplexity: 23.6404\n",
      "Epoch [3/3], Step [95600/138038], Loss: 1.5113, Perplexity: 4.532819\n",
      "Epoch [3/3], Step [95700/138038], Loss: 2.6812, Perplexity: 14.6032\n",
      "Epoch [3/3], Step [95800/138038], Loss: 2.4625, Perplexity: 11.73402\n",
      "Epoch [3/3], Step [95900/138038], Loss: 3.7271, Perplexity: 41.5580\n",
      "Epoch [3/3], Step [96000/138038], Loss: 2.8435, Perplexity: 17.17636\n",
      "Epoch [3/3], Step [96100/138038], Loss: 2.4498, Perplexity: 11.5861\n",
      "Epoch [3/3], Step [96200/138038], Loss: 2.2076, Perplexity: 9.094113\n",
      "Epoch [3/3], Step [96300/138038], Loss: 2.8690, Perplexity: 17.6200\n",
      "Epoch [3/3], Step [96400/138038], Loss: 2.7769, Perplexity: 16.0694\n",
      "Epoch [3/3], Step [96500/138038], Loss: 3.5017, Perplexity: 33.17206\n",
      "Epoch [3/3], Step [96600/138038], Loss: 2.6883, Perplexity: 14.7071\n",
      "Epoch [3/3], Step [96700/138038], Loss: 2.6203, Perplexity: 13.7393\n",
      "Epoch [3/3], Step [96800/138038], Loss: 2.1966, Perplexity: 8.994780\n",
      "Epoch [3/3], Step [96900/138038], Loss: 2.2522, Perplexity: 9.50899\n",
      "Epoch [3/3], Step [97000/138038], Loss: 2.4776, Perplexity: 11.9123\n",
      "Epoch [3/3], Step [97100/138038], Loss: 2.6338, Perplexity: 13.9260\n",
      "Epoch [3/3], Step [97200/138038], Loss: 3.1267, Perplexity: 22.7996\n",
      "Epoch [3/3], Step [97300/138038], Loss: 2.1541, Perplexity: 8.62013\n",
      "Epoch [3/3], Step [97400/138038], Loss: 2.8744, Perplexity: 17.71402\n",
      "Epoch [3/3], Step [97500/138038], Loss: 2.1535, Perplexity: 8.614940\n",
      "Epoch [3/3], Step [97600/138038], Loss: 2.3310, Perplexity: 10.2879\n",
      "Epoch [3/3], Step [97700/138038], Loss: 1.4139, Perplexity: 4.11189\n",
      "Epoch [3/3], Step [97800/138038], Loss: 1.8651, Perplexity: 6.45675\n",
      "Epoch [3/3], Step [97900/138038], Loss: 3.2248, Perplexity: 25.14868\n",
      "Epoch [3/3], Step [98000/138038], Loss: 2.4482, Perplexity: 11.5673\n",
      "Epoch [3/3], Step [98100/138038], Loss: 3.2320, Perplexity: 25.3292\n",
      "Epoch [3/3], Step [98200/138038], Loss: 3.4404, Perplexity: 31.1981\n",
      "Epoch [3/3], Step [98300/138038], Loss: 2.3974, Perplexity: 10.9948\n",
      "Epoch [3/3], Step [98400/138038], Loss: 2.5544, Perplexity: 12.8639\n",
      "Epoch [3/3], Step [98500/138038], Loss: 2.0351, Perplexity: 7.65271\n",
      "Epoch [3/3], Step [98600/138038], Loss: 3.6792, Perplexity: 39.6162\n",
      "Epoch [3/3], Step [98700/138038], Loss: 3.0782, Perplexity: 21.7193\n",
      "Epoch [3/3], Step [98800/138038], Loss: 1.8594, Perplexity: 6.42013\n",
      "Epoch [3/3], Step [98900/138038], Loss: 1.9953, Perplexity: 7.35439\n",
      "Epoch [3/3], Step [99000/138038], Loss: 2.3897, Perplexity: 10.91078\n",
      "Epoch [3/3], Step [99100/138038], Loss: 2.5964, Perplexity: 13.4147\n",
      "Epoch [3/3], Step [99200/138038], Loss: 3.3954, Perplexity: 29.8261\n",
      "Epoch [3/3], Step [99300/138038], Loss: 2.7136, Perplexity: 15.08387\n",
      "Epoch [3/3], Step [99400/138038], Loss: 2.0760, Perplexity: 7.97238\n",
      "Epoch [3/3], Step [99500/138038], Loss: 3.2318, Perplexity: 25.3255\n",
      "Epoch [3/3], Step [99600/138038], Loss: 1.8819, Perplexity: 6.566285\n",
      "Epoch [3/3], Step [99700/138038], Loss: 2.4827, Perplexity: 11.9740\n",
      "Epoch [3/3], Step [99800/138038], Loss: 2.3463, Perplexity: 10.4468\n",
      "Epoch [3/3], Step [99900/138038], Loss: 2.8571, Perplexity: 17.4102\n",
      "Epoch [3/3], Step [100000/138038], Loss: 2.8054, Perplexity: 16.5335\n",
      "Epoch [3/3], Step [100100/138038], Loss: 2.8016, Perplexity: 16.4702\n",
      "Epoch [3/3], Step [100200/138038], Loss: 3.0007, Perplexity: 20.0990\n",
      "Epoch [3/3], Step [100300/138038], Loss: 2.8054, Perplexity: 16.5343\n",
      "Epoch [3/3], Step [100400/138038], Loss: 2.7672, Perplexity: 15.9133\n",
      "Epoch [3/3], Step [100500/138038], Loss: 2.0294, Perplexity: 7.609775\n",
      "Epoch [3/3], Step [100600/138038], Loss: 3.1856, Perplexity: 24.18100\n",
      "Epoch [3/3], Step [100700/138038], Loss: 4.1858, Perplexity: 65.7466\n",
      "Epoch [3/3], Step [100800/138038], Loss: 2.5794, Perplexity: 13.1894\n",
      "Epoch [3/3], Step [100900/138038], Loss: 2.0160, Perplexity: 7.50848\n",
      "Epoch [3/3], Step [101000/138038], Loss: 2.3348, Perplexity: 10.3278\n",
      "Epoch [3/3], Step [101100/138038], Loss: 3.3471, Perplexity: 28.4214\n",
      "Epoch [3/3], Step [101200/138038], Loss: 1.7354, Perplexity: 5.67115\n",
      "Epoch [3/3], Step [101300/138038], Loss: 2.0732, Perplexity: 7.94998\n",
      "Epoch [3/3], Step [101400/138038], Loss: 2.5052, Perplexity: 12.2466\n",
      "Epoch [3/3], Step [101500/138038], Loss: 2.4003, Perplexity: 11.0269\n",
      "Epoch [3/3], Step [101600/138038], Loss: 2.6858, Perplexity: 14.66994\n",
      "Epoch [3/3], Step [101700/138038], Loss: 2.5079, Perplexity: 12.2791\n",
      "Epoch [3/3], Step [101800/138038], Loss: 3.2986, Perplexity: 27.0759\n",
      "Epoch [3/3], Step [101900/138038], Loss: 3.3056, Perplexity: 27.2659\n",
      "Epoch [3/3], Step [102000/138038], Loss: 3.0241, Perplexity: 20.5760\n",
      "Epoch [3/3], Step [102100/138038], Loss: 3.2032, Perplexity: 24.6105\n",
      "Epoch [3/3], Step [102200/138038], Loss: 3.2559, Perplexity: 25.94370\n",
      "Epoch [3/3], Step [102300/138038], Loss: 2.7281, Perplexity: 15.3042\n",
      "Epoch [3/3], Step [102400/138038], Loss: 3.3542, Perplexity: 28.62185\n",
      "Epoch [3/3], Step [102500/138038], Loss: 2.2446, Perplexity: 9.436608\n",
      "Epoch [3/3], Step [102600/138038], Loss: 2.2126, Perplexity: 9.139381\n",
      "Epoch [3/3], Step [102700/138038], Loss: 2.4115, Perplexity: 11.15062\n",
      "Epoch [3/3], Step [102800/138038], Loss: 2.5615, Perplexity: 12.95477\n",
      "Epoch [3/3], Step [102900/138038], Loss: 2.2432, Perplexity: 9.42303\n",
      "Epoch [3/3], Step [103000/138038], Loss: 3.5624, Perplexity: 35.2460\n",
      "Epoch [3/3], Step [103100/138038], Loss: 2.0317, Perplexity: 7.62685\n",
      "Epoch [3/3], Step [103200/138038], Loss: 1.5882, Perplexity: 4.895079\n",
      "Epoch [3/3], Step [103300/138038], Loss: 2.4002, Perplexity: 11.0250\n",
      "Epoch [3/3], Step [103400/138038], Loss: 2.4143, Perplexity: 11.18243\n",
      "Epoch [3/3], Step [103500/138038], Loss: 2.4868, Perplexity: 12.0228\n",
      "Epoch [3/3], Step [103600/138038], Loss: 2.3507, Perplexity: 10.4932\n",
      "Epoch [3/3], Step [103700/138038], Loss: 3.1622, Perplexity: 23.62255\n",
      "Epoch [3/3], Step [103800/138038], Loss: 2.6166, Perplexity: 13.6889\n",
      "Epoch [3/3], Step [103900/138038], Loss: 3.1511, Perplexity: 23.3608\n",
      "Epoch [3/3], Step [104000/138038], Loss: 2.3627, Perplexity: 10.61915\n",
      "Epoch [3/3], Step [104100/138038], Loss: 3.2092, Perplexity: 24.7596\n",
      "Epoch [3/3], Step [104200/138038], Loss: 2.0392, Perplexity: 7.68450\n",
      "Epoch [3/3], Step [104300/138038], Loss: 2.9009, Perplexity: 18.18992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [104400/138038], Loss: 2.6964, Perplexity: 14.8267\n",
      "Epoch [3/3], Step [104500/138038], Loss: 2.9104, Perplexity: 18.3633\n",
      "Epoch [3/3], Step [104600/138038], Loss: 2.1864, Perplexity: 8.90275\n",
      "Epoch [3/3], Step [104700/138038], Loss: 2.9735, Perplexity: 19.5612\n",
      "Epoch [3/3], Step [104800/138038], Loss: 3.5560, Perplexity: 35.0225\n",
      "Epoch [3/3], Step [104900/138038], Loss: 2.5002, Perplexity: 12.1852\n",
      "Epoch [3/3], Step [105000/138038], Loss: 2.6323, Perplexity: 13.9058\n",
      "Epoch [3/3], Step [105100/138038], Loss: 3.0775, Perplexity: 21.7035\n",
      "Epoch [3/3], Step [105200/138038], Loss: 2.6850, Perplexity: 14.6575\n",
      "Epoch [3/3], Step [105300/138038], Loss: 2.7760, Perplexity: 16.05529\n",
      "Epoch [3/3], Step [105400/138038], Loss: 2.0775, Perplexity: 7.98472\n",
      "Epoch [3/3], Step [105500/138038], Loss: 2.2336, Perplexity: 9.33368\n",
      "Epoch [3/3], Step [105600/138038], Loss: 2.7395, Perplexity: 15.4798\n",
      "Epoch [3/3], Step [105700/138038], Loss: 2.1162, Perplexity: 8.29929\n",
      "Epoch [3/3], Step [105800/138038], Loss: 2.3373, Perplexity: 10.3536\n",
      "Epoch [3/3], Step [105900/138038], Loss: 2.9940, Perplexity: 19.96581\n",
      "Epoch [3/3], Step [106000/138038], Loss: 2.0895, Perplexity: 8.08113\n",
      "Epoch [3/3], Step [106100/138038], Loss: 2.2899, Perplexity: 9.87359\n",
      "Epoch [3/3], Step [106200/138038], Loss: 2.8232, Perplexity: 16.8302\n",
      "Epoch [3/3], Step [106300/138038], Loss: 2.8327, Perplexity: 16.9906\n",
      "Epoch [3/3], Step [106400/138038], Loss: 2.5519, Perplexity: 12.8313\n",
      "Epoch [3/3], Step [106500/138038], Loss: 2.4124, Perplexity: 11.1608\n",
      "Epoch [3/3], Step [106600/138038], Loss: 2.7347, Perplexity: 15.4045\n",
      "Epoch [3/3], Step [106700/138038], Loss: 2.9240, Perplexity: 18.6163\n",
      "Epoch [3/3], Step [106800/138038], Loss: 1.8756, Perplexity: 6.52484\n",
      "Epoch [3/3], Step [106900/138038], Loss: 3.0339, Perplexity: 20.7785\n",
      "Epoch [3/3], Step [107000/138038], Loss: 2.2130, Perplexity: 9.143666\n",
      "Epoch [3/3], Step [107100/138038], Loss: 2.2464, Perplexity: 9.453311\n",
      "Epoch [3/3], Step [107200/138038], Loss: 2.2696, Perplexity: 9.67529\n",
      "Epoch [3/3], Step [107300/138038], Loss: 2.4098, Perplexity: 11.13126\n",
      "Epoch [3/3], Step [107400/138038], Loss: 2.2081, Perplexity: 9.09874\n",
      "Epoch [3/3], Step [107500/138038], Loss: 2.9830, Perplexity: 19.7460\n",
      "Epoch [3/3], Step [107600/138038], Loss: 2.2033, Perplexity: 9.05505\n",
      "Epoch [3/3], Step [107700/138038], Loss: 2.3454, Perplexity: 10.4378\n",
      "Epoch [3/3], Step [107800/138038], Loss: 1.4239, Perplexity: 4.15316\n",
      "Epoch [3/3], Step [107900/138038], Loss: 2.0875, Perplexity: 8.064818\n",
      "Epoch [3/3], Step [108000/138038], Loss: 2.4698, Perplexity: 11.8202\n",
      "Epoch [3/3], Step [108100/138038], Loss: 2.2490, Perplexity: 9.47827\n",
      "Epoch [3/3], Step [108200/138038], Loss: 2.6801, Perplexity: 14.5867\n",
      "Epoch [3/3], Step [108300/138038], Loss: 2.0474, Perplexity: 7.74810\n",
      "Epoch [3/3], Step [108400/138038], Loss: 2.4787, Perplexity: 11.9257\n",
      "Epoch [3/3], Step [108500/138038], Loss: 2.1492, Perplexity: 8.57817\n",
      "Epoch [3/3], Step [108600/138038], Loss: 1.6982, Perplexity: 5.46395\n",
      "Epoch [3/3], Step [108700/138038], Loss: 1.8825, Perplexity: 6.569716\n",
      "Epoch [3/3], Step [108800/138038], Loss: 1.4485, Perplexity: 4.25686\n",
      "Epoch [3/3], Step [108900/138038], Loss: 3.2377, Perplexity: 25.4743\n",
      "Epoch [3/3], Step [109000/138038], Loss: 2.7409, Perplexity: 15.5003\n",
      "Epoch [3/3], Step [109100/138038], Loss: 2.1305, Perplexity: 8.419332\n",
      "Epoch [3/3], Step [109200/138038], Loss: 2.1315, Perplexity: 8.42746\n",
      "Epoch [3/3], Step [109300/138038], Loss: 2.1872, Perplexity: 8.91024\n",
      "Epoch [3/3], Step [109400/138038], Loss: 2.2394, Perplexity: 9.38827\n",
      "Epoch [3/3], Step [109500/138038], Loss: 3.3999, Perplexity: 29.9608\n",
      "Epoch [3/3], Step [109600/138038], Loss: 4.0905, Perplexity: 59.7701\n",
      "Epoch [3/3], Step [109700/138038], Loss: 2.7276, Perplexity: 15.29683\n",
      "Epoch [3/3], Step [109800/138038], Loss: 1.7974, Perplexity: 6.03412\n",
      "Epoch [3/3], Step [109900/138038], Loss: 3.5099, Perplexity: 33.4460\n",
      "Epoch [3/3], Step [110000/138038], Loss: 2.3286, Perplexity: 10.2631\n",
      "Epoch [3/3], Step [110100/138038], Loss: 3.6445, Perplexity: 38.26336\n",
      "Epoch [3/3], Step [110200/138038], Loss: 1.7026, Perplexity: 5.488277\n",
      "Epoch [3/3], Step [110300/138038], Loss: 3.2887, Perplexity: 26.8083\n",
      "Epoch [3/3], Step [110400/138038], Loss: 1.9465, Perplexity: 7.004324\n",
      "Epoch [3/3], Step [110500/138038], Loss: 1.5748, Perplexity: 4.82990\n",
      "Epoch [3/3], Step [110600/138038], Loss: 2.0091, Perplexity: 7.45691\n",
      "Epoch [3/3], Step [110700/138038], Loss: 2.9610, Perplexity: 19.3169\n",
      "Epoch [3/3], Step [110800/138038], Loss: 2.8615, Perplexity: 17.4877\n",
      "Epoch [3/3], Step [110900/138038], Loss: 3.5667, Perplexity: 35.3987\n",
      "Epoch [3/3], Step [111000/138038], Loss: 2.0521, Perplexity: 7.78424\n",
      "Epoch [3/3], Step [111100/138038], Loss: 2.7177, Perplexity: 15.1450\n",
      "Epoch [3/3], Step [111200/138038], Loss: 3.7976, Perplexity: 44.5942\n",
      "Epoch [3/3], Step [111300/138038], Loss: 3.2066, Perplexity: 24.6944\n",
      "Epoch [3/3], Step [111400/138038], Loss: 3.0524, Perplexity: 21.1670\n",
      "Epoch [3/3], Step [111500/138038], Loss: 3.0691, Perplexity: 21.5219\n",
      "Epoch [3/3], Step [111600/138038], Loss: 3.1876, Perplexity: 24.23078\n",
      "Epoch [3/3], Step [111700/138038], Loss: 2.9060, Perplexity: 18.2828\n",
      "Epoch [3/3], Step [111800/138038], Loss: 2.4449, Perplexity: 11.5295\n",
      "Epoch [3/3], Step [111900/138038], Loss: 1.9339, Perplexity: 6.91672\n",
      "Epoch [3/3], Step [112000/138038], Loss: 3.8113, Perplexity: 45.2098\n",
      "Epoch [3/3], Step [112100/138038], Loss: 3.7105, Perplexity: 40.8754\n",
      "Epoch [3/3], Step [112200/138038], Loss: 2.7241, Perplexity: 15.24279\n",
      "Epoch [3/3], Step [112300/138038], Loss: 3.4666, Perplexity: 32.02772\n",
      "Epoch [3/3], Step [112400/138038], Loss: 3.4365, Perplexity: 31.0782\n",
      "Epoch [3/3], Step [112500/138038], Loss: 2.8697, Perplexity: 17.63096\n",
      "Epoch [3/3], Step [112600/138038], Loss: 1.9000, Perplexity: 6.68581\n",
      "Epoch [3/3], Step [112700/138038], Loss: 3.3921, Perplexity: 29.7286\n",
      "Epoch [3/3], Step [112800/138038], Loss: 2.9250, Perplexity: 18.6344\n",
      "Epoch [3/3], Step [112900/138038], Loss: 1.4142, Perplexity: 4.11336\n",
      "Epoch [3/3], Step [113000/138038], Loss: 2.5296, Perplexity: 12.5481\n",
      "Epoch [3/3], Step [113100/138038], Loss: 1.6486, Perplexity: 5.19952\n",
      "Epoch [3/3], Step [113200/138038], Loss: 2.4885, Perplexity: 12.0430\n",
      "Epoch [3/3], Step [113300/138038], Loss: 3.2174, Perplexity: 24.9643\n",
      "Epoch [3/3], Step [113400/138038], Loss: 3.9240, Perplexity: 50.6031\n",
      "Epoch [3/3], Step [113500/138038], Loss: 3.4393, Perplexity: 31.16661\n",
      "Epoch [3/3], Step [113600/138038], Loss: 3.0402, Perplexity: 20.9097\n",
      "Epoch [3/3], Step [113700/138038], Loss: 2.6848, Perplexity: 14.6547\n",
      "Epoch [3/3], Step [113800/138038], Loss: 3.0016, Perplexity: 20.11709\n",
      "Epoch [3/3], Step [113900/138038], Loss: 3.3679, Perplexity: 29.0161\n",
      "Epoch [3/3], Step [114000/138038], Loss: 1.6374, Perplexity: 5.14161\n",
      "Epoch [3/3], Step [114100/138038], Loss: 3.0811, Perplexity: 21.7833\n",
      "Epoch [3/3], Step [114200/138038], Loss: 3.1934, Perplexity: 24.3717\n",
      "Epoch [3/3], Step [114300/138038], Loss: 4.8280, Perplexity: 124.9578\n",
      "Epoch [3/3], Step [114400/138038], Loss: 3.3887, Perplexity: 29.6262\n",
      "Epoch [3/3], Step [114500/138038], Loss: 3.2183, Perplexity: 24.98638\n",
      "Epoch [3/3], Step [114600/138038], Loss: 1.2184, Perplexity: 3.38196\n",
      "Epoch [3/3], Step [114700/138038], Loss: 2.7099, Perplexity: 15.0279\n",
      "Epoch [3/3], Step [114800/138038], Loss: 2.3172, Perplexity: 10.1470\n",
      "Epoch [3/3], Step [114900/138038], Loss: 2.5297, Perplexity: 12.54953\n",
      "Epoch [3/3], Step [115000/138038], Loss: 2.0223, Perplexity: 7.55546\n",
      "Epoch [3/3], Step [115100/138038], Loss: 2.6412, Perplexity: 14.02985\n",
      "Epoch [3/3], Step [115200/138038], Loss: 3.2862, Perplexity: 26.7411\n",
      "Epoch [3/3], Step [115300/138038], Loss: 2.4069, Perplexity: 11.0991\n",
      "Epoch [3/3], Step [115400/138038], Loss: 2.6878, Perplexity: 14.69893\n",
      "Epoch [3/3], Step [115500/138038], Loss: 2.1719, Perplexity: 8.77508\n",
      "Epoch [3/3], Step [115600/138038], Loss: 1.3190, Perplexity: 3.73981\n",
      "Epoch [3/3], Step [115700/138038], Loss: 2.3511, Perplexity: 10.49736\n",
      "Epoch [3/3], Step [115800/138038], Loss: 2.0638, Perplexity: 7.87577\n",
      "Epoch [3/3], Step [115900/138038], Loss: 2.4804, Perplexity: 11.9457\n",
      "Epoch [3/3], Step [116000/138038], Loss: 3.1884, Perplexity: 24.2490\n",
      "Epoch [3/3], Step [116100/138038], Loss: 3.2014, Perplexity: 24.56666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [116200/138038], Loss: 2.3896, Perplexity: 10.9088\n",
      "Epoch [3/3], Step [116300/138038], Loss: 3.0512, Perplexity: 21.1408\n",
      "Epoch [3/3], Step [116400/138038], Loss: 4.3008, Perplexity: 73.7593\n",
      "Epoch [3/3], Step [116500/138038], Loss: 2.6851, Perplexity: 14.6599\n",
      "Epoch [3/3], Step [116600/138038], Loss: 2.8647, Perplexity: 17.5434\n",
      "Epoch [3/3], Step [116700/138038], Loss: 2.0040, Perplexity: 7.41887\n",
      "Epoch [3/3], Step [116800/138038], Loss: 2.7541, Perplexity: 15.70725\n",
      "Epoch [3/3], Step [116900/138038], Loss: 3.0219, Perplexity: 20.5293\n",
      "Epoch [3/3], Step [117000/138038], Loss: 2.9881, Perplexity: 19.84803\n",
      "Epoch [3/3], Step [117100/138038], Loss: 3.0766, Perplexity: 21.6835\n",
      "Epoch [3/3], Step [117200/138038], Loss: 1.5370, Perplexity: 4.650532\n",
      "Epoch [3/3], Step [117300/138038], Loss: 2.6679, Perplexity: 14.4101\n",
      "Epoch [3/3], Step [117400/138038], Loss: 2.1249, Perplexity: 8.372249\n",
      "Epoch [3/3], Step [117500/138038], Loss: 4.0636, Perplexity: 58.1814\n",
      "Epoch [3/3], Step [117600/138038], Loss: 3.2379, Perplexity: 25.4790\n",
      "Epoch [3/3], Step [117700/138038], Loss: 2.6002, Perplexity: 13.4663\n",
      "Epoch [3/3], Step [117800/138038], Loss: 2.9946, Perplexity: 19.9772\n",
      "Epoch [3/3], Step [117900/138038], Loss: 2.6727, Perplexity: 14.47936\n",
      "Epoch [3/3], Step [118000/138038], Loss: 3.1249, Perplexity: 22.7574\n",
      "Epoch [3/3], Step [118100/138038], Loss: 1.7827, Perplexity: 5.94564\n",
      "Epoch [3/3], Step [118200/138038], Loss: 4.3664, Perplexity: 78.7594\n",
      "Epoch [3/3], Step [118300/138038], Loss: 1.9118, Perplexity: 6.76535\n",
      "Epoch [3/3], Step [118400/138038], Loss: 2.9987, Perplexity: 20.05991\n",
      "Epoch [3/3], Step [118500/138038], Loss: 1.7198, Perplexity: 5.58337\n",
      "Epoch [3/3], Step [118600/138038], Loss: 3.4207, Perplexity: 30.5918\n",
      "Epoch [3/3], Step [118700/138038], Loss: 1.5030, Perplexity: 4.49519\n",
      "Epoch [3/3], Step [118800/138038], Loss: 3.5391, Perplexity: 34.43541\n",
      "Epoch [3/3], Step [118900/138038], Loss: 3.1229, Perplexity: 22.7128\n",
      "Epoch [3/3], Step [119000/138038], Loss: 2.7740, Perplexity: 16.02306\n",
      "Epoch [3/3], Step [119100/138038], Loss: 2.3276, Perplexity: 10.2533\n",
      "Epoch [3/3], Step [119200/138038], Loss: 2.8449, Perplexity: 17.1997\n",
      "Epoch [3/3], Step [119300/138038], Loss: 2.4422, Perplexity: 11.49824\n",
      "Epoch [3/3], Step [119400/138038], Loss: 2.5218, Perplexity: 12.4515\n",
      "Epoch [3/3], Step [119500/138038], Loss: 2.2555, Perplexity: 9.54038\n",
      "Epoch [3/3], Step [119600/138038], Loss: 3.6324, Perplexity: 37.80233\n",
      "Epoch [3/3], Step [119700/138038], Loss: 3.9102, Perplexity: 49.91073\n",
      "Epoch [3/3], Step [119800/138038], Loss: 3.7729, Perplexity: 43.50417\n",
      "Epoch [3/3], Step [119900/138038], Loss: 2.5390, Perplexity: 12.6666\n",
      "Epoch [3/3], Step [120000/138038], Loss: 3.4926, Perplexity: 32.8712\n",
      "Epoch [3/3], Step [120100/138038], Loss: 3.3450, Perplexity: 28.35976\n",
      "Epoch [3/3], Step [120200/138038], Loss: 2.9099, Perplexity: 18.3547\n",
      "Epoch [3/3], Step [120300/138038], Loss: 2.4698, Perplexity: 11.82013\n",
      "Epoch [3/3], Step [120400/138038], Loss: 3.3344, Perplexity: 28.0603\n",
      "Epoch [3/3], Step [120500/138038], Loss: 3.2039, Perplexity: 24.62876\n",
      "Epoch [3/3], Step [120600/138038], Loss: 3.2838, Perplexity: 26.6769\n",
      "Epoch [3/3], Step [120700/138038], Loss: 2.5387, Perplexity: 12.66380\n",
      "Epoch [3/3], Step [120800/138038], Loss: 3.2264, Perplexity: 25.1900\n",
      "Epoch [3/3], Step [120900/138038], Loss: 1.9148, Perplexity: 6.78567\n",
      "Epoch [3/3], Step [121000/138038], Loss: 2.4170, Perplexity: 11.2123\n",
      "Epoch [3/3], Step [121100/138038], Loss: 1.9698, Perplexity: 7.169084\n",
      "Epoch [3/3], Step [121200/138038], Loss: 1.5967, Perplexity: 4.93688\n",
      "Epoch [3/3], Step [121300/138038], Loss: 2.0426, Perplexity: 7.71109\n",
      "Epoch [3/3], Step [121400/138038], Loss: 3.1015, Perplexity: 22.2314\n",
      "Epoch [3/3], Step [121500/138038], Loss: 3.1136, Perplexity: 22.5017\n",
      "Epoch [3/3], Step [121600/138038], Loss: 4.0467, Perplexity: 57.2073\n",
      "Epoch [3/3], Step [121700/138038], Loss: 3.6056, Perplexity: 36.8034\n",
      "Epoch [3/3], Step [121800/138038], Loss: 2.1539, Perplexity: 8.618060\n",
      "Epoch [3/3], Step [121900/138038], Loss: 3.0560, Perplexity: 21.2422\n",
      "Epoch [3/3], Step [122000/138038], Loss: 1.9415, Perplexity: 6.969163\n",
      "Epoch [3/3], Step [122100/138038], Loss: 1.9023, Perplexity: 6.70137\n",
      "Epoch [3/3], Step [122200/138038], Loss: 2.0819, Perplexity: 8.01977\n",
      "Epoch [3/3], Step [122300/138038], Loss: 3.2459, Perplexity: 25.6845\n",
      "Epoch [3/3], Step [122400/138038], Loss: 2.6952, Perplexity: 14.8087\n",
      "Epoch [3/3], Step [122500/138038], Loss: 2.5795, Perplexity: 13.18991\n",
      "Epoch [3/3], Step [122600/138038], Loss: 3.2821, Perplexity: 26.6319\n",
      "Epoch [3/3], Step [122700/138038], Loss: 3.7417, Perplexity: 42.1680\n",
      "Epoch [3/3], Step [122800/138038], Loss: 3.1595, Perplexity: 23.5597\n",
      "Epoch [3/3], Step [122900/138038], Loss: 2.1027, Perplexity: 8.18820\n",
      "Epoch [3/3], Step [123000/138038], Loss: 2.1694, Perplexity: 8.75320\n",
      "Epoch [3/3], Step [123100/138038], Loss: 2.8394, Perplexity: 17.10586\n",
      "Epoch [3/3], Step [123200/138038], Loss: 2.2839, Perplexity: 9.81517\n",
      "Epoch [3/3], Step [123300/138038], Loss: 2.8641, Perplexity: 17.5331\n",
      "Epoch [3/3], Step [123400/138038], Loss: 2.1794, Perplexity: 8.841282\n",
      "Epoch [3/3], Step [123500/138038], Loss: 2.0196, Perplexity: 7.53562\n",
      "Epoch [3/3], Step [123600/138038], Loss: 2.9044, Perplexity: 18.2541\n",
      "Epoch [3/3], Step [123700/138038], Loss: 2.5639, Perplexity: 12.98580\n",
      "Epoch [3/3], Step [123800/138038], Loss: 3.2840, Perplexity: 26.68310\n",
      "Epoch [3/3], Step [123900/138038], Loss: 2.9173, Perplexity: 18.4917\n",
      "Epoch [3/3], Step [124000/138038], Loss: 1.7382, Perplexity: 5.68732\n",
      "Epoch [3/3], Step [124100/138038], Loss: 2.9851, Perplexity: 19.7885\n",
      "Epoch [3/3], Step [124200/138038], Loss: 3.1531, Perplexity: 23.4091\n",
      "Epoch [3/3], Step [124300/138038], Loss: 1.8731, Perplexity: 6.50869\n",
      "Epoch [3/3], Step [124400/138038], Loss: 1.9836, Perplexity: 7.269020\n",
      "Epoch [3/3], Step [124500/138038], Loss: 2.5674, Perplexity: 13.03169\n",
      "Epoch [3/3], Step [124600/138038], Loss: 1.8296, Perplexity: 6.23128\n",
      "Epoch [3/3], Step [124700/138038], Loss: 2.1470, Perplexity: 8.558846\n",
      "Epoch [3/3], Step [124800/138038], Loss: 2.2660, Perplexity: 9.64125\n",
      "Epoch [3/3], Step [124900/138038], Loss: 4.0470, Perplexity: 57.22830\n",
      "Epoch [3/3], Step [125000/138038], Loss: 3.1038, Perplexity: 22.28190\n",
      "Epoch [3/3], Step [125100/138038], Loss: 3.2951, Perplexity: 26.9797\n",
      "Epoch [3/3], Step [125200/138038], Loss: 2.9196, Perplexity: 18.5333\n",
      "Epoch [3/3], Step [125300/138038], Loss: 3.1293, Perplexity: 22.8568\n",
      "Epoch [3/3], Step [125400/138038], Loss: 1.7803, Perplexity: 5.93195\n",
      "Epoch [3/3], Step [125500/138038], Loss: 2.6468, Perplexity: 14.1092\n",
      "Epoch [3/3], Step [125600/138038], Loss: 2.3499, Perplexity: 10.48491\n",
      "Epoch [3/3], Step [125700/138038], Loss: 2.9902, Perplexity: 19.88991\n",
      "Epoch [3/3], Step [125800/138038], Loss: 1.7789, Perplexity: 5.923689\n",
      "Epoch [3/3], Step [125900/138038], Loss: 2.7306, Perplexity: 15.34150\n",
      "Epoch [3/3], Step [126000/138038], Loss: 1.8866, Perplexity: 6.59691\n",
      "Epoch [3/3], Step [126100/138038], Loss: 2.1092, Perplexity: 8.24183\n",
      "Epoch [3/3], Step [126200/138038], Loss: 3.2920, Perplexity: 26.8954\n",
      "Epoch [3/3], Step [126300/138038], Loss: 3.7377, Perplexity: 42.0031\n",
      "Epoch [3/3], Step [126400/138038], Loss: 2.0181, Perplexity: 7.52383\n",
      "Epoch [3/3], Step [126500/138038], Loss: 3.1910, Perplexity: 24.3133\n",
      "Epoch [3/3], Step [126600/138038], Loss: 2.8021, Perplexity: 16.4800\n",
      "Epoch [3/3], Step [126700/138038], Loss: 2.6629, Perplexity: 14.3379\n",
      "Epoch [3/3], Step [126800/138038], Loss: 2.3389, Perplexity: 10.36974\n",
      "Epoch [3/3], Step [126900/138038], Loss: 1.7400, Perplexity: 5.69756\n",
      "Epoch [3/3], Step [127000/138038], Loss: 1.7209, Perplexity: 5.58988\n",
      "Epoch [3/3], Step [127100/138038], Loss: 2.4467, Perplexity: 11.5496\n",
      "Epoch [3/3], Step [127200/138038], Loss: 2.1511, Perplexity: 8.59453\n",
      "Epoch [3/3], Step [127300/138038], Loss: 2.6556, Perplexity: 14.2337\n",
      "Epoch [3/3], Step [127400/138038], Loss: 2.4898, Perplexity: 12.0589\n",
      "Epoch [3/3], Step [127500/138038], Loss: 2.9378, Perplexity: 18.8745\n",
      "Epoch [3/3], Step [127600/138038], Loss: 3.1131, Perplexity: 22.4902\n",
      "Epoch [3/3], Step [127700/138038], Loss: 2.1806, Perplexity: 8.85194\n",
      "Epoch [3/3], Step [127800/138038], Loss: 2.4954, Perplexity: 12.1260\n",
      "Epoch [3/3], Step [127900/138038], Loss: 2.7804, Perplexity: 16.1256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [128000/138038], Loss: 2.1731, Perplexity: 8.785794\n",
      "Epoch [3/3], Step [128100/138038], Loss: 3.4485, Perplexity: 31.4527\n",
      "Epoch [3/3], Step [128200/138038], Loss: 2.6992, Perplexity: 14.8679\n",
      "Epoch [3/3], Step [128300/138038], Loss: 3.1350, Perplexity: 22.98910\n",
      "Epoch [3/3], Step [128400/138038], Loss: 3.2690, Perplexity: 26.2851\n",
      "Epoch [3/3], Step [128500/138038], Loss: 2.2970, Perplexity: 9.943987\n",
      "Epoch [3/3], Step [128600/138038], Loss: 2.3822, Perplexity: 10.8283\n",
      "Epoch [3/3], Step [128700/138038], Loss: 2.8874, Perplexity: 17.94601\n",
      "Epoch [3/3], Step [128800/138038], Loss: 1.8701, Perplexity: 6.48936\n",
      "Epoch [3/3], Step [128900/138038], Loss: 3.4509, Perplexity: 31.5277\n",
      "Epoch [3/3], Step [129000/138038], Loss: 3.0091, Perplexity: 20.2693\n",
      "Epoch [3/3], Step [129100/138038], Loss: 3.1505, Perplexity: 23.3478\n",
      "Epoch [3/3], Step [129200/138038], Loss: 2.3955, Perplexity: 10.9734\n",
      "Epoch [3/3], Step [129300/138038], Loss: 2.5236, Perplexity: 12.4738\n",
      "Epoch [3/3], Step [129400/138038], Loss: 3.1414, Perplexity: 23.1362\n",
      "Epoch [3/3], Step [129500/138038], Loss: 2.3045, Perplexity: 10.0190\n",
      "Epoch [3/3], Step [129600/138038], Loss: 2.0380, Perplexity: 7.675248\n",
      "Epoch [3/3], Step [129700/138038], Loss: 2.3004, Perplexity: 9.97849\n",
      "Epoch [3/3], Step [129800/138038], Loss: 2.5674, Perplexity: 13.0318\n",
      "Epoch [3/3], Step [129900/138038], Loss: 2.9674, Perplexity: 19.4411\n",
      "Epoch [3/3], Step [130000/138038], Loss: 2.5602, Perplexity: 12.93833\n",
      "Epoch [3/3], Step [130100/138038], Loss: 2.2300, Perplexity: 9.300027\n",
      "Epoch [3/3], Step [130200/138038], Loss: 2.4333, Perplexity: 11.3962\n",
      "Epoch [3/3], Step [130300/138038], Loss: 2.0385, Perplexity: 7.67940\n",
      "Epoch [3/3], Step [130400/138038], Loss: 3.8156, Perplexity: 45.4061\n",
      "Epoch [3/3], Step [130500/138038], Loss: 1.8147, Perplexity: 6.13941\n",
      "Epoch [3/3], Step [130600/138038], Loss: 2.5994, Perplexity: 13.45514\n",
      "Epoch [3/3], Step [130700/138038], Loss: 2.1718, Perplexity: 8.77407\n",
      "Epoch [3/3], Step [130800/138038], Loss: 2.3727, Perplexity: 10.7268\n",
      "Epoch [3/3], Step [130900/138038], Loss: 1.9711, Perplexity: 7.178330\n",
      "Epoch [3/3], Step [131000/138038], Loss: 3.1059, Perplexity: 22.3296\n",
      "Epoch [3/3], Step [131100/138038], Loss: 2.2100, Perplexity: 9.11619\n",
      "Epoch [3/3], Step [131200/138038], Loss: 2.7072, Perplexity: 14.9871\n",
      "Epoch [3/3], Step [131300/138038], Loss: 1.1689, Perplexity: 3.21840\n",
      "Epoch [3/3], Step [131400/138038], Loss: 2.9463, Perplexity: 19.03636\n",
      "Epoch [3/3], Step [131500/138038], Loss: 3.3801, Perplexity: 29.3733\n",
      "Epoch [3/3], Step [131600/138038], Loss: 2.9968, Perplexity: 20.0219\n",
      "Epoch [3/3], Step [131700/138038], Loss: 2.9208, Perplexity: 18.5565\n",
      "Epoch [3/3], Step [131800/138038], Loss: 2.2336, Perplexity: 9.33387\n",
      "Epoch [3/3], Step [131900/138038], Loss: 2.6836, Perplexity: 14.6379\n",
      "Epoch [3/3], Step [132000/138038], Loss: 2.7156, Perplexity: 15.11445\n",
      "Epoch [3/3], Step [132100/138038], Loss: 1.8573, Perplexity: 6.40667\n",
      "Epoch [3/3], Step [132200/138038], Loss: 2.5402, Perplexity: 12.6828\n",
      "Epoch [3/3], Step [132300/138038], Loss: 2.4083, Perplexity: 11.1156\n",
      "Epoch [3/3], Step [132400/138038], Loss: 2.2808, Perplexity: 9.78409\n",
      "Epoch [3/3], Step [132500/138038], Loss: 2.2275, Perplexity: 9.27672\n",
      "Epoch [3/3], Step [132600/138038], Loss: 4.8059, Perplexity: 122.2333\n",
      "Epoch [3/3], Step [132700/138038], Loss: 2.3013, Perplexity: 9.987532\n",
      "Epoch [3/3], Step [132800/138038], Loss: 3.4537, Perplexity: 31.61616\n",
      "Epoch [3/3], Step [132900/138038], Loss: 2.6810, Perplexity: 14.6003\n",
      "Epoch [3/3], Step [133000/138038], Loss: 2.6815, Perplexity: 14.6073\n",
      "Epoch [3/3], Step [133100/138038], Loss: 1.7457, Perplexity: 5.73027\n",
      "Epoch [3/3], Step [133200/138038], Loss: 4.6225, Perplexity: 101.7507\n",
      "Epoch [3/3], Step [133300/138038], Loss: 2.5597, Perplexity: 12.9324\n",
      "Epoch [3/3], Step [133400/138038], Loss: 2.0096, Perplexity: 7.46052\n",
      "Epoch [3/3], Step [133500/138038], Loss: 2.5294, Perplexity: 12.5459\n",
      "Epoch [3/3], Step [133600/138038], Loss: 2.5765, Perplexity: 13.1511\n",
      "Epoch [3/3], Step [133700/138038], Loss: 1.7823, Perplexity: 5.943374\n",
      "Epoch [3/3], Step [133800/138038], Loss: 2.8985, Perplexity: 18.14777\n",
      "Epoch [3/3], Step [133900/138038], Loss: 2.3520, Perplexity: 10.50686\n",
      "Epoch [3/3], Step [134000/138038], Loss: 4.2173, Perplexity: 67.8509\n",
      "Epoch [3/3], Step [134100/138038], Loss: 2.2006, Perplexity: 9.03088\n",
      "Epoch [3/3], Step [134200/138038], Loss: 3.3737, Perplexity: 29.1873\n",
      "Epoch [3/3], Step [134300/138038], Loss: 2.4798, Perplexity: 11.93865\n",
      "Epoch [3/3], Step [134400/138038], Loss: 1.8292, Perplexity: 6.229100\n",
      "Epoch [3/3], Step [134500/138038], Loss: 2.0059, Perplexity: 7.43284\n",
      "Epoch [3/3], Step [134600/138038], Loss: 2.8263, Perplexity: 16.8828\n",
      "Epoch [3/3], Step [134700/138038], Loss: 2.6430, Perplexity: 14.0549\n",
      "Epoch [3/3], Step [134800/138038], Loss: 2.9776, Perplexity: 19.6405\n",
      "Epoch [3/3], Step [134900/138038], Loss: 2.7363, Perplexity: 15.43030\n",
      "Epoch [3/3], Step [135000/138038], Loss: 2.7254, Perplexity: 15.2630\n",
      "Epoch [3/3], Step [135100/138038], Loss: 3.1667, Perplexity: 23.72985\n",
      "Epoch [3/3], Step [135200/138038], Loss: 1.7338, Perplexity: 5.66248\n",
      "Epoch [3/3], Step [135300/138038], Loss: 3.6369, Perplexity: 37.9752\n",
      "Epoch [3/3], Step [135400/138038], Loss: 2.0182, Perplexity: 7.52503\n",
      "Epoch [3/3], Step [135500/138038], Loss: 1.6819, Perplexity: 5.376008\n",
      "Epoch [3/3], Step [135600/138038], Loss: 2.9609, Perplexity: 19.3158\n",
      "Epoch [3/3], Step [135700/138038], Loss: 2.0177, Perplexity: 7.52082\n",
      "Epoch [3/3], Step [135800/138038], Loss: 2.0338, Perplexity: 7.64329\n",
      "Epoch [3/3], Step [135900/138038], Loss: 2.2900, Perplexity: 9.874830\n",
      "Epoch [3/3], Step [136000/138038], Loss: 2.8491, Perplexity: 17.2720\n",
      "Epoch [3/3], Step [136100/138038], Loss: 2.2259, Perplexity: 9.26187\n",
      "Epoch [3/3], Step [136200/138038], Loss: 2.1708, Perplexity: 8.76504\n",
      "Epoch [3/3], Step [136300/138038], Loss: 2.6949, Perplexity: 14.8044\n",
      "Epoch [3/3], Step [136400/138038], Loss: 2.5448, Perplexity: 12.7407\n",
      "Epoch [3/3], Step [136500/138038], Loss: 2.4607, Perplexity: 11.71350\n",
      "Epoch [3/3], Step [136600/138038], Loss: 2.2836, Perplexity: 9.811990\n",
      "Epoch [3/3], Step [136700/138038], Loss: 3.7783, Perplexity: 43.7425\n",
      "Epoch [3/3], Step [136800/138038], Loss: 1.5549, Perplexity: 4.734444\n",
      "Epoch [3/3], Step [136900/138038], Loss: 2.5245, Perplexity: 12.48435\n",
      "Epoch [3/3], Step [137000/138038], Loss: 3.9606, Perplexity: 52.48851\n",
      "Epoch [3/3], Step [137100/138038], Loss: 3.6379, Perplexity: 38.0131\n",
      "Epoch [3/3], Step [137200/138038], Loss: 3.2745, Perplexity: 26.4298\n",
      "Epoch [3/3], Step [137300/138038], Loss: 2.7710, Perplexity: 15.9752\n",
      "Epoch [3/3], Step [137400/138038], Loss: 1.7785, Perplexity: 5.92129\n",
      "Epoch [3/3], Step [137500/138038], Loss: 2.7978, Perplexity: 16.4086\n",
      "Epoch [3/3], Step [137600/138038], Loss: 2.0501, Perplexity: 7.76880\n",
      "Epoch [3/3], Step [137700/138038], Loss: 2.0841, Perplexity: 8.03755\n",
      "Epoch [3/3], Step [137800/138038], Loss: 2.7064, Perplexity: 14.9753\n",
      "Epoch [3/3], Step [137900/138038], Loss: 3.1239, Perplexity: 22.7342\n",
      "Epoch [3/3], Step [138000/138038], Loss: 3.5178, Perplexity: 33.7111\n",
      "Epoch [3/3], Step [138038/138038], Loss: 2.1969, Perplexity: 8.99680"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions[:, :-1])\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
