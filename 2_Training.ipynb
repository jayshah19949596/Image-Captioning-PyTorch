{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/share/applications/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /usr/share/applications/anaconda3/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "[nltk_data] Downloading package punkt to /home/jai/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/414113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:29<00:00, 14156.99it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 3          # batch size\n",
    "vocab_threshold = 5       # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 128          # dimensionality of image and word embeddings\n",
    "hidden_size = 100        # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = decoder.parameters()\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/138038], Loss: 2.1073, Perplexity: 8.22573\n",
      "Epoch [1/3], Step [200/138038], Loss: 1.8138, Perplexity: 6.133653\n",
      "Epoch [1/3], Step [300/138038], Loss: 3.0154, Perplexity: 20.3978\n",
      "Epoch [1/3], Step [400/138038], Loss: 2.0732, Perplexity: 7.950005\n",
      "Epoch [1/3], Step [500/138038], Loss: 2.2363, Perplexity: 9.358334\n",
      "Epoch [1/3], Step [600/138038], Loss: 3.2319, Perplexity: 25.32776\n",
      "Epoch [1/3], Step [700/138038], Loss: 2.9498, Perplexity: 19.1027\n",
      "Epoch [1/3], Step [800/138038], Loss: 2.7602, Perplexity: 15.8037\n",
      "Epoch [1/3], Step [900/138038], Loss: 1.7479, Perplexity: 5.74269\n",
      "Epoch [1/3], Step [1000/138038], Loss: 2.0718, Perplexity: 7.93890\n",
      "Epoch [1/3], Step [1100/138038], Loss: 3.0420, Perplexity: 20.9478\n",
      "Epoch [1/3], Step [1200/138038], Loss: 2.5171, Perplexity: 12.3921\n",
      "Epoch [1/3], Step [1300/138038], Loss: 1.9686, Perplexity: 7.16056\n",
      "Epoch [1/3], Step [1400/138038], Loss: 2.5132, Perplexity: 12.3443\n",
      "Epoch [1/3], Step [1500/138038], Loss: 3.2980, Perplexity: 27.0586\n",
      "Epoch [1/3], Step [1600/138038], Loss: 2.6375, Perplexity: 13.9786\n",
      "Epoch [1/3], Step [1700/138038], Loss: 3.2219, Perplexity: 25.07554\n",
      "Epoch [1/3], Step [1800/138038], Loss: 2.2376, Perplexity: 9.37119\n",
      "Epoch [1/3], Step [1900/138038], Loss: 2.2104, Perplexity: 9.11926\n",
      "Epoch [1/3], Step [2000/138038], Loss: 3.1115, Perplexity: 22.45386\n",
      "Epoch [1/3], Step [2100/138038], Loss: 3.5321, Perplexity: 34.1957\n",
      "Epoch [1/3], Step [2200/138038], Loss: 3.1376, Perplexity: 23.0484\n",
      "Epoch [1/3], Step [2300/138038], Loss: 2.0856, Perplexity: 8.049797\n",
      "Epoch [1/3], Step [2400/138038], Loss: 3.0488, Perplexity: 21.0904\n",
      "Epoch [1/3], Step [2500/138038], Loss: 2.5878, Perplexity: 13.3006\n",
      "Epoch [1/3], Step [2600/138038], Loss: 3.1419, Perplexity: 23.1486\n",
      "Epoch [1/3], Step [2700/138038], Loss: 2.5369, Perplexity: 12.64075\n",
      "Epoch [1/3], Step [2800/138038], Loss: 2.2196, Perplexity: 9.20344\n",
      "Epoch [1/3], Step [2900/138038], Loss: 2.4310, Perplexity: 11.37079\n",
      "Epoch [1/3], Step [3000/138038], Loss: 2.6617, Perplexity: 14.32095\n",
      "Epoch [1/3], Step [3100/138038], Loss: 3.4029, Perplexity: 30.05261\n",
      "Epoch [1/3], Step [3200/138038], Loss: 2.9106, Perplexity: 18.3672\n",
      "Epoch [1/3], Step [3300/138038], Loss: 3.1540, Perplexity: 23.43068\n",
      "Epoch [1/3], Step [3400/138038], Loss: 2.4320, Perplexity: 11.3812\n",
      "Epoch [1/3], Step [3500/138038], Loss: 2.1350, Perplexity: 8.457152\n",
      "Epoch [1/3], Step [3600/138038], Loss: 2.2460, Perplexity: 9.450373\n",
      "Epoch [1/3], Step [3700/138038], Loss: 2.7354, Perplexity: 15.41543\n",
      "Epoch [1/3], Step [3800/138038], Loss: 2.5824, Perplexity: 13.2285\n",
      "Epoch [1/3], Step [3900/138038], Loss: 2.2518, Perplexity: 9.50459\n",
      "Epoch [1/3], Step [4000/138038], Loss: 3.8642, Perplexity: 47.66719\n",
      "Epoch [1/3], Step [4100/138038], Loss: 1.9593, Perplexity: 7.094376\n",
      "Epoch [1/3], Step [4200/138038], Loss: 2.1954, Perplexity: 8.98373\n",
      "Epoch [1/3], Step [4300/138038], Loss: 2.7424, Perplexity: 15.5242\n",
      "Epoch [1/3], Step [4400/138038], Loss: 2.1685, Perplexity: 8.74491\n",
      "Epoch [1/3], Step [4500/138038], Loss: 2.2748, Perplexity: 9.72571\n",
      "Epoch [1/3], Step [4600/138038], Loss: 2.6596, Perplexity: 14.2907\n",
      "Epoch [1/3], Step [4700/138038], Loss: 2.8323, Perplexity: 16.98491\n",
      "Epoch [1/3], Step [4800/138038], Loss: 3.2503, Perplexity: 25.79785\n",
      "Epoch [1/3], Step [4900/138038], Loss: 2.0741, Perplexity: 7.957323\n",
      "Epoch [1/3], Step [5000/138038], Loss: 2.4345, Perplexity: 11.41062\n",
      "Epoch [1/3], Step [5100/138038], Loss: 4.1158, Perplexity: 61.30151\n",
      "Epoch [1/3], Step [5200/138038], Loss: 2.0723, Perplexity: 7.942956\n",
      "Epoch [1/3], Step [5300/138038], Loss: 4.1070, Perplexity: 60.7630\n",
      "Epoch [1/3], Step [5400/138038], Loss: 2.1841, Perplexity: 8.882562\n",
      "Epoch [1/3], Step [5500/138038], Loss: 2.0451, Perplexity: 7.729908\n",
      "Epoch [1/3], Step [5600/138038], Loss: 2.1004, Perplexity: 8.16932\n",
      "Epoch [1/3], Step [5700/138038], Loss: 2.3192, Perplexity: 10.16734\n",
      "Epoch [1/3], Step [5800/138038], Loss: 2.6890, Perplexity: 14.7168\n",
      "Epoch [1/3], Step [5900/138038], Loss: 3.6866, Perplexity: 39.9106\n",
      "Epoch [1/3], Step [6000/138038], Loss: 2.1911, Perplexity: 8.945186\n",
      "Epoch [1/3], Step [6100/138038], Loss: 3.4202, Perplexity: 30.5745\n",
      "Epoch [1/3], Step [6200/138038], Loss: 3.9198, Perplexity: 50.3880\n",
      "Epoch [1/3], Step [6300/138038], Loss: 2.2012, Perplexity: 9.036267\n",
      "Epoch [1/3], Step [6400/138038], Loss: 1.8274, Perplexity: 6.21764\n",
      "Epoch [1/3], Step [6500/138038], Loss: 2.6663, Perplexity: 14.38658\n",
      "Epoch [1/3], Step [6600/138038], Loss: 4.1174, Perplexity: 61.40012\n",
      "Epoch [1/3], Step [6700/138038], Loss: 2.9323, Perplexity: 18.7701\n",
      "Epoch [1/3], Step [6800/138038], Loss: 2.5901, Perplexity: 13.3308\n",
      "Epoch [1/3], Step [6900/138038], Loss: 3.0084, Perplexity: 20.25445\n",
      "Epoch [1/3], Step [7000/138038], Loss: 1.9457, Perplexity: 6.998949\n",
      "Epoch [1/3], Step [7100/138038], Loss: 2.1056, Perplexity: 8.211846\n",
      "Epoch [1/3], Step [7200/138038], Loss: 4.0098, Perplexity: 55.1381\n",
      "Epoch [1/3], Step [7300/138038], Loss: 2.8144, Perplexity: 16.6833\n",
      "Epoch [1/3], Step [7400/138038], Loss: 2.7492, Perplexity: 15.63004\n",
      "Epoch [1/3], Step [7500/138038], Loss: 4.2844, Perplexity: 72.55670\n",
      "Epoch [1/3], Step [7600/138038], Loss: 2.4794, Perplexity: 11.9346\n",
      "Epoch [1/3], Step [7700/138038], Loss: 2.6712, Perplexity: 14.4571\n",
      "Epoch [1/3], Step [7800/138038], Loss: 2.8041, Perplexity: 16.51159\n",
      "Epoch [1/3], Step [7900/138038], Loss: 2.6055, Perplexity: 13.5385\n",
      "Epoch [1/3], Step [8000/138038], Loss: 1.7719, Perplexity: 5.88211\n",
      "Epoch [1/3], Step [8100/138038], Loss: 2.8352, Perplexity: 17.03454\n",
      "Epoch [1/3], Step [8200/138038], Loss: 2.9239, Perplexity: 18.61348\n",
      "Epoch [1/3], Step [8300/138038], Loss: 3.5235, Perplexity: 33.9028\n",
      "Epoch [1/3], Step [8400/138038], Loss: 2.6167, Perplexity: 13.69045\n",
      "Epoch [1/3], Step [8500/138038], Loss: 2.9256, Perplexity: 18.64512\n",
      "Epoch [1/3], Step [8600/138038], Loss: 2.9709, Perplexity: 19.5087\n",
      "Epoch [1/3], Step [8700/138038], Loss: 4.3306, Perplexity: 75.99289\n",
      "Epoch [1/3], Step [8800/138038], Loss: 3.3919, Perplexity: 29.72350\n",
      "Epoch [1/3], Step [8900/138038], Loss: 2.4935, Perplexity: 12.10315\n",
      "Epoch [1/3], Step [9000/138038], Loss: 3.8713, Perplexity: 48.00495\n",
      "Epoch [1/3], Step [9100/138038], Loss: 2.1823, Perplexity: 8.86668\n",
      "Epoch [1/3], Step [9200/138038], Loss: 2.7805, Perplexity: 16.12758\n",
      "Epoch [1/3], Step [9300/138038], Loss: 2.7514, Perplexity: 15.6645\n",
      "Epoch [1/3], Step [9400/138038], Loss: 2.8615, Perplexity: 17.48748\n",
      "Epoch [1/3], Step [9500/138038], Loss: 3.1073, Perplexity: 22.36017\n",
      "Epoch [1/3], Step [9600/138038], Loss: 2.9350, Perplexity: 18.8213\n",
      "Epoch [1/3], Step [9700/138038], Loss: 1.5810, Perplexity: 4.859961\n",
      "Epoch [1/3], Step [9800/138038], Loss: 3.4703, Perplexity: 32.14631\n",
      "Epoch [1/3], Step [9900/138038], Loss: 2.6975, Perplexity: 14.84258\n",
      "Epoch [1/3], Step [10000/138038], Loss: 3.4403, Perplexity: 31.1973\n",
      "Epoch [1/3], Step [10100/138038], Loss: 2.1178, Perplexity: 8.31283\n",
      "Epoch [1/3], Step [10200/138038], Loss: 2.9814, Perplexity: 19.71530\n",
      "Epoch [1/3], Step [10300/138038], Loss: 3.1198, Perplexity: 22.6415\n",
      "Epoch [1/3], Step [10400/138038], Loss: 2.8083, Perplexity: 16.58238\n",
      "Epoch [1/3], Step [10500/138038], Loss: 2.7550, Perplexity: 15.72061\n",
      "Epoch [1/3], Step [10600/138038], Loss: 3.3896, Perplexity: 29.65414\n",
      "Epoch [1/3], Step [10700/138038], Loss: 2.8833, Perplexity: 17.87244\n",
      "Epoch [1/3], Step [10800/138038], Loss: 2.6199, Perplexity: 13.7341\n",
      "Epoch [1/3], Step [10900/138038], Loss: 3.0881, Perplexity: 21.9350\n",
      "Epoch [1/3], Step [11000/138038], Loss: 3.0930, Perplexity: 22.04310\n",
      "Epoch [1/3], Step [11100/138038], Loss: 2.3695, Perplexity: 10.6917\n",
      "Epoch [1/3], Step [11200/138038], Loss: 3.0109, Perplexity: 20.30640\n",
      "Epoch [1/3], Step [11300/138038], Loss: 2.6096, Perplexity: 13.59304\n",
      "Epoch [1/3], Step [11400/138038], Loss: 4.0056, Perplexity: 54.90561\n",
      "Epoch [1/3], Step [11500/138038], Loss: 2.4181, Perplexity: 11.2247\n",
      "Epoch [1/3], Step [11600/138038], Loss: 3.0773, Perplexity: 21.69876\n",
      "Epoch [1/3], Step [11700/138038], Loss: 4.0816, Perplexity: 59.24132\n",
      "Epoch [1/3], Step [11800/138038], Loss: 2.3979, Perplexity: 10.99962\n",
      "Epoch [1/3], Step [11900/138038], Loss: 2.7521, Perplexity: 15.67622\n",
      "Epoch [1/3], Step [12000/138038], Loss: 3.9864, Perplexity: 53.86203\n",
      "Epoch [1/3], Step [12100/138038], Loss: 2.5085, Perplexity: 12.28677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [12200/138038], Loss: 2.0956, Perplexity: 8.13073\n",
      "Epoch [1/3], Step [12300/138038], Loss: 3.6374, Perplexity: 37.9922\n",
      "Epoch [1/3], Step [12400/138038], Loss: 2.1328, Perplexity: 8.438724\n",
      "Epoch [1/3], Step [12500/138038], Loss: 3.4372, Perplexity: 31.1000\n",
      "Epoch [1/3], Step [12600/138038], Loss: 3.2756, Perplexity: 26.4595\n",
      "Epoch [1/3], Step [12700/138038], Loss: 3.0183, Perplexity: 20.4566\n",
      "Epoch [1/3], Step [12800/138038], Loss: 2.2882, Perplexity: 9.857315\n",
      "Epoch [1/3], Step [12900/138038], Loss: 2.5864, Perplexity: 13.28180\n",
      "Epoch [1/3], Step [13000/138038], Loss: 2.2209, Perplexity: 9.21555\n",
      "Epoch [1/3], Step [13100/138038], Loss: 3.4572, Perplexity: 31.72958\n",
      "Epoch [1/3], Step [13200/138038], Loss: 2.8030, Perplexity: 16.49360\n",
      "Epoch [1/3], Step [13300/138038], Loss: 1.9693, Perplexity: 7.16598\n",
      "Epoch [1/3], Step [13400/138038], Loss: 2.5268, Perplexity: 12.51348\n",
      "Epoch [1/3], Step [13500/138038], Loss: 2.9461, Perplexity: 19.0319\n",
      "Epoch [1/3], Step [13600/138038], Loss: 2.6753, Perplexity: 14.51665\n",
      "Epoch [1/3], Step [13700/138038], Loss: 2.0945, Perplexity: 8.121165\n",
      "Epoch [1/3], Step [13800/138038], Loss: 2.9764, Perplexity: 19.6168\n",
      "Epoch [1/3], Step [13900/138038], Loss: 2.4591, Perplexity: 11.69460\n",
      "Epoch [1/3], Step [14000/138038], Loss: 3.4419, Perplexity: 31.24680\n",
      "Epoch [1/3], Step [14100/138038], Loss: 3.3971, Perplexity: 29.8774\n",
      "Epoch [1/3], Step [14200/138038], Loss: 2.8622, Perplexity: 17.50079\n",
      "Epoch [1/3], Step [14300/138038], Loss: 3.2809, Perplexity: 26.5990\n",
      "Epoch [1/3], Step [14400/138038], Loss: 2.4384, Perplexity: 11.45451\n",
      "Epoch [1/3], Step [14500/138038], Loss: 3.3659, Perplexity: 28.96059\n",
      "Epoch [1/3], Step [14600/138038], Loss: 2.5341, Perplexity: 12.6045\n",
      "Epoch [1/3], Step [14700/138038], Loss: 2.5320, Perplexity: 12.5791\n",
      "Epoch [1/3], Step [14800/138038], Loss: 3.0944, Perplexity: 22.0733\n",
      "Epoch [1/3], Step [14900/138038], Loss: 3.3286, Perplexity: 27.90066\n",
      "Epoch [1/3], Step [15000/138038], Loss: 3.2229, Perplexity: 25.1006\n",
      "Epoch [1/3], Step [15100/138038], Loss: 2.9407, Perplexity: 18.92860\n",
      "Epoch [1/3], Step [15200/138038], Loss: 2.2149, Perplexity: 9.160121\n",
      "Epoch [1/3], Step [15300/138038], Loss: 3.1957, Perplexity: 24.42635\n",
      "Epoch [1/3], Step [15400/138038], Loss: 3.3008, Perplexity: 27.1355\n",
      "Epoch [1/3], Step [15500/138038], Loss: 3.4483, Perplexity: 31.44700\n",
      "Epoch [1/3], Step [15600/138038], Loss: 3.3206, Perplexity: 27.67699\n",
      "Epoch [1/3], Step [15700/138038], Loss: 3.8182, Perplexity: 45.5239\n",
      "Epoch [1/3], Step [15800/138038], Loss: 2.6136, Perplexity: 13.64740\n",
      "Epoch [1/3], Step [15900/138038], Loss: 3.1221, Perplexity: 22.6944\n",
      "Epoch [1/3], Step [16000/138038], Loss: 2.1868, Perplexity: 8.906345\n",
      "Epoch [1/3], Step [16100/138038], Loss: 3.2172, Perplexity: 24.95871\n",
      "Epoch [1/3], Step [16200/138038], Loss: 2.5917, Perplexity: 13.35198\n",
      "Epoch [1/3], Step [16300/138038], Loss: 2.9689, Perplexity: 19.4710\n",
      "Epoch [1/3], Step [16400/138038], Loss: 2.7995, Perplexity: 16.43695\n",
      "Epoch [1/3], Step [16500/138038], Loss: 2.7282, Perplexity: 15.3048\n",
      "Epoch [1/3], Step [16600/138038], Loss: 2.2592, Perplexity: 9.57541\n",
      "Epoch [1/3], Step [16700/138038], Loss: 2.7888, Perplexity: 16.26136\n",
      "Epoch [1/3], Step [16800/138038], Loss: 3.3904, Perplexity: 29.67870\n",
      "Epoch [1/3], Step [16900/138038], Loss: 2.9141, Perplexity: 18.4331\n",
      "Epoch [1/3], Step [17000/138038], Loss: 2.0541, Perplexity: 7.800141\n",
      "Epoch [1/3], Step [17100/138038], Loss: 3.2519, Perplexity: 25.83998\n",
      "Epoch [1/3], Step [17200/138038], Loss: 3.3978, Perplexity: 29.89744\n",
      "Epoch [1/3], Step [17300/138038], Loss: 3.2058, Perplexity: 24.6761\n",
      "Epoch [1/3], Step [17400/138038], Loss: 2.7272, Perplexity: 15.29019\n",
      "Epoch [1/3], Step [17500/138038], Loss: 1.8890, Perplexity: 6.61284\n",
      "Epoch [1/3], Step [17600/138038], Loss: 3.5965, Perplexity: 36.4721\n",
      "Epoch [1/3], Step [17700/138038], Loss: 3.3295, Perplexity: 27.9231\n",
      "Epoch [1/3], Step [17800/138038], Loss: 2.6221, Perplexity: 13.76412\n",
      "Epoch [1/3], Step [17900/138038], Loss: 3.5530, Perplexity: 34.9189\n",
      "Epoch [1/3], Step [18000/138038], Loss: 3.2958, Perplexity: 26.9981\n",
      "Epoch [1/3], Step [18100/138038], Loss: 2.1197, Perplexity: 8.328268\n",
      "Epoch [1/3], Step [18200/138038], Loss: 2.5435, Perplexity: 12.7245\n",
      "Epoch [1/3], Step [18300/138038], Loss: 1.9768, Perplexity: 7.219582\n",
      "Epoch [1/3], Step [18400/138038], Loss: 2.6495, Perplexity: 14.1464\n",
      "Epoch [1/3], Step [18500/138038], Loss: 2.8928, Perplexity: 18.04335\n",
      "Epoch [1/3], Step [18600/138038], Loss: 3.0150, Perplexity: 20.3889\n",
      "Epoch [1/3], Step [18700/138038], Loss: 2.3779, Perplexity: 10.7823\n",
      "Epoch [1/3], Step [18800/138038], Loss: 3.2475, Perplexity: 25.7258\n",
      "Epoch [1/3], Step [18900/138038], Loss: 3.0064, Perplexity: 20.2155\n",
      "Epoch [1/3], Step [19000/138038], Loss: 3.1287, Perplexity: 22.8438\n",
      "Epoch [1/3], Step [19100/138038], Loss: 2.5483, Perplexity: 12.7854\n",
      "Epoch [1/3], Step [19200/138038], Loss: 2.9914, Perplexity: 19.91274\n",
      "Epoch [1/3], Step [19300/138038], Loss: 2.5172, Perplexity: 12.39428\n",
      "Epoch [1/3], Step [19400/138038], Loss: 2.3329, Perplexity: 10.30819\n",
      "Epoch [1/3], Step [19500/138038], Loss: 2.4974, Perplexity: 12.1512\n",
      "Epoch [1/3], Step [19600/138038], Loss: 2.8579, Perplexity: 17.42560\n",
      "Epoch [1/3], Step [19700/138038], Loss: 3.1286, Perplexity: 22.84167\n",
      "Epoch [1/3], Step [19800/138038], Loss: 2.9184, Perplexity: 18.5110\n",
      "Epoch [1/3], Step [19900/138038], Loss: 3.6717, Perplexity: 39.3170\n",
      "Epoch [1/3], Step [20000/138038], Loss: 3.7170, Perplexity: 41.1418\n",
      "Epoch [1/3], Step [20100/138038], Loss: 3.0137, Perplexity: 20.36207\n",
      "Epoch [1/3], Step [20200/138038], Loss: 2.1988, Perplexity: 9.01391\n",
      "Epoch [1/3], Step [20300/138038], Loss: 2.2804, Perplexity: 9.780814\n",
      "Epoch [1/3], Step [20400/138038], Loss: 2.3525, Perplexity: 10.5115\n",
      "Epoch [1/3], Step [20500/138038], Loss: 2.5444, Perplexity: 12.7352\n",
      "Epoch [1/3], Step [20600/138038], Loss: 3.3925, Perplexity: 29.7406\n",
      "Epoch [1/3], Step [20700/138038], Loss: 3.4985, Perplexity: 33.0642\n",
      "Epoch [1/3], Step [20800/138038], Loss: 3.4191, Perplexity: 30.54316\n",
      "Epoch [1/3], Step [20900/138038], Loss: 3.5989, Perplexity: 36.55761\n",
      "Epoch [1/3], Step [21000/138038], Loss: 2.7054, Perplexity: 14.9609\n",
      "Epoch [1/3], Step [21100/138038], Loss: 2.6859, Perplexity: 14.6709\n",
      "Epoch [1/3], Step [21200/138038], Loss: 4.1257, Perplexity: 61.90849\n",
      "Epoch [1/3], Step [21300/138038], Loss: 2.6496, Perplexity: 14.14901\n",
      "Epoch [1/3], Step [21400/138038], Loss: 3.9568, Perplexity: 52.28742\n",
      "Epoch [1/3], Step [21500/138038], Loss: 2.9226, Perplexity: 18.58914\n",
      "Epoch [1/3], Step [21600/138038], Loss: 2.3655, Perplexity: 10.6493\n",
      "Epoch [1/3], Step [21700/138038], Loss: 3.1917, Perplexity: 24.3309\n",
      "Epoch [1/3], Step [21800/138038], Loss: 2.5928, Perplexity: 13.3667\n",
      "Epoch [1/3], Step [21900/138038], Loss: 2.7402, Perplexity: 15.4896\n",
      "Epoch [1/3], Step [22000/138038], Loss: 2.9347, Perplexity: 18.8166\n",
      "Epoch [1/3], Step [22100/138038], Loss: 3.3706, Perplexity: 29.09486\n",
      "Epoch [1/3], Step [22200/138038], Loss: 2.8116, Perplexity: 16.6366\n",
      "Epoch [1/3], Step [22300/138038], Loss: 3.1196, Perplexity: 22.63636\n",
      "Epoch [1/3], Step [22400/138038], Loss: 2.8138, Perplexity: 16.6724\n",
      "Epoch [1/3], Step [22500/138038], Loss: 3.1880, Perplexity: 24.23937\n",
      "Epoch [1/3], Step [22600/138038], Loss: 3.0758, Perplexity: 21.6670\n",
      "Epoch [1/3], Step [22700/138038], Loss: 1.9139, Perplexity: 6.77967\n",
      "Epoch [1/3], Step [22800/138038], Loss: 2.1141, Perplexity: 8.28227\n",
      "Epoch [1/3], Step [22900/138038], Loss: 2.9487, Perplexity: 19.08162\n",
      "Epoch [1/3], Step [23000/138038], Loss: 2.7531, Perplexity: 15.69101\n",
      "Epoch [1/3], Step [23100/138038], Loss: 4.7895, Perplexity: 120.2432\n",
      "Epoch [1/3], Step [23200/138038], Loss: 2.0275, Perplexity: 7.59515\n",
      "Epoch [1/3], Step [23300/138038], Loss: 2.8456, Perplexity: 17.21164\n",
      "Epoch [1/3], Step [23400/138038], Loss: 2.3365, Perplexity: 10.3446\n",
      "Epoch [1/3], Step [23500/138038], Loss: 3.1795, Perplexity: 24.0344\n",
      "Epoch [1/3], Step [23600/138038], Loss: 2.4743, Perplexity: 11.87351\n",
      "Epoch [1/3], Step [23700/138038], Loss: 3.5692, Perplexity: 35.48789\n",
      "Epoch [1/3], Step [23800/138038], Loss: 5.2419, Perplexity: 189.0357\n",
      "Epoch [1/3], Step [23900/138038], Loss: 2.2927, Perplexity: 9.90170\n",
      "Epoch [1/3], Step [24000/138038], Loss: 2.4866, Perplexity: 12.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24100/138038], Loss: 2.8817, Perplexity: 17.84521\n",
      "Epoch [1/3], Step [24200/138038], Loss: 3.3792, Perplexity: 29.34755\n",
      "Epoch [1/3], Step [24300/138038], Loss: 2.5739, Perplexity: 13.11694\n",
      "Epoch [1/3], Step [24400/138038], Loss: 2.3819, Perplexity: 10.82539\n",
      "Epoch [1/3], Step [24500/138038], Loss: 3.8884, Perplexity: 48.83479\n",
      "Epoch [1/3], Step [24600/138038], Loss: 2.5974, Perplexity: 13.42875\n",
      "Epoch [1/3], Step [24700/138038], Loss: 3.3128, Perplexity: 27.46327\n",
      "Epoch [1/3], Step [24800/138038], Loss: 2.7550, Perplexity: 15.72175\n",
      "Epoch [1/3], Step [24900/138038], Loss: 3.2608, Perplexity: 26.0709\n",
      "Epoch [1/3], Step [25000/138038], Loss: 2.9898, Perplexity: 19.88090\n",
      "Epoch [1/3], Step [25100/138038], Loss: 2.1415, Perplexity: 8.51213\n",
      "Epoch [1/3], Step [25200/138038], Loss: 3.0569, Perplexity: 21.2626\n",
      "Epoch [1/3], Step [25300/138038], Loss: 3.1833, Perplexity: 24.12686\n",
      "Epoch [1/3], Step [25400/138038], Loss: 2.4326, Perplexity: 11.38811\n",
      "Epoch [1/3], Step [25500/138038], Loss: 3.1835, Perplexity: 24.1308\n",
      "Epoch [1/3], Step [25600/138038], Loss: 2.9541, Perplexity: 19.18435\n",
      "Epoch [1/3], Step [25700/138038], Loss: 3.0049, Perplexity: 20.1834\n",
      "Epoch [1/3], Step [25800/138038], Loss: 3.6883, Perplexity: 39.9762\n",
      "Epoch [1/3], Step [25900/138038], Loss: 2.2833, Perplexity: 9.80897\n",
      "Epoch [1/3], Step [26000/138038], Loss: 2.1590, Perplexity: 8.66228\n",
      "Epoch [1/3], Step [26100/138038], Loss: 2.9406, Perplexity: 18.92648\n",
      "Epoch [1/3], Step [26200/138038], Loss: 2.4859, Perplexity: 12.01209\n",
      "Epoch [1/3], Step [26300/138038], Loss: 2.1343, Perplexity: 8.45131\n",
      "Epoch [1/3], Step [26400/138038], Loss: 3.8050, Perplexity: 44.92530\n",
      "Epoch [1/3], Step [26500/138038], Loss: 3.0997, Perplexity: 22.1910\n",
      "Epoch [1/3], Step [26600/138038], Loss: 2.7732, Perplexity: 16.00910\n",
      "Epoch [1/3], Step [26700/138038], Loss: 2.8673, Perplexity: 17.5903\n",
      "Epoch [1/3], Step [26800/138038], Loss: 3.6759, Perplexity: 39.48311\n",
      "Epoch [1/3], Step [26900/138038], Loss: 2.9767, Perplexity: 19.6238\n",
      "Epoch [1/3], Step [27000/138038], Loss: 3.5826, Perplexity: 35.96725\n",
      "Epoch [1/3], Step [27100/138038], Loss: 2.2879, Perplexity: 9.854542\n",
      "Epoch [1/3], Step [27200/138038], Loss: 2.2989, Perplexity: 9.963541\n",
      "Epoch [1/3], Step [27300/138038], Loss: 1.9863, Perplexity: 7.288555\n",
      "Epoch [1/3], Step [27400/138038], Loss: 3.2620, Perplexity: 26.10086\n",
      "Epoch [1/3], Step [27500/138038], Loss: 2.4710, Perplexity: 11.83454\n",
      "Epoch [1/3], Step [27600/138038], Loss: 3.6292, Perplexity: 37.6814\n",
      "Epoch [1/3], Step [27700/138038], Loss: 2.8199, Perplexity: 16.77544\n",
      "Epoch [1/3], Step [27800/138038], Loss: 2.5060, Perplexity: 12.25524\n",
      "Epoch [1/3], Step [27900/138038], Loss: 4.1454, Perplexity: 63.14345\n",
      "Epoch [1/3], Step [28000/138038], Loss: 2.5134, Perplexity: 12.3470\n",
      "Epoch [1/3], Step [28100/138038], Loss: 2.1350, Perplexity: 8.45743\n",
      "Epoch [1/3], Step [28200/138038], Loss: 3.6048, Perplexity: 36.7746\n",
      "Epoch [1/3], Step [28300/138038], Loss: 4.1940, Perplexity: 66.2853\n",
      "Epoch [1/3], Step [28400/138038], Loss: 2.6930, Perplexity: 14.7765\n",
      "Epoch [1/3], Step [28500/138038], Loss: 2.2130, Perplexity: 9.143233\n",
      "Epoch [1/3], Step [28600/138038], Loss: 3.4621, Perplexity: 31.88292\n",
      "Epoch [1/3], Step [28700/138038], Loss: 2.4031, Perplexity: 11.0572\n",
      "Epoch [1/3], Step [28800/138038], Loss: 2.7839, Perplexity: 16.18252\n",
      "Epoch [1/3], Step [28900/138038], Loss: 3.2513, Perplexity: 25.82509\n",
      "Epoch [1/3], Step [29000/138038], Loss: 2.7124, Perplexity: 15.06525\n",
      "Epoch [1/3], Step [29100/138038], Loss: 3.4552, Perplexity: 31.66589\n",
      "Epoch [1/3], Step [29200/138038], Loss: 3.3529, Perplexity: 28.58505\n",
      "Epoch [1/3], Step [29300/138038], Loss: 4.9950, Perplexity: 147.6802\n",
      "Epoch [1/3], Step [29400/138038], Loss: 2.5671, Perplexity: 13.02858\n",
      "Epoch [1/3], Step [29500/138038], Loss: 2.2581, Perplexity: 9.565070\n",
      "Epoch [1/3], Step [29600/138038], Loss: 2.7691, Perplexity: 15.94417\n",
      "Epoch [1/3], Step [29700/138038], Loss: 2.7117, Perplexity: 15.0552\n",
      "Epoch [1/3], Step [29800/138038], Loss: 3.6613, Perplexity: 38.9104\n",
      "Epoch [1/3], Step [29900/138038], Loss: 4.9034, Perplexity: 134.7465\n",
      "Epoch [1/3], Step [30000/138038], Loss: 2.6536, Perplexity: 14.20455\n",
      "Epoch [1/3], Step [30100/138038], Loss: 2.9823, Perplexity: 19.7322\n",
      "Epoch [1/3], Step [30200/138038], Loss: 2.4917, Perplexity: 12.0820\n",
      "Epoch [1/3], Step [30300/138038], Loss: 2.3837, Perplexity: 10.8453\n",
      "Epoch [1/3], Step [30400/138038], Loss: 2.3758, Perplexity: 10.7598\n",
      "Epoch [1/3], Step [30500/138038], Loss: 2.6806, Perplexity: 14.59415\n",
      "Epoch [1/3], Step [30600/138038], Loss: 2.1636, Perplexity: 8.70238\n",
      "Epoch [1/3], Step [30700/138038], Loss: 2.6528, Perplexity: 14.1932\n",
      "Epoch [1/3], Step [30800/138038], Loss: 3.3214, Perplexity: 27.69874\n",
      "Epoch [1/3], Step [30900/138038], Loss: 1.7303, Perplexity: 5.642467\n",
      "Epoch [1/3], Step [31000/138038], Loss: 2.4139, Perplexity: 11.1776\n",
      "Epoch [1/3], Step [31100/138038], Loss: 2.8039, Perplexity: 16.5084\n",
      "Epoch [1/3], Step [31200/138038], Loss: 3.1706, Perplexity: 23.8216\n",
      "Epoch [1/3], Step [31300/138038], Loss: 3.8460, Perplexity: 46.8036\n",
      "Epoch [1/3], Step [31400/138038], Loss: 2.9242, Perplexity: 18.61909\n",
      "Epoch [1/3], Step [31500/138038], Loss: 3.5327, Perplexity: 34.2152\n",
      "Epoch [1/3], Step [31600/138038], Loss: 2.3953, Perplexity: 10.9712\n",
      "Epoch [1/3], Step [31700/138038], Loss: 3.3274, Perplexity: 27.86608\n",
      "Epoch [1/3], Step [31800/138038], Loss: 2.0925, Perplexity: 8.10510\n",
      "Epoch [1/3], Step [31900/138038], Loss: 2.9153, Perplexity: 18.45473\n",
      "Epoch [1/3], Step [32000/138038], Loss: 2.6993, Perplexity: 14.8687\n",
      "Epoch [1/3], Step [32100/138038], Loss: 3.1898, Perplexity: 24.2824\n",
      "Epoch [1/3], Step [32200/138038], Loss: 2.8065, Perplexity: 16.55181\n",
      "Epoch [1/3], Step [32300/138038], Loss: 3.6956, Perplexity: 40.2695\n",
      "Epoch [1/3], Step [32400/138038], Loss: 2.8736, Perplexity: 17.7012\n",
      "Epoch [1/3], Step [32500/138038], Loss: 2.5733, Perplexity: 13.10888\n",
      "Epoch [1/3], Step [32600/138038], Loss: 2.3958, Perplexity: 10.9768\n",
      "Epoch [1/3], Step [32700/138038], Loss: 3.3463, Perplexity: 28.3972\n",
      "Epoch [1/3], Step [32800/138038], Loss: 2.2486, Perplexity: 9.47454\n",
      "Epoch [1/3], Step [32900/138038], Loss: 3.0242, Perplexity: 20.57828\n",
      "Epoch [1/3], Step [33000/138038], Loss: 3.6221, Perplexity: 37.4174\n",
      "Epoch [1/3], Step [33100/138038], Loss: 2.7971, Perplexity: 16.39769\n",
      "Epoch [1/3], Step [33200/138038], Loss: 2.8764, Perplexity: 17.75039\n",
      "Epoch [1/3], Step [33300/138038], Loss: 4.1871, Perplexity: 65.83295\n",
      "Epoch [1/3], Step [33400/138038], Loss: 2.9060, Perplexity: 18.28386\n",
      "Epoch [1/3], Step [33500/138038], Loss: 2.8740, Perplexity: 17.70777\n",
      "Epoch [1/3], Step [33600/138038], Loss: 2.3722, Perplexity: 10.7211\n",
      "Epoch [1/3], Step [33700/138038], Loss: 2.7678, Perplexity: 15.92339\n",
      "Epoch [1/3], Step [33800/138038], Loss: 3.5119, Perplexity: 33.51115\n",
      "Epoch [1/3], Step [33900/138038], Loss: 2.3091, Perplexity: 10.06491\n",
      "Epoch [1/3], Step [34000/138038], Loss: 2.5520, Perplexity: 12.8330\n",
      "Epoch [1/3], Step [34100/138038], Loss: 3.7371, Perplexity: 41.97756\n",
      "Epoch [1/3], Step [34200/138038], Loss: 2.9323, Perplexity: 18.7712\n",
      "Epoch [1/3], Step [34300/138038], Loss: 3.3710, Perplexity: 29.10715\n",
      "Epoch [1/3], Step [34400/138038], Loss: 3.4037, Perplexity: 30.07520\n",
      "Epoch [1/3], Step [34500/138038], Loss: 3.6737, Perplexity: 39.3977\n",
      "Epoch [1/3], Step [34600/138038], Loss: 2.4689, Perplexity: 11.8090\n",
      "Epoch [1/3], Step [34700/138038], Loss: 2.6047, Perplexity: 13.52755\n",
      "Epoch [1/3], Step [34800/138038], Loss: 3.1612, Perplexity: 23.5983\n",
      "Epoch [1/3], Step [34900/138038], Loss: 2.0014, Perplexity: 7.39978\n",
      "Epoch [1/3], Step [35000/138038], Loss: 2.4573, Perplexity: 11.67297\n",
      "Epoch [1/3], Step [35100/138038], Loss: 1.8911, Perplexity: 6.626573\n",
      "Epoch [1/3], Step [35200/138038], Loss: 2.7160, Perplexity: 15.1193\n",
      "Epoch [1/3], Step [35300/138038], Loss: 4.0972, Perplexity: 60.16968\n",
      "Epoch [1/3], Step [35400/138038], Loss: 2.8079, Perplexity: 16.5751\n",
      "Epoch [1/3], Step [35500/138038], Loss: 3.0318, Perplexity: 20.73409\n",
      "Epoch [1/3], Step [35600/138038], Loss: 2.4852, Perplexity: 12.00344\n",
      "Epoch [1/3], Step [35700/138038], Loss: 3.0469, Perplexity: 21.05107\n",
      "Epoch [1/3], Step [35800/138038], Loss: 3.1381, Perplexity: 23.0607\n",
      "Epoch [1/3], Step [35900/138038], Loss: 2.3085, Perplexity: 10.05889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [36000/138038], Loss: 2.2816, Perplexity: 9.79264\n",
      "Epoch [1/3], Step [36100/138038], Loss: 2.5006, Perplexity: 12.19005\n",
      "Epoch [1/3], Step [36200/138038], Loss: 1.8668, Perplexity: 6.46731\n",
      "Epoch [1/3], Step [36300/138038], Loss: 3.1962, Perplexity: 24.43902\n",
      "Epoch [1/3], Step [36400/138038], Loss: 3.3298, Perplexity: 27.93414\n",
      "Epoch [1/3], Step [36500/138038], Loss: 2.1622, Perplexity: 8.69063\n",
      "Epoch [1/3], Step [36600/138038], Loss: 3.1644, Perplexity: 23.6756\n",
      "Epoch [1/3], Step [36700/138038], Loss: 2.7430, Perplexity: 15.5332\n",
      "Epoch [1/3], Step [36800/138038], Loss: 1.8140, Perplexity: 6.134733\n",
      "Epoch [1/3], Step [36900/138038], Loss: 3.8543, Perplexity: 47.19407\n",
      "Epoch [1/3], Step [37000/138038], Loss: 3.4390, Perplexity: 31.1562\n",
      "Epoch [1/3], Step [37100/138038], Loss: 2.2522, Perplexity: 9.50888\n",
      "Epoch [1/3], Step [37200/138038], Loss: 1.8642, Perplexity: 6.45114\n",
      "Epoch [1/3], Step [37300/138038], Loss: 2.6673, Perplexity: 14.40172\n",
      "Epoch [1/3], Step [37400/138038], Loss: 2.5333, Perplexity: 12.5949\n",
      "Epoch [1/3], Step [37500/138038], Loss: 3.2179, Perplexity: 24.97490\n",
      "Epoch [1/3], Step [37600/138038], Loss: 2.0699, Perplexity: 7.924122\n",
      "Epoch [1/3], Step [37700/138038], Loss: 3.2231, Perplexity: 25.1065\n",
      "Epoch [1/3], Step [37800/138038], Loss: 3.0442, Perplexity: 20.9932\n",
      "Epoch [1/3], Step [37900/138038], Loss: 4.2211, Perplexity: 68.11017\n",
      "Epoch [1/3], Step [38000/138038], Loss: 3.4825, Perplexity: 32.5401\n",
      "Epoch [1/3], Step [38100/138038], Loss: 2.1963, Perplexity: 8.991895\n",
      "Epoch [1/3], Step [38200/138038], Loss: 2.5911, Perplexity: 13.34468\n",
      "Epoch [1/3], Step [38300/138038], Loss: 2.6910, Perplexity: 14.7462\n",
      "Epoch [1/3], Step [38400/138038], Loss: 3.0775, Perplexity: 21.7043\n",
      "Epoch [1/3], Step [38500/138038], Loss: 3.7078, Perplexity: 40.76392\n",
      "Epoch [1/3], Step [38600/138038], Loss: 1.9002, Perplexity: 6.68745\n",
      "Epoch [1/3], Step [38700/138038], Loss: 2.6444, Perplexity: 14.0753\n",
      "Epoch [1/3], Step [38800/138038], Loss: 2.8999, Perplexity: 18.1714\n",
      "Epoch [1/3], Step [38900/138038], Loss: 2.5442, Perplexity: 12.7328\n",
      "Epoch [1/3], Step [39000/138038], Loss: 2.4207, Perplexity: 11.2536\n",
      "Epoch [1/3], Step [39100/138038], Loss: 3.3802, Perplexity: 29.37670\n",
      "Epoch [1/3], Step [39200/138038], Loss: 2.6227, Perplexity: 13.7734\n",
      "Epoch [1/3], Step [39300/138038], Loss: 3.3401, Perplexity: 28.22258\n",
      "Epoch [1/3], Step [39400/138038], Loss: 2.5554, Perplexity: 12.87701\n",
      "Epoch [1/3], Step [39500/138038], Loss: 2.4518, Perplexity: 11.6097\n",
      "Epoch [1/3], Step [39600/138038], Loss: 2.8291, Perplexity: 16.9309\n",
      "Epoch [1/3], Step [39700/138038], Loss: 3.5542, Perplexity: 34.9591\n",
      "Epoch [1/3], Step [39800/138038], Loss: 2.8875, Perplexity: 17.94845\n",
      "Epoch [1/3], Step [39900/138038], Loss: 3.6248, Perplexity: 37.51814\n",
      "Epoch [1/3], Step [40000/138038], Loss: 1.7653, Perplexity: 5.843408\n",
      "Epoch [1/3], Step [40100/138038], Loss: 2.6667, Perplexity: 14.3917\n",
      "Epoch [1/3], Step [40200/138038], Loss: 2.9446, Perplexity: 19.0025\n",
      "Epoch [1/3], Step [40300/138038], Loss: 2.8873, Perplexity: 17.94541\n",
      "Epoch [1/3], Step [40400/138038], Loss: 3.1997, Perplexity: 24.52548\n",
      "Epoch [1/3], Step [40500/138038], Loss: 3.1142, Perplexity: 22.5144\n",
      "Epoch [1/3], Step [40600/138038], Loss: 2.5926, Perplexity: 13.36380\n",
      "Epoch [1/3], Step [40700/138038], Loss: 2.4788, Perplexity: 11.92712\n",
      "Epoch [1/3], Step [40800/138038], Loss: 2.9974, Perplexity: 20.0334\n",
      "Epoch [1/3], Step [40900/138038], Loss: 1.9546, Perplexity: 7.06135\n",
      "Epoch [1/3], Step [41000/138038], Loss: 2.8932, Perplexity: 18.05118\n",
      "Epoch [1/3], Step [41100/138038], Loss: 2.4168, Perplexity: 11.2097\n",
      "Epoch [1/3], Step [41200/138038], Loss: 3.2803, Perplexity: 26.5847\n",
      "Epoch [1/3], Step [41300/138038], Loss: 4.1863, Perplexity: 65.78193\n",
      "Epoch [1/3], Step [41400/138038], Loss: 2.3791, Perplexity: 10.79482\n",
      "Epoch [1/3], Step [41500/138038], Loss: 3.6674, Perplexity: 39.15178\n",
      "Epoch [1/3], Step [41600/138038], Loss: 2.2264, Perplexity: 9.26675\n",
      "Epoch [1/3], Step [41700/138038], Loss: 3.6961, Perplexity: 40.29135\n",
      "Epoch [1/3], Step [41800/138038], Loss: 3.5041, Perplexity: 33.25040\n",
      "Epoch [1/3], Step [41900/138038], Loss: 2.9499, Perplexity: 19.10462\n",
      "Epoch [1/3], Step [42000/138038], Loss: 3.0501, Perplexity: 21.1175\n",
      "Epoch [1/3], Step [42100/138038], Loss: 3.2457, Perplexity: 25.67949\n",
      "Epoch [1/3], Step [42200/138038], Loss: 2.5262, Perplexity: 12.5055\n",
      "Epoch [1/3], Step [42300/138038], Loss: 3.2197, Perplexity: 25.0203\n",
      "Epoch [1/3], Step [42400/138038], Loss: 2.9972, Perplexity: 20.02928\n",
      "Epoch [1/3], Step [42500/138038], Loss: 2.3714, Perplexity: 10.7120\n",
      "Epoch [1/3], Step [42600/138038], Loss: 3.6628, Perplexity: 38.9692\n",
      "Epoch [1/3], Step [42700/138038], Loss: 3.5641, Perplexity: 35.3084\n",
      "Epoch [1/3], Step [42800/138038], Loss: 4.1692, Perplexity: 64.66181\n",
      "Epoch [1/3], Step [42900/138038], Loss: 3.3047, Perplexity: 27.24089\n",
      "Epoch [1/3], Step [43000/138038], Loss: 2.7769, Perplexity: 16.06929\n",
      "Epoch [1/3], Step [43100/138038], Loss: 2.9938, Perplexity: 19.96160\n",
      "Epoch [1/3], Step [43200/138038], Loss: 3.3754, Perplexity: 29.23487\n",
      "Epoch [1/3], Step [43300/138038], Loss: 4.6907, Perplexity: 108.9334\n",
      "Epoch [1/3], Step [43400/138038], Loss: 2.0584, Perplexity: 7.83352\n",
      "Epoch [1/3], Step [43500/138038], Loss: 1.7588, Perplexity: 5.805323\n",
      "Epoch [1/3], Step [43600/138038], Loss: 3.0215, Perplexity: 20.5227\n",
      "Epoch [1/3], Step [43700/138038], Loss: 2.6640, Perplexity: 14.35379\n",
      "Epoch [1/3], Step [43800/138038], Loss: 2.3028, Perplexity: 10.0023\n",
      "Epoch [1/3], Step [43900/138038], Loss: 2.9775, Perplexity: 19.63843\n",
      "Epoch [1/3], Step [44000/138038], Loss: 2.3542, Perplexity: 10.53015\n",
      "Epoch [1/3], Step [44100/138038], Loss: 1.7782, Perplexity: 5.91929\n",
      "Epoch [1/3], Step [44200/138038], Loss: 2.6525, Perplexity: 14.18885\n",
      "Epoch [1/3], Step [44300/138038], Loss: 1.7391, Perplexity: 5.692046\n",
      "Epoch [1/3], Step [44400/138038], Loss: 2.7761, Perplexity: 16.0556\n",
      "Epoch [1/3], Step [44500/138038], Loss: 4.0560, Perplexity: 57.7442\n",
      "Epoch [1/3], Step [44600/138038], Loss: 2.7741, Perplexity: 16.0242\n",
      "Epoch [1/3], Step [44700/138038], Loss: 2.7534, Perplexity: 15.6956\n",
      "Epoch [1/3], Step [44800/138038], Loss: 3.6549, Perplexity: 38.66215\n",
      "Epoch [1/3], Step [44900/138038], Loss: 1.8318, Perplexity: 6.24538\n",
      "Epoch [1/3], Step [45000/138038], Loss: 2.6859, Perplexity: 14.67140\n",
      "Epoch [1/3], Step [45100/138038], Loss: 2.4604, Perplexity: 11.70950\n",
      "Epoch [1/3], Step [45200/138038], Loss: 2.4747, Perplexity: 11.8785\n",
      "Epoch [1/3], Step [45300/138038], Loss: 2.8354, Perplexity: 17.0373\n",
      "Epoch [1/3], Step [45400/138038], Loss: 2.4273, Perplexity: 11.32823\n",
      "Epoch [1/3], Step [45500/138038], Loss: 4.5002, Perplexity: 90.0359\n",
      "Epoch [1/3], Step [45600/138038], Loss: 3.0790, Perplexity: 21.7372\n",
      "Epoch [1/3], Step [45700/138038], Loss: 2.1110, Perplexity: 8.25672\n",
      "Epoch [1/3], Step [45800/138038], Loss: 3.9385, Perplexity: 51.34118\n",
      "Epoch [1/3], Step [45900/138038], Loss: 2.8642, Perplexity: 17.53539\n",
      "Epoch [1/3], Step [46000/138038], Loss: 2.4333, Perplexity: 11.39697\n",
      "Epoch [1/3], Step [46100/138038], Loss: 2.5277, Perplexity: 12.52513\n",
      "Epoch [1/3], Step [46200/138038], Loss: 1.8605, Perplexity: 6.427351\n",
      "Epoch [1/3], Step [46300/138038], Loss: 2.1914, Perplexity: 8.94798\n",
      "Epoch [1/3], Step [46400/138038], Loss: 2.4145, Perplexity: 11.1847\n",
      "Epoch [1/3], Step [46500/138038], Loss: 3.1180, Perplexity: 22.6003\n",
      "Epoch [1/3], Step [46600/138038], Loss: 2.5270, Perplexity: 12.51629\n",
      "Epoch [1/3], Step [46700/138038], Loss: 3.2990, Perplexity: 27.08474\n",
      "Epoch [1/3], Step [46800/138038], Loss: 2.0895, Perplexity: 8.08052\n",
      "Epoch [1/3], Step [46900/138038], Loss: 2.0383, Perplexity: 7.677780\n",
      "Epoch [1/3], Step [47000/138038], Loss: 2.0195, Perplexity: 7.53446\n",
      "Epoch [1/3], Step [47100/138038], Loss: 2.6540, Perplexity: 14.21109\n",
      "Epoch [1/3], Step [47200/138038], Loss: 3.0040, Perplexity: 20.1662\n",
      "Epoch [1/3], Step [47300/138038], Loss: 3.3135, Perplexity: 27.48142\n",
      "Epoch [1/3], Step [47400/138038], Loss: 3.4832, Perplexity: 32.5632\n",
      "Epoch [1/3], Step [47500/138038], Loss: 2.5540, Perplexity: 12.85845\n",
      "Epoch [1/3], Step [47600/138038], Loss: 3.2677, Perplexity: 26.25123\n",
      "Epoch [1/3], Step [47700/138038], Loss: 2.2353, Perplexity: 9.34906\n",
      "Epoch [1/3], Step [47800/138038], Loss: 2.5317, Perplexity: 12.5746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [47900/138038], Loss: 3.1202, Perplexity: 22.65009\n",
      "Epoch [1/3], Step [48000/138038], Loss: 3.8524, Perplexity: 47.1068\n",
      "Epoch [1/3], Step [48100/138038], Loss: 2.5847, Perplexity: 13.25969\n",
      "Epoch [1/3], Step [48200/138038], Loss: 2.0853, Perplexity: 8.04739\n",
      "Epoch [1/3], Step [48300/138038], Loss: 1.9570, Perplexity: 7.078428\n",
      "Epoch [1/3], Step [48400/138038], Loss: 3.4398, Perplexity: 31.18156\n",
      "Epoch [1/3], Step [48500/138038], Loss: 3.7587, Perplexity: 42.8926\n",
      "Epoch [1/3], Step [48600/138038], Loss: 2.0457, Perplexity: 7.73420\n",
      "Epoch [1/3], Step [48700/138038], Loss: 2.3135, Perplexity: 10.10992\n",
      "Epoch [1/3], Step [48800/138038], Loss: 2.7472, Perplexity: 15.5997\n",
      "Epoch [1/3], Step [48900/138038], Loss: 4.0048, Perplexity: 54.8629\n",
      "Epoch [1/3], Step [49000/138038], Loss: 2.4247, Perplexity: 11.2989\n",
      "Epoch [1/3], Step [49100/138038], Loss: 1.5961, Perplexity: 4.93354\n",
      "Epoch [1/3], Step [49200/138038], Loss: 3.0476, Perplexity: 21.06469\n",
      "Epoch [1/3], Step [49300/138038], Loss: 3.0597, Perplexity: 21.32198\n",
      "Epoch [1/3], Step [49400/138038], Loss: 2.3231, Perplexity: 10.2076\n",
      "Epoch [1/3], Step [49500/138038], Loss: 3.5327, Perplexity: 34.2159\n",
      "Epoch [1/3], Step [49600/138038], Loss: 3.8043, Perplexity: 44.89531\n",
      "Epoch [1/3], Step [49700/138038], Loss: 3.1085, Perplexity: 22.3883\n",
      "Epoch [1/3], Step [49800/138038], Loss: 2.3280, Perplexity: 10.2578\n",
      "Epoch [1/3], Step [49900/138038], Loss: 2.8797, Perplexity: 17.8095\n",
      "Epoch [1/3], Step [50000/138038], Loss: 4.3218, Perplexity: 75.32240\n",
      "Epoch [1/3], Step [50100/138038], Loss: 2.7091, Perplexity: 15.01566\n",
      "Epoch [1/3], Step [50200/138038], Loss: 2.6780, Perplexity: 14.55627\n",
      "Epoch [1/3], Step [50300/138038], Loss: 3.1625, Perplexity: 23.62858\n",
      "Epoch [1/3], Step [50400/138038], Loss: 2.2484, Perplexity: 9.47293\n",
      "Epoch [1/3], Step [50500/138038], Loss: 2.8551, Perplexity: 17.37615\n",
      "Epoch [1/3], Step [50600/138038], Loss: 2.6563, Perplexity: 14.2431\n",
      "Epoch [1/3], Step [50700/138038], Loss: 3.5644, Perplexity: 35.3168\n",
      "Epoch [1/3], Step [50800/138038], Loss: 2.6273, Perplexity: 13.8360\n",
      "Epoch [1/3], Step [50900/138038], Loss: 2.9122, Perplexity: 18.3967\n",
      "Epoch [1/3], Step [51000/138038], Loss: 2.2223, Perplexity: 9.228721\n",
      "Epoch [1/3], Step [51100/138038], Loss: 3.3126, Perplexity: 27.45740\n",
      "Epoch [1/3], Step [51200/138038], Loss: 2.7701, Perplexity: 15.9600\n",
      "Epoch [1/3], Step [51300/138038], Loss: 2.1141, Perplexity: 8.28188\n",
      "Epoch [1/3], Step [51400/138038], Loss: 6.0518, Perplexity: 424.8878\n",
      "Epoch [1/3], Step [51500/138038], Loss: 4.3811, Perplexity: 79.92761\n",
      "Epoch [1/3], Step [51600/138038], Loss: 2.6720, Perplexity: 14.46947\n",
      "Epoch [1/3], Step [51700/138038], Loss: 2.9036, Perplexity: 18.2394\n",
      "Epoch [1/3], Step [51800/138038], Loss: 2.3067, Perplexity: 10.04167\n",
      "Epoch [1/3], Step [51900/138038], Loss: 2.2352, Perplexity: 9.348720\n",
      "Epoch [1/3], Step [52000/138038], Loss: 2.5785, Perplexity: 13.1772\n",
      "Epoch [1/3], Step [52100/138038], Loss: 2.9780, Perplexity: 19.6490\n",
      "Epoch [1/3], Step [52200/138038], Loss: 2.9419, Perplexity: 18.9517\n",
      "Epoch [1/3], Step [52300/138038], Loss: 3.4599, Perplexity: 31.81364\n",
      "Epoch [1/3], Step [52400/138038], Loss: 3.5658, Perplexity: 35.3666\n",
      "Epoch [1/3], Step [52500/138038], Loss: 3.2144, Perplexity: 24.8874\n",
      "Epoch [1/3], Step [52600/138038], Loss: 3.0722, Perplexity: 21.5897\n",
      "Epoch [1/3], Step [52700/138038], Loss: 3.1795, Perplexity: 24.0336\n",
      "Epoch [1/3], Step [52800/138038], Loss: 3.1046, Perplexity: 22.2997\n",
      "Epoch [1/3], Step [52900/138038], Loss: 2.2865, Perplexity: 9.840461\n",
      "Epoch [1/3], Step [53000/138038], Loss: 3.2425, Perplexity: 25.59646\n",
      "Epoch [1/3], Step [53100/138038], Loss: 2.0252, Perplexity: 7.577413\n",
      "Epoch [1/3], Step [53200/138038], Loss: 2.9570, Perplexity: 19.23945\n",
      "Epoch [1/3], Step [53300/138038], Loss: 2.0376, Perplexity: 7.67185\n",
      "Epoch [1/3], Step [53400/138038], Loss: 2.8543, Perplexity: 17.3625\n",
      "Epoch [1/3], Step [53500/138038], Loss: 2.9072, Perplexity: 18.30502\n",
      "Epoch [1/3], Step [53600/138038], Loss: 3.8999, Perplexity: 49.39625\n",
      "Epoch [1/3], Step [53700/138038], Loss: 3.0890, Perplexity: 21.95452\n",
      "Epoch [1/3], Step [53800/138038], Loss: 2.0690, Perplexity: 7.91717\n",
      "Epoch [1/3], Step [53900/138038], Loss: 3.4945, Perplexity: 32.9333\n",
      "Epoch [1/3], Step [54000/138038], Loss: 2.9289, Perplexity: 18.7062\n",
      "Epoch [1/3], Step [54100/138038], Loss: 2.9851, Perplexity: 19.78804\n",
      "Epoch [1/3], Step [54200/138038], Loss: 2.9945, Perplexity: 19.9764\n",
      "Epoch [1/3], Step [54300/138038], Loss: 3.1355, Perplexity: 23.0011\n",
      "Epoch [1/3], Step [54400/138038], Loss: 2.9248, Perplexity: 18.63109\n",
      "Epoch [1/3], Step [54500/138038], Loss: 3.7195, Perplexity: 41.2457\n",
      "Epoch [1/3], Step [54600/138038], Loss: 3.0591, Perplexity: 21.3094\n",
      "Epoch [1/3], Step [54700/138038], Loss: 4.0239, Perplexity: 55.91939\n",
      "Epoch [1/3], Step [54800/138038], Loss: 2.8337, Perplexity: 17.00750\n",
      "Epoch [1/3], Step [54900/138038], Loss: 3.1205, Perplexity: 22.6568\n",
      "Epoch [1/3], Step [55000/138038], Loss: 2.5380, Perplexity: 12.65372\n",
      "Epoch [1/3], Step [55100/138038], Loss: 2.6632, Perplexity: 14.34214\n",
      "Epoch [1/3], Step [55200/138038], Loss: 1.8449, Perplexity: 6.32731\n",
      "Epoch [1/3], Step [55300/138038], Loss: 3.5361, Perplexity: 34.33164\n",
      "Epoch [1/3], Step [55400/138038], Loss: 3.0280, Perplexity: 20.6555\n",
      "Epoch [1/3], Step [55500/138038], Loss: 2.3372, Perplexity: 10.3520\n",
      "Epoch [1/3], Step [55600/138038], Loss: 1.8415, Perplexity: 6.306246\n",
      "Epoch [1/3], Step [55700/138038], Loss: 2.0724, Perplexity: 7.94350\n",
      "Epoch [1/3], Step [55800/138038], Loss: 3.1051, Perplexity: 22.31200\n",
      "Epoch [1/3], Step [55900/138038], Loss: 2.0423, Perplexity: 7.70811\n",
      "Epoch [1/3], Step [56000/138038], Loss: 3.9890, Perplexity: 54.00192\n",
      "Epoch [1/3], Step [56100/138038], Loss: 3.6246, Perplexity: 37.5106\n",
      "Epoch [1/3], Step [56200/138038], Loss: 3.1763, Perplexity: 23.95828\n",
      "Epoch [1/3], Step [56300/138038], Loss: 3.4836, Perplexity: 32.57767\n",
      "Epoch [1/3], Step [56400/138038], Loss: 2.9372, Perplexity: 18.8624\n",
      "Epoch [1/3], Step [56500/138038], Loss: 2.6470, Perplexity: 14.1114\n",
      "Epoch [1/3], Step [56600/138038], Loss: 3.4652, Perplexity: 31.98253\n",
      "Epoch [1/3], Step [56700/138038], Loss: 3.4268, Perplexity: 30.7777\n",
      "Epoch [1/3], Step [56800/138038], Loss: 2.5349, Perplexity: 12.6149\n",
      "Epoch [1/3], Step [56900/138038], Loss: 3.1093, Perplexity: 22.40533\n",
      "Epoch [1/3], Step [57000/138038], Loss: 1.5342, Perplexity: 4.637532\n",
      "Epoch [1/3], Step [57100/138038], Loss: 2.0003, Perplexity: 7.391439\n",
      "Epoch [1/3], Step [57200/138038], Loss: 2.8696, Perplexity: 17.62994\n",
      "Epoch [1/3], Step [57300/138038], Loss: 2.5714, Perplexity: 13.08416\n",
      "Epoch [1/3], Step [57400/138038], Loss: 1.7898, Perplexity: 5.988377\n",
      "Epoch [1/3], Step [57500/138038], Loss: 1.5002, Perplexity: 4.482424\n",
      "Epoch [1/3], Step [57600/138038], Loss: 2.0230, Perplexity: 7.560684\n",
      "Epoch [1/3], Step [57700/138038], Loss: 2.3504, Perplexity: 10.48933\n",
      "Epoch [1/3], Step [57800/138038], Loss: 2.5611, Perplexity: 12.9498\n",
      "Epoch [1/3], Step [57900/138038], Loss: 2.1692, Perplexity: 8.751705\n",
      "Epoch [1/3], Step [58000/138038], Loss: 1.9998, Perplexity: 7.387650\n",
      "Epoch [1/3], Step [58100/138038], Loss: 2.7585, Perplexity: 15.77609\n",
      "Epoch [1/3], Step [58200/138038], Loss: 2.4865, Perplexity: 12.01917\n",
      "Epoch [1/3], Step [58300/138038], Loss: 2.6280, Perplexity: 13.8457\n",
      "Epoch [1/3], Step [58400/138038], Loss: 2.7646, Perplexity: 15.87265\n",
      "Epoch [1/3], Step [58500/138038], Loss: 3.1338, Perplexity: 22.96010\n",
      "Epoch [1/3], Step [58600/138038], Loss: 2.7919, Perplexity: 16.31260\n",
      "Epoch [1/3], Step [58700/138038], Loss: 2.1403, Perplexity: 8.502262\n",
      "Epoch [1/3], Step [58800/138038], Loss: 2.0615, Perplexity: 7.85813\n",
      "Epoch [1/3], Step [58900/138038], Loss: 2.7991, Perplexity: 16.42911\n",
      "Epoch [1/3], Step [59000/138038], Loss: 3.2917, Perplexity: 26.8892\n",
      "Epoch [1/3], Step [59100/138038], Loss: 3.0778, Perplexity: 21.71025\n",
      "Epoch [1/3], Step [59200/138038], Loss: 2.8095, Perplexity: 16.60190\n",
      "Epoch [1/3], Step [59300/138038], Loss: 1.4783, Perplexity: 4.385678\n",
      "Epoch [1/3], Step [59400/138038], Loss: 3.5161, Perplexity: 33.6513\n",
      "Epoch [1/3], Step [59500/138038], Loss: 3.4378, Perplexity: 31.1171\n",
      "Epoch [1/3], Step [59600/138038], Loss: 2.8182, Perplexity: 16.74580\n",
      "Epoch [1/3], Step [59700/138038], Loss: 1.8268, Perplexity: 6.21427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [59800/138038], Loss: 3.2039, Perplexity: 24.62826\n",
      "Epoch [1/3], Step [59900/138038], Loss: 3.1835, Perplexity: 24.1308\n",
      "Epoch [1/3], Step [60000/138038], Loss: 3.7621, Perplexity: 43.03709\n",
      "Epoch [1/3], Step [60100/138038], Loss: 2.2384, Perplexity: 9.37847\n",
      "Epoch [1/3], Step [60200/138038], Loss: 3.4748, Perplexity: 32.29114\n",
      "Epoch [1/3], Step [60300/138038], Loss: 2.4585, Perplexity: 11.6874\n",
      "Epoch [1/3], Step [60400/138038], Loss: 2.9516, Perplexity: 19.13622\n",
      "Epoch [1/3], Step [60500/138038], Loss: 2.9734, Perplexity: 19.5580\n",
      "Epoch [1/3], Step [60600/138038], Loss: 4.2401, Perplexity: 69.41329\n",
      "Epoch [1/3], Step [60700/138038], Loss: 2.4597, Perplexity: 11.7018\n",
      "Epoch [1/3], Step [60800/138038], Loss: 2.0369, Perplexity: 7.66692\n",
      "Epoch [1/3], Step [60900/138038], Loss: 2.8112, Perplexity: 16.6297\n",
      "Epoch [1/3], Step [61000/138038], Loss: 2.7987, Perplexity: 16.42354\n",
      "Epoch [1/3], Step [61100/138038], Loss: 2.2821, Perplexity: 9.796909\n",
      "Epoch [1/3], Step [61200/138038], Loss: 2.6994, Perplexity: 14.8710\n",
      "Epoch [1/3], Step [61300/138038], Loss: 2.4925, Perplexity: 12.0918\n",
      "Epoch [1/3], Step [61400/138038], Loss: 3.0089, Perplexity: 20.2643\n",
      "Epoch [1/3], Step [61500/138038], Loss: 2.6212, Perplexity: 13.7524\n",
      "Epoch [1/3], Step [61600/138038], Loss: 1.7530, Perplexity: 5.77188\n",
      "Epoch [1/3], Step [61700/138038], Loss: 3.2921, Perplexity: 26.89842\n",
      "Epoch [1/3], Step [61800/138038], Loss: 2.3915, Perplexity: 10.9304\n",
      "Epoch [1/3], Step [61900/138038], Loss: 2.7564, Perplexity: 15.74317\n",
      "Epoch [1/3], Step [62000/138038], Loss: 4.1896, Perplexity: 65.9971\n",
      "Epoch [1/3], Step [62100/138038], Loss: 3.6380, Perplexity: 38.0158\n",
      "Epoch [1/3], Step [62200/138038], Loss: 3.4771, Perplexity: 32.36660\n",
      "Epoch [1/3], Step [62300/138038], Loss: 2.9940, Perplexity: 19.96478\n",
      "Epoch [1/3], Step [62400/138038], Loss: 3.6528, Perplexity: 38.5812\n",
      "Epoch [1/3], Step [62500/138038], Loss: 1.8230, Perplexity: 6.190772\n",
      "Epoch [1/3], Step [62600/138038], Loss: 2.2613, Perplexity: 9.59562\n",
      "Epoch [1/3], Step [62700/138038], Loss: 2.0754, Perplexity: 7.96791\n",
      "Epoch [1/3], Step [62800/138038], Loss: 2.4916, Perplexity: 12.08017\n",
      "Epoch [1/3], Step [62900/138038], Loss: 1.8648, Perplexity: 6.45487\n",
      "Epoch [1/3], Step [63000/138038], Loss: 2.4513, Perplexity: 11.6039\n",
      "Epoch [1/3], Step [63100/138038], Loss: 2.8191, Perplexity: 16.7610\n",
      "Epoch [1/3], Step [63200/138038], Loss: 3.2540, Perplexity: 25.89259\n",
      "Epoch [1/3], Step [63300/138038], Loss: 3.2418, Perplexity: 25.58091\n",
      "Epoch [1/3], Step [63400/138038], Loss: 4.0757, Perplexity: 58.8895\n",
      "Epoch [1/3], Step [63500/138038], Loss: 3.0254, Perplexity: 20.6014\n",
      "Epoch [1/3], Step [63600/138038], Loss: 2.7773, Perplexity: 16.0757\n",
      "Epoch [1/3], Step [63700/138038], Loss: 2.1268, Perplexity: 8.38807\n",
      "Epoch [1/3], Step [63800/138038], Loss: 2.4917, Perplexity: 12.0815\n",
      "Epoch [1/3], Step [63900/138038], Loss: 3.1459, Perplexity: 23.23983\n",
      "Epoch [1/3], Step [64000/138038], Loss: 2.6460, Perplexity: 14.0971\n",
      "Epoch [1/3], Step [64100/138038], Loss: 2.4472, Perplexity: 11.55582\n",
      "Epoch [1/3], Step [64200/138038], Loss: 2.7432, Perplexity: 15.53634\n",
      "Epoch [1/3], Step [64300/138038], Loss: 2.5288, Perplexity: 12.5388\n",
      "Epoch [1/3], Step [64400/138038], Loss: 4.8371, Perplexity: 126.1014\n",
      "Epoch [1/3], Step [64500/138038], Loss: 3.2353, Perplexity: 25.41498\n",
      "Epoch [1/3], Step [64600/138038], Loss: 2.4464, Perplexity: 11.5473\n",
      "Epoch [1/3], Step [64700/138038], Loss: 2.5021, Perplexity: 12.20806\n",
      "Epoch [1/3], Step [64800/138038], Loss: 3.4812, Perplexity: 32.5001\n",
      "Epoch [1/3], Step [64900/138038], Loss: 2.3541, Perplexity: 10.5292\n",
      "Epoch [1/3], Step [65000/138038], Loss: 2.4676, Perplexity: 11.7938\n",
      "Epoch [1/3], Step [65100/138038], Loss: 3.1241, Perplexity: 22.73899\n",
      "Epoch [1/3], Step [65200/138038], Loss: 2.5524, Perplexity: 12.83763\n",
      "Epoch [1/3], Step [65300/138038], Loss: 2.8620, Perplexity: 17.4971\n",
      "Epoch [1/3], Step [65400/138038], Loss: 2.6607, Perplexity: 14.30701\n",
      "Epoch [1/3], Step [65500/138038], Loss: 2.7494, Perplexity: 15.63317\n",
      "Epoch [1/3], Step [65600/138038], Loss: 2.8108, Perplexity: 16.6230\n",
      "Epoch [1/3], Step [65700/138038], Loss: 2.2309, Perplexity: 9.308400\n",
      "Epoch [1/3], Step [65800/138038], Loss: 2.6214, Perplexity: 13.7553\n",
      "Epoch [1/3], Step [65900/138038], Loss: 2.8030, Perplexity: 16.4933\n",
      "Epoch [1/3], Step [66000/138038], Loss: 2.4668, Perplexity: 11.78522\n",
      "Epoch [1/3], Step [66100/138038], Loss: 3.7145, Perplexity: 41.0377\n",
      "Epoch [1/3], Step [66200/138038], Loss: 4.3358, Perplexity: 76.38851\n",
      "Epoch [1/3], Step [66300/138038], Loss: 3.2221, Perplexity: 25.0805\n",
      "Epoch [1/3], Step [66400/138038], Loss: 2.7375, Perplexity: 15.44898\n",
      "Epoch [1/3], Step [66500/138038], Loss: 2.4578, Perplexity: 11.6786\n",
      "Epoch [1/3], Step [66600/138038], Loss: 3.9158, Perplexity: 50.18799\n",
      "Epoch [1/3], Step [66700/138038], Loss: 2.7004, Perplexity: 14.8859\n",
      "Epoch [1/3], Step [66800/138038], Loss: 3.3210, Perplexity: 27.6878\n",
      "Epoch [1/3], Step [66900/138038], Loss: 2.8233, Perplexity: 16.83198\n",
      "Epoch [1/3], Step [67000/138038], Loss: 3.7419, Perplexity: 42.1792\n",
      "Epoch [1/3], Step [67100/138038], Loss: 2.8815, Perplexity: 17.8403\n",
      "Epoch [1/3], Step [67200/138038], Loss: 3.0332, Perplexity: 20.7631\n",
      "Epoch [1/3], Step [67300/138038], Loss: 2.7164, Perplexity: 15.1258\n",
      "Epoch [1/3], Step [67400/138038], Loss: 2.2504, Perplexity: 9.491565\n",
      "Epoch [1/3], Step [67500/138038], Loss: 3.2220, Perplexity: 25.0771\n",
      "Epoch [1/3], Step [67600/138038], Loss: 2.4430, Perplexity: 11.50757\n",
      "Epoch [1/3], Step [67700/138038], Loss: 2.0933, Perplexity: 8.111515\n",
      "Epoch [1/3], Step [67800/138038], Loss: 2.1987, Perplexity: 9.012977\n",
      "Epoch [1/3], Step [67900/138038], Loss: 3.8556, Perplexity: 47.2561\n",
      "Epoch [1/3], Step [68000/138038], Loss: 3.3246, Perplexity: 27.7883\n",
      "Epoch [1/3], Step [68100/138038], Loss: 3.1008, Perplexity: 22.2162\n",
      "Epoch [1/3], Step [68200/138038], Loss: 3.8608, Perplexity: 47.5051\n",
      "Epoch [1/3], Step [68300/138038], Loss: 2.7888, Perplexity: 16.2609\n",
      "Epoch [1/3], Step [68400/138038], Loss: 2.3132, Perplexity: 10.10620\n",
      "Epoch [1/3], Step [68500/138038], Loss: 2.7689, Perplexity: 15.94147\n",
      "Epoch [1/3], Step [68600/138038], Loss: 2.5550, Perplexity: 12.8719\n",
      "Epoch [1/3], Step [68700/138038], Loss: 2.6651, Perplexity: 14.3695\n",
      "Epoch [1/3], Step [68800/138038], Loss: 2.5308, Perplexity: 12.56361\n",
      "Epoch [1/3], Step [68900/138038], Loss: 2.8317, Perplexity: 16.97395\n",
      "Epoch [1/3], Step [69000/138038], Loss: 2.4143, Perplexity: 11.18220\n",
      "Epoch [1/3], Step [69100/138038], Loss: 1.3754, Perplexity: 3.95667\n",
      "Epoch [1/3], Step [69200/138038], Loss: 2.7263, Perplexity: 15.2757\n",
      "Epoch [1/3], Step [69300/138038], Loss: 2.0806, Perplexity: 8.009044\n",
      "Epoch [1/3], Step [69400/138038], Loss: 2.2527, Perplexity: 9.513000\n",
      "Epoch [1/3], Step [69500/138038], Loss: 2.3845, Perplexity: 10.85374\n",
      "Epoch [1/3], Step [69600/138038], Loss: 2.3531, Perplexity: 10.51812\n",
      "Epoch [1/3], Step [69700/138038], Loss: 2.1762, Perplexity: 8.81286\n",
      "Epoch [1/3], Step [69800/138038], Loss: 2.4781, Perplexity: 11.9183\n",
      "Epoch [1/3], Step [69900/138038], Loss: 4.0287, Perplexity: 56.1854\n",
      "Epoch [1/3], Step [70000/138038], Loss: 2.1789, Perplexity: 8.836625\n",
      "Epoch [1/3], Step [70100/138038], Loss: 3.1000, Perplexity: 22.1990\n",
      "Epoch [1/3], Step [70200/138038], Loss: 3.0104, Perplexity: 20.2947\n",
      "Epoch [1/3], Step [70300/138038], Loss: 3.2710, Perplexity: 26.33747\n",
      "Epoch [1/3], Step [70400/138038], Loss: 4.2401, Perplexity: 69.4170\n",
      "Epoch [1/3], Step [70500/138038], Loss: 2.2469, Perplexity: 9.458134\n",
      "Epoch [1/3], Step [70600/138038], Loss: 2.2599, Perplexity: 9.58206\n",
      "Epoch [1/3], Step [70700/138038], Loss: 3.7863, Perplexity: 44.0915\n",
      "Epoch [1/3], Step [70800/138038], Loss: 2.4598, Perplexity: 11.70208\n",
      "Epoch [1/3], Step [70900/138038], Loss: 1.9637, Perplexity: 7.12568\n",
      "Epoch [1/3], Step [71000/138038], Loss: 2.2241, Perplexity: 9.24523\n",
      "Epoch [1/3], Step [71100/138038], Loss: 2.9735, Perplexity: 19.5606\n",
      "Epoch [1/3], Step [71200/138038], Loss: 2.9419, Perplexity: 18.9516\n",
      "Epoch [1/3], Step [71300/138038], Loss: 2.1794, Perplexity: 8.841074\n",
      "Epoch [1/3], Step [71400/138038], Loss: 2.4758, Perplexity: 11.8916\n",
      "Epoch [1/3], Step [71500/138038], Loss: 2.8518, Perplexity: 17.31841\n",
      "Epoch [1/3], Step [71600/138038], Loss: 3.1336, Perplexity: 22.95632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [71700/138038], Loss: 3.0512, Perplexity: 21.14105\n",
      "Epoch [1/3], Step [71800/138038], Loss: 2.5655, Perplexity: 13.0065\n",
      "Epoch [1/3], Step [71900/138038], Loss: 3.0512, Perplexity: 21.14111\n",
      "Epoch [1/3], Step [72000/138038], Loss: 1.8329, Perplexity: 6.25216\n",
      "Epoch [1/3], Step [72100/138038], Loss: 2.8314, Perplexity: 16.9689\n",
      "Epoch [1/3], Step [72200/138038], Loss: 1.7297, Perplexity: 5.63907\n",
      "Epoch [1/3], Step [72300/138038], Loss: 4.9277, Perplexity: 138.0590\n",
      "Epoch [1/3], Step [72400/138038], Loss: 3.3444, Perplexity: 28.34284\n",
      "Epoch [1/3], Step [72500/138038], Loss: 3.4984, Perplexity: 33.0630\n",
      "Epoch [1/3], Step [72600/138038], Loss: 2.6906, Perplexity: 14.74080\n",
      "Epoch [1/3], Step [72700/138038], Loss: 3.2189, Perplexity: 24.9999\n",
      "Epoch [1/3], Step [72800/138038], Loss: 2.7656, Perplexity: 15.88868\n",
      "Epoch [1/3], Step [72900/138038], Loss: 1.8735, Perplexity: 6.51130\n",
      "Epoch [1/3], Step [73000/138038], Loss: 2.6148, Perplexity: 13.6645\n",
      "Epoch [1/3], Step [73100/138038], Loss: 1.7306, Perplexity: 5.64420\n",
      "Epoch [1/3], Step [73200/138038], Loss: 4.3333, Perplexity: 76.1937\n",
      "Epoch [1/3], Step [73300/138038], Loss: 2.8376, Perplexity: 17.0755\n",
      "Epoch [1/3], Step [73400/138038], Loss: 2.5543, Perplexity: 12.86230\n",
      "Epoch [1/3], Step [73500/138038], Loss: 3.7928, Perplexity: 44.37980\n",
      "Epoch [1/3], Step [73600/138038], Loss: 3.7661, Perplexity: 43.21347\n",
      "Epoch [1/3], Step [73700/138038], Loss: 2.4979, Perplexity: 12.1566\n",
      "Epoch [1/3], Step [73800/138038], Loss: 2.7450, Perplexity: 15.5653\n",
      "Epoch [1/3], Step [73900/138038], Loss: 2.8745, Perplexity: 17.7171\n",
      "Epoch [1/3], Step [74000/138038], Loss: 3.0859, Perplexity: 21.88762\n",
      "Epoch [1/3], Step [74100/138038], Loss: 2.7717, Perplexity: 15.98506\n",
      "Epoch [1/3], Step [74200/138038], Loss: 2.5979, Perplexity: 13.43545\n",
      "Epoch [1/3], Step [74300/138038], Loss: 3.2723, Perplexity: 26.3710\n",
      "Epoch [1/3], Step [74400/138038], Loss: 1.7677, Perplexity: 5.85727\n",
      "Epoch [1/3], Step [74500/138038], Loss: 2.4429, Perplexity: 11.50675\n",
      "Epoch [1/3], Step [74600/138038], Loss: 1.9538, Perplexity: 7.05532\n",
      "Epoch [1/3], Step [74700/138038], Loss: 2.1790, Perplexity: 8.83745\n",
      "Epoch [1/3], Step [74800/138038], Loss: 3.2191, Perplexity: 25.00657\n",
      "Epoch [1/3], Step [74900/138038], Loss: 2.8409, Perplexity: 17.1310\n",
      "Epoch [1/3], Step [75000/138038], Loss: 2.8062, Perplexity: 16.5472\n",
      "Epoch [1/3], Step [75100/138038], Loss: 1.8823, Perplexity: 6.56883\n",
      "Epoch [1/3], Step [75200/138038], Loss: 2.2981, Perplexity: 9.955001\n",
      "Epoch [1/3], Step [75300/138038], Loss: 2.7853, Perplexity: 16.20489\n",
      "Epoch [1/3], Step [75400/138038], Loss: 2.3660, Perplexity: 10.65513\n",
      "Epoch [1/3], Step [75500/138038], Loss: 3.0878, Perplexity: 21.9292\n",
      "Epoch [1/3], Step [75600/138038], Loss: 1.9011, Perplexity: 6.69304\n",
      "Epoch [1/3], Step [75700/138038], Loss: 2.2601, Perplexity: 9.58440\n",
      "Epoch [1/3], Step [75800/138038], Loss: 2.1790, Perplexity: 8.83762\n",
      "Epoch [1/3], Step [75900/138038], Loss: 3.2759, Perplexity: 26.46794\n",
      "Epoch [1/3], Step [76000/138038], Loss: 2.7120, Perplexity: 15.05895\n",
      "Epoch [1/3], Step [76100/138038], Loss: 2.0115, Perplexity: 7.474290\n",
      "Epoch [1/3], Step [76200/138038], Loss: 3.6363, Perplexity: 37.94999\n",
      "Epoch [1/3], Step [76300/138038], Loss: 1.8350, Perplexity: 6.26524\n",
      "Epoch [1/3], Step [76400/138038], Loss: 2.9112, Perplexity: 18.3781\n",
      "Epoch [1/3], Step [76500/138038], Loss: 3.4217, Perplexity: 30.62049\n",
      "Epoch [1/3], Step [76600/138038], Loss: 2.7286, Perplexity: 15.31189\n",
      "Epoch [1/3], Step [76700/138038], Loss: 2.8309, Perplexity: 16.9604\n",
      "Epoch [1/3], Step [76800/138038], Loss: 2.6111, Perplexity: 13.6136\n",
      "Epoch [1/3], Step [76900/138038], Loss: 2.8050, Perplexity: 16.5271\n",
      "Epoch [1/3], Step [77000/138038], Loss: 3.0400, Perplexity: 20.9053\n",
      "Epoch [1/3], Step [77100/138038], Loss: 2.8995, Perplexity: 18.16605\n",
      "Epoch [1/3], Step [77200/138038], Loss: 3.1260, Perplexity: 22.7819\n",
      "Epoch [1/3], Step [77300/138038], Loss: 3.1529, Perplexity: 23.4037\n",
      "Epoch [1/3], Step [77400/138038], Loss: 2.1511, Perplexity: 8.59420\n",
      "Epoch [1/3], Step [77500/138038], Loss: 2.6588, Perplexity: 14.27900\n",
      "Epoch [1/3], Step [77600/138038], Loss: 2.2794, Perplexity: 9.77123\n",
      "Epoch [1/3], Step [77700/138038], Loss: 3.2631, Perplexity: 26.1315\n",
      "Epoch [1/3], Step [77800/138038], Loss: 2.5415, Perplexity: 12.6984\n",
      "Epoch [1/3], Step [77900/138038], Loss: 2.7394, Perplexity: 15.4781\n",
      "Epoch [1/3], Step [78000/138038], Loss: 1.8210, Perplexity: 6.17829\n",
      "Epoch [1/3], Step [78100/138038], Loss: 3.2361, Perplexity: 25.4336\n",
      "Epoch [1/3], Step [78200/138038], Loss: 3.3327, Perplexity: 28.01277\n",
      "Epoch [1/3], Step [78300/138038], Loss: 1.6399, Perplexity: 5.154588\n",
      "Epoch [1/3], Step [78400/138038], Loss: 2.6206, Perplexity: 13.7435\n",
      "Epoch [1/3], Step [78500/138038], Loss: 4.1003, Perplexity: 60.35746\n",
      "Epoch [1/3], Step [78600/138038], Loss: 2.9089, Perplexity: 18.33697\n",
      "Epoch [1/3], Step [78700/138038], Loss: 2.4062, Perplexity: 11.09172\n",
      "Epoch [1/3], Step [78800/138038], Loss: 2.1932, Perplexity: 8.96381\n",
      "Epoch [1/3], Step [78900/138038], Loss: 3.9698, Perplexity: 52.97626\n",
      "Epoch [1/3], Step [79000/138038], Loss: 2.1790, Perplexity: 8.837417\n",
      "Epoch [1/3], Step [79100/138038], Loss: 3.3409, Perplexity: 28.2450\n",
      "Epoch [1/3], Step [79200/138038], Loss: 2.2488, Perplexity: 9.476454\n",
      "Epoch [1/3], Step [79300/138038], Loss: 3.0197, Perplexity: 20.4851\n",
      "Epoch [1/3], Step [79400/138038], Loss: 3.7438, Perplexity: 42.2587\n",
      "Epoch [1/3], Step [79500/138038], Loss: 3.6565, Perplexity: 38.7245\n",
      "Epoch [1/3], Step [79600/138038], Loss: 2.2427, Perplexity: 9.41894\n",
      "Epoch [1/3], Step [79700/138038], Loss: 3.6510, Perplexity: 38.5135\n",
      "Epoch [1/3], Step [79800/138038], Loss: 1.9069, Perplexity: 6.73213\n",
      "Epoch [1/3], Step [79900/138038], Loss: 3.0397, Perplexity: 20.89970\n",
      "Epoch [1/3], Step [80000/138038], Loss: 3.1153, Perplexity: 22.5411\n",
      "Epoch [1/3], Step [80100/138038], Loss: 2.0084, Perplexity: 7.451289\n",
      "Epoch [1/3], Step [80200/138038], Loss: 3.6510, Perplexity: 38.51185\n",
      "Epoch [1/3], Step [80300/138038], Loss: 1.7487, Perplexity: 5.74725\n",
      "Epoch [1/3], Step [80400/138038], Loss: 3.1433, Perplexity: 23.1809\n",
      "Epoch [1/3], Step [80500/138038], Loss: 3.0262, Perplexity: 20.61950\n",
      "Epoch [1/3], Step [80600/138038], Loss: 2.3296, Perplexity: 10.27360\n",
      "Epoch [1/3], Step [80700/138038], Loss: 2.4376, Perplexity: 11.44512\n",
      "Epoch [1/3], Step [80800/138038], Loss: 3.6684, Perplexity: 39.1888\n",
      "Epoch [1/3], Step [80900/138038], Loss: 2.3948, Perplexity: 10.9662\n",
      "Epoch [1/3], Step [81000/138038], Loss: 2.8229, Perplexity: 16.8262\n",
      "Epoch [1/3], Step [81100/138038], Loss: 3.9595, Perplexity: 52.4320\n",
      "Epoch [1/3], Step [81200/138038], Loss: 2.4491, Perplexity: 11.5781\n",
      "Epoch [1/3], Step [81300/138038], Loss: 2.6593, Perplexity: 14.2857\n",
      "Epoch [1/3], Step [81400/138038], Loss: 2.4638, Perplexity: 11.74918\n",
      "Epoch [1/3], Step [81500/138038], Loss: 3.8587, Perplexity: 47.40523\n",
      "Epoch [1/3], Step [81600/138038], Loss: 2.5000, Perplexity: 12.1823\n",
      "Epoch [1/3], Step [81700/138038], Loss: 4.6531, Perplexity: 104.9095\n",
      "Epoch [1/3], Step [81800/138038], Loss: 3.1018, Perplexity: 22.2380\n",
      "Epoch [1/3], Step [81900/138038], Loss: 2.6146, Perplexity: 13.6617\n",
      "Epoch [1/3], Step [82000/138038], Loss: 2.6373, Perplexity: 13.9757\n",
      "Epoch [1/3], Step [82100/138038], Loss: 2.7534, Perplexity: 15.69554\n",
      "Epoch [1/3], Step [82200/138038], Loss: 2.9224, Perplexity: 18.5860\n",
      "Epoch [1/3], Step [82300/138038], Loss: 2.8343, Perplexity: 17.0183\n",
      "Epoch [1/3], Step [82400/138038], Loss: 3.6643, Perplexity: 39.02812\n",
      "Epoch [1/3], Step [82500/138038], Loss: 2.7740, Perplexity: 16.02333\n",
      "Epoch [1/3], Step [82600/138038], Loss: 2.8827, Perplexity: 17.8627\n",
      "Epoch [1/3], Step [82700/138038], Loss: 2.3272, Perplexity: 10.24941\n",
      "Epoch [1/3], Step [82800/138038], Loss: 1.7663, Perplexity: 5.849412\n",
      "Epoch [1/3], Step [82900/138038], Loss: 1.7536, Perplexity: 5.775676\n",
      "Epoch [1/3], Step [83000/138038], Loss: 2.9971, Perplexity: 20.0269\n",
      "Epoch [1/3], Step [83100/138038], Loss: 2.6400, Perplexity: 14.0127\n",
      "Epoch [1/3], Step [83200/138038], Loss: 2.2567, Perplexity: 9.55176\n",
      "Epoch [1/3], Step [83300/138038], Loss: 2.9391, Perplexity: 18.89916\n",
      "Epoch [1/3], Step [83400/138038], Loss: 3.3385, Perplexity: 28.17678\n",
      "Epoch [1/3], Step [83500/138038], Loss: 2.9405, Perplexity: 18.92449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [83600/138038], Loss: 2.7917, Perplexity: 16.3090\n",
      "Epoch [1/3], Step [83700/138038], Loss: 2.4942, Perplexity: 12.11215\n",
      "Epoch [1/3], Step [83800/138038], Loss: 3.3196, Perplexity: 27.6495\n",
      "Epoch [1/3], Step [83900/138038], Loss: 2.2727, Perplexity: 9.70569\n",
      "Epoch [1/3], Step [84000/138038], Loss: 2.4913, Perplexity: 12.0770\n",
      "Epoch [1/3], Step [84100/138038], Loss: 2.4564, Perplexity: 11.66243\n",
      "Epoch [1/3], Step [84200/138038], Loss: 3.1149, Perplexity: 22.5304\n",
      "Epoch [1/3], Step [84300/138038], Loss: 2.3465, Perplexity: 10.4488\n",
      "Epoch [1/3], Step [84400/138038], Loss: 2.3136, Perplexity: 10.1113\n",
      "Epoch [1/3], Step [84500/138038], Loss: 2.3524, Perplexity: 10.5106\n",
      "Epoch [1/3], Step [84600/138038], Loss: 2.5660, Perplexity: 13.0139\n",
      "Epoch [1/3], Step [84700/138038], Loss: 3.1738, Perplexity: 23.8993\n",
      "Epoch [1/3], Step [84800/138038], Loss: 2.6870, Perplexity: 14.6873\n",
      "Epoch [1/3], Step [84900/138038], Loss: 2.6521, Perplexity: 14.18338\n",
      "Epoch [1/3], Step [85000/138038], Loss: 2.0542, Perplexity: 7.80037\n",
      "Epoch [1/3], Step [85100/138038], Loss: 3.3114, Perplexity: 27.42453\n",
      "Epoch [1/3], Step [85200/138038], Loss: 3.1245, Perplexity: 22.74966\n",
      "Epoch [1/3], Step [85300/138038], Loss: 2.0333, Perplexity: 7.638935\n",
      "Epoch [1/3], Step [85400/138038], Loss: 4.8238, Perplexity: 124.4429\n",
      "Epoch [1/3], Step [85500/138038], Loss: 3.1225, Perplexity: 22.7030\n",
      "Epoch [1/3], Step [85600/138038], Loss: 2.5839, Perplexity: 13.2489\n",
      "Epoch [1/3], Step [85700/138038], Loss: 3.2124, Perplexity: 24.83836\n",
      "Epoch [1/3], Step [85800/138038], Loss: 2.5423, Perplexity: 12.7088\n",
      "Epoch [1/3], Step [85900/138038], Loss: 2.9802, Perplexity: 19.6915\n",
      "Epoch [1/3], Step [86000/138038], Loss: 2.8834, Perplexity: 17.87488\n",
      "Epoch [1/3], Step [86100/138038], Loss: 2.8084, Perplexity: 16.58287\n",
      "Epoch [1/3], Step [86200/138038], Loss: 1.5485, Perplexity: 4.704394\n",
      "Epoch [1/3], Step [86300/138038], Loss: 2.2323, Perplexity: 9.321146\n",
      "Epoch [1/3], Step [86400/138038], Loss: 2.7505, Perplexity: 15.6510\n",
      "Epoch [1/3], Step [86500/138038], Loss: 2.8129, Perplexity: 16.65786\n",
      "Epoch [1/3], Step [86600/138038], Loss: 2.9445, Perplexity: 19.0016\n",
      "Epoch [1/3], Step [86700/138038], Loss: 4.2169, Perplexity: 67.81977\n",
      "Epoch [1/3], Step [86800/138038], Loss: 2.5041, Perplexity: 12.23209\n",
      "Epoch [1/3], Step [86900/138038], Loss: 2.0799, Perplexity: 8.003498\n",
      "Epoch [1/3], Step [87000/138038], Loss: 1.7663, Perplexity: 5.849024\n",
      "Epoch [1/3], Step [87100/138038], Loss: 3.0160, Perplexity: 20.41047\n",
      "Epoch [1/3], Step [87200/138038], Loss: 1.7271, Perplexity: 5.62437\n",
      "Epoch [1/3], Step [87300/138038], Loss: 2.5390, Perplexity: 12.6665\n",
      "Epoch [1/3], Step [87400/138038], Loss: 2.8284, Perplexity: 16.91830\n",
      "Epoch [1/3], Step [87500/138038], Loss: 3.0521, Perplexity: 21.1599\n",
      "Epoch [1/3], Step [87600/138038], Loss: 2.4411, Perplexity: 11.4856\n",
      "Epoch [1/3], Step [87700/138038], Loss: 2.9383, Perplexity: 18.88442\n",
      "Epoch [1/3], Step [87800/138038], Loss: 2.5963, Perplexity: 13.4142\n",
      "Epoch [1/3], Step [87900/138038], Loss: 2.6862, Perplexity: 14.67568\n",
      "Epoch [1/3], Step [88000/138038], Loss: 2.2028, Perplexity: 9.05044\n",
      "Epoch [1/3], Step [88100/138038], Loss: 3.3805, Perplexity: 29.3843\n",
      "Epoch [1/3], Step [88200/138038], Loss: 2.0716, Perplexity: 7.937112\n",
      "Epoch [1/3], Step [88300/138038], Loss: 2.7220, Perplexity: 15.2112\n",
      "Epoch [1/3], Step [88400/138038], Loss: 2.1618, Perplexity: 8.686662\n",
      "Epoch [1/3], Step [88500/138038], Loss: 2.2272, Perplexity: 9.27372\n",
      "Epoch [1/3], Step [88600/138038], Loss: 2.6809, Perplexity: 14.59824\n",
      "Epoch [1/3], Step [88700/138038], Loss: 3.9356, Perplexity: 51.1928\n",
      "Epoch [1/3], Step [88800/138038], Loss: 2.2768, Perplexity: 9.745094\n",
      "Epoch [1/3], Step [88900/138038], Loss: 2.5701, Perplexity: 13.0669\n",
      "Epoch [1/3], Step [89000/138038], Loss: 3.0465, Perplexity: 21.04207\n",
      "Epoch [1/3], Step [89100/138038], Loss: 1.9410, Perplexity: 6.96551\n",
      "Epoch [1/3], Step [89200/138038], Loss: 2.8539, Perplexity: 17.35513\n",
      "Epoch [1/3], Step [89300/138038], Loss: 2.3908, Perplexity: 10.9223\n",
      "Epoch [1/3], Step [89400/138038], Loss: 2.7965, Perplexity: 16.3868\n",
      "Epoch [1/3], Step [89500/138038], Loss: 3.2715, Perplexity: 26.3510\n",
      "Epoch [1/3], Step [89600/138038], Loss: 2.1822, Perplexity: 8.866132\n",
      "Epoch [1/3], Step [89700/138038], Loss: 2.1969, Perplexity: 8.99692\n",
      "Epoch [1/3], Step [89800/138038], Loss: 2.2838, Perplexity: 9.814164\n",
      "Epoch [1/3], Step [89900/138038], Loss: 2.7424, Perplexity: 15.5242\n",
      "Epoch [1/3], Step [90000/138038], Loss: 3.3981, Perplexity: 29.9073\n",
      "Epoch [1/3], Step [90100/138038], Loss: 3.1470, Perplexity: 23.26693\n",
      "Epoch [1/3], Step [90200/138038], Loss: 2.6496, Perplexity: 14.1482\n",
      "Epoch [1/3], Step [90300/138038], Loss: 3.3175, Perplexity: 27.5920\n",
      "Epoch [1/3], Step [90400/138038], Loss: 3.4554, Perplexity: 31.6719\n",
      "Epoch [1/3], Step [90500/138038], Loss: 2.6152, Perplexity: 13.67011\n",
      "Epoch [1/3], Step [90600/138038], Loss: 2.3072, Perplexity: 10.0467\n",
      "Epoch [1/3], Step [90700/138038], Loss: 2.1015, Perplexity: 8.178830\n",
      "Epoch [1/3], Step [90800/138038], Loss: 2.4071, Perplexity: 11.10157\n",
      "Epoch [1/3], Step [90900/138038], Loss: 2.5564, Perplexity: 12.8897\n",
      "Epoch [1/3], Step [91000/138038], Loss: 2.2289, Perplexity: 9.289554\n",
      "Epoch [1/3], Step [91100/138038], Loss: 3.6794, Perplexity: 39.6222\n",
      "Epoch [1/3], Step [91200/138038], Loss: 2.6930, Perplexity: 14.7762\n",
      "Epoch [1/3], Step [91300/138038], Loss: 1.5038, Perplexity: 4.498876\n",
      "Epoch [1/3], Step [91400/138038], Loss: 3.5601, Perplexity: 35.16731\n",
      "Epoch [1/3], Step [91500/138038], Loss: 3.5210, Perplexity: 33.8192\n",
      "Epoch [1/3], Step [91600/138038], Loss: 2.2252, Perplexity: 9.25510\n",
      "Epoch [1/3], Step [91700/138038], Loss: 3.3556, Perplexity: 28.6622\n",
      "Epoch [1/3], Step [91800/138038], Loss: 2.2171, Perplexity: 9.18088\n",
      "Epoch [1/3], Step [91900/138038], Loss: 3.2689, Perplexity: 26.28257\n",
      "Epoch [1/3], Step [92000/138038], Loss: 2.8500, Perplexity: 17.2879\n",
      "Epoch [1/3], Step [92100/138038], Loss: 2.1959, Perplexity: 8.98791\n",
      "Epoch [1/3], Step [92200/138038], Loss: 2.4908, Perplexity: 12.07071\n",
      "Epoch [1/3], Step [92300/138038], Loss: 4.0587, Perplexity: 57.8966\n",
      "Epoch [1/3], Step [92400/138038], Loss: 2.2184, Perplexity: 9.19301\n",
      "Epoch [1/3], Step [92500/138038], Loss: 2.2736, Perplexity: 9.71460\n",
      "Epoch [1/3], Step [92600/138038], Loss: 3.2419, Perplexity: 25.58182\n",
      "Epoch [1/3], Step [92700/138038], Loss: 3.7967, Perplexity: 44.5532\n",
      "Epoch [1/3], Step [92800/138038], Loss: 2.5567, Perplexity: 12.8937\n",
      "Epoch [1/3], Step [92900/138038], Loss: 3.3489, Perplexity: 28.4718\n",
      "Epoch [1/3], Step [93000/138038], Loss: 3.6999, Perplexity: 40.4415\n",
      "Epoch [1/3], Step [93100/138038], Loss: 3.4347, Perplexity: 31.0234\n",
      "Epoch [1/3], Step [93200/138038], Loss: 2.2069, Perplexity: 9.08772\n",
      "Epoch [1/3], Step [93300/138038], Loss: 2.3154, Perplexity: 10.12937\n",
      "Epoch [1/3], Step [93400/138038], Loss: 2.1760, Perplexity: 8.81134\n",
      "Epoch [1/3], Step [93500/138038], Loss: 4.6397, Perplexity: 103.5166\n",
      "Epoch [1/3], Step [93600/138038], Loss: 2.1530, Perplexity: 8.61046\n",
      "Epoch [1/3], Step [93700/138038], Loss: 2.3640, Perplexity: 10.63297\n",
      "Epoch [1/3], Step [93800/138038], Loss: 2.5701, Perplexity: 13.0674\n",
      "Epoch [1/3], Step [93900/138038], Loss: 2.6436, Perplexity: 14.0631\n",
      "Epoch [1/3], Step [94000/138038], Loss: 2.9828, Perplexity: 19.7423\n",
      "Epoch [1/3], Step [94100/138038], Loss: 2.6610, Perplexity: 14.3099\n",
      "Epoch [1/3], Step [94200/138038], Loss: 2.4412, Perplexity: 11.48681\n",
      "Epoch [1/3], Step [94300/138038], Loss: 3.3955, Perplexity: 29.8303\n",
      "Epoch [1/3], Step [94400/138038], Loss: 3.0561, Perplexity: 21.24554\n",
      "Epoch [1/3], Step [94500/138038], Loss: 2.6175, Perplexity: 13.70122\n",
      "Epoch [1/3], Step [94600/138038], Loss: 2.7884, Perplexity: 16.25459\n",
      "Epoch [1/3], Step [94700/138038], Loss: 1.9373, Perplexity: 6.94029\n",
      "Epoch [1/3], Step [94800/138038], Loss: 3.6680, Perplexity: 39.17385\n",
      "Epoch [1/3], Step [94900/138038], Loss: 2.5354, Perplexity: 12.6216\n",
      "Epoch [1/3], Step [95000/138038], Loss: 1.7704, Perplexity: 5.87335\n",
      "Epoch [1/3], Step [95100/138038], Loss: 2.8599, Perplexity: 17.45936\n",
      "Epoch [1/3], Step [95200/138038], Loss: 3.1063, Perplexity: 22.33784\n",
      "Epoch [1/3], Step [95300/138038], Loss: 2.5569, Perplexity: 12.8952\n",
      "Epoch [1/3], Step [95400/138038], Loss: 2.2847, Perplexity: 9.823139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [95500/138038], Loss: 2.7259, Perplexity: 15.2699\n",
      "Epoch [1/3], Step [95600/138038], Loss: 3.3018, Perplexity: 27.16115\n",
      "Epoch [1/3], Step [95700/138038], Loss: 2.2180, Perplexity: 9.18872\n",
      "Epoch [1/3], Step [95800/138038], Loss: 2.4779, Perplexity: 11.9159\n",
      "Epoch [1/3], Step [95900/138038], Loss: 3.4462, Perplexity: 31.38175\n",
      "Epoch [1/3], Step [96000/138038], Loss: 2.8026, Perplexity: 16.4879\n",
      "Epoch [1/3], Step [96100/138038], Loss: 2.4503, Perplexity: 11.5919\n",
      "Epoch [1/3], Step [96200/138038], Loss: 2.2113, Perplexity: 9.12804\n",
      "Epoch [1/3], Step [96300/138038], Loss: 2.3321, Perplexity: 10.29967\n",
      "Epoch [1/3], Step [96400/138038], Loss: 3.3091, Perplexity: 27.36150\n",
      "Epoch [1/3], Step [96500/138038], Loss: 2.5188, Perplexity: 12.4141\n",
      "Epoch [1/3], Step [96600/138038], Loss: 3.4956, Perplexity: 32.9687\n",
      "Epoch [1/3], Step [96700/138038], Loss: 2.8276, Perplexity: 16.9049\n",
      "Epoch [1/3], Step [96800/138038], Loss: 2.1412, Perplexity: 8.50938\n",
      "Epoch [1/3], Step [96900/138038], Loss: 2.1817, Perplexity: 8.86102\n",
      "Epoch [1/3], Step [97000/138038], Loss: 3.3815, Perplexity: 29.41500\n",
      "Epoch [1/3], Step [97100/138038], Loss: 2.2871, Perplexity: 9.84665\n",
      "Epoch [1/3], Step [97200/138038], Loss: 2.6062, Perplexity: 13.54754\n",
      "Epoch [1/3], Step [97300/138038], Loss: 2.4855, Perplexity: 12.0077\n",
      "Epoch [1/3], Step [97400/138038], Loss: 2.5413, Perplexity: 12.69626\n",
      "Epoch [1/3], Step [97500/138038], Loss: 2.1723, Perplexity: 8.77860\n",
      "Epoch [1/3], Step [97600/138038], Loss: 1.9932, Perplexity: 7.339369\n",
      "Epoch [1/3], Step [97700/138038], Loss: 2.9875, Perplexity: 19.83519\n",
      "Epoch [1/3], Step [97800/138038], Loss: 2.4861, Perplexity: 12.0146\n",
      "Epoch [1/3], Step [97900/138038], Loss: 2.4596, Perplexity: 11.7001\n",
      "Epoch [1/3], Step [98000/138038], Loss: 2.5405, Perplexity: 12.6860\n",
      "Epoch [1/3], Step [98100/138038], Loss: 3.2585, Perplexity: 26.00970\n",
      "Epoch [1/3], Step [98200/138038], Loss: 2.5204, Perplexity: 12.4339\n",
      "Epoch [1/3], Step [98300/138038], Loss: 3.1907, Perplexity: 24.30645\n",
      "Epoch [1/3], Step [98400/138038], Loss: 3.1200, Perplexity: 22.6454\n",
      "Epoch [1/3], Step [98500/138038], Loss: 2.2219, Perplexity: 9.225101\n",
      "Epoch [1/3], Step [98600/138038], Loss: 1.5423, Perplexity: 4.675233\n",
      "Epoch [1/3], Step [98700/138038], Loss: 2.8277, Perplexity: 16.9062\n",
      "Epoch [1/3], Step [98800/138038], Loss: 2.4999, Perplexity: 12.18076\n",
      "Epoch [1/3], Step [98900/138038], Loss: 3.3352, Perplexity: 28.0844\n",
      "Epoch [1/3], Step [99000/138038], Loss: 2.2878, Perplexity: 9.85350\n",
      "Epoch [1/3], Step [99100/138038], Loss: 1.9639, Perplexity: 7.127366\n",
      "Epoch [1/3], Step [99200/138038], Loss: 2.1070, Perplexity: 8.22330\n",
      "Epoch [1/3], Step [99300/138038], Loss: 2.1763, Perplexity: 8.81364\n",
      "Epoch [1/3], Step [99400/138038], Loss: 2.5699, Perplexity: 13.06434\n",
      "Epoch [1/3], Step [99500/138038], Loss: 2.4665, Perplexity: 11.7811\n",
      "Epoch [1/3], Step [99600/138038], Loss: 3.1707, Perplexity: 23.82423\n",
      "Epoch [1/3], Step [99700/138038], Loss: 3.5911, Perplexity: 36.2749\n",
      "Epoch [1/3], Step [99800/138038], Loss: 2.8594, Perplexity: 17.4519\n",
      "Epoch [1/3], Step [99900/138038], Loss: 2.7925, Perplexity: 16.3226\n",
      "Epoch [1/3], Step [100000/138038], Loss: 2.3558, Perplexity: 10.5461\n",
      "Epoch [1/3], Step [100100/138038], Loss: 2.3564, Perplexity: 10.55320\n",
      "Epoch [1/3], Step [100200/138038], Loss: 2.5898, Perplexity: 13.3275\n",
      "Epoch [1/3], Step [100300/138038], Loss: 2.1103, Perplexity: 8.25089\n",
      "Epoch [1/3], Step [100400/138038], Loss: 2.0717, Perplexity: 7.938442\n",
      "Epoch [1/3], Step [100500/138038], Loss: 3.1882, Perplexity: 24.2444\n",
      "Epoch [1/3], Step [100600/138038], Loss: 1.8448, Perplexity: 6.327081\n",
      "Epoch [1/3], Step [100700/138038], Loss: 1.8778, Perplexity: 6.539092\n",
      "Epoch [1/3], Step [100800/138038], Loss: 2.7086, Perplexity: 15.0089\n",
      "Epoch [1/3], Step [100900/138038], Loss: 2.9330, Perplexity: 18.7844\n",
      "Epoch [1/3], Step [101000/138038], Loss: 3.7118, Perplexity: 40.9281\n",
      "Epoch [1/3], Step [101100/138038], Loss: 1.7745, Perplexity: 5.89734\n",
      "Epoch [1/3], Step [101200/138038], Loss: 2.5233, Perplexity: 12.4691\n",
      "Epoch [1/3], Step [101300/138038], Loss: 1.8398, Perplexity: 6.29515\n",
      "Epoch [1/3], Step [101400/138038], Loss: 2.9972, Perplexity: 20.0285\n",
      "Epoch [1/3], Step [101500/138038], Loss: 1.9170, Perplexity: 6.80078\n",
      "Epoch [1/3], Step [101600/138038], Loss: 2.1826, Perplexity: 8.868993\n",
      "Epoch [1/3], Step [101700/138038], Loss: 6.2267, Perplexity: 506.1043\n",
      "Epoch [1/3], Step [101800/138038], Loss: 2.0645, Perplexity: 7.881774\n",
      "Epoch [1/3], Step [101900/138038], Loss: 2.7844, Perplexity: 16.1895\n",
      "Epoch [1/3], Step [102000/138038], Loss: 2.6922, Perplexity: 14.7635\n",
      "Epoch [1/3], Step [102100/138038], Loss: 3.3363, Perplexity: 28.1161\n",
      "Epoch [1/3], Step [102200/138038], Loss: 1.9084, Perplexity: 6.74213\n",
      "Epoch [1/3], Step [102300/138038], Loss: 1.7239, Perplexity: 5.60634\n",
      "Epoch [1/3], Step [102400/138038], Loss: 3.4467, Perplexity: 31.39534\n",
      "Epoch [1/3], Step [102500/138038], Loss: 3.5377, Perplexity: 34.3880\n",
      "Epoch [1/3], Step [102600/138038], Loss: 4.1394, Perplexity: 62.7634\n",
      "Epoch [1/3], Step [102700/138038], Loss: 3.5255, Perplexity: 33.9716\n",
      "Epoch [1/3], Step [102800/138038], Loss: 2.6039, Perplexity: 13.51678\n",
      "Epoch [1/3], Step [102900/138038], Loss: 3.4980, Perplexity: 33.04854\n",
      "Epoch [1/3], Step [103000/138038], Loss: 2.6926, Perplexity: 14.77061\n",
      "Epoch [1/3], Step [103100/138038], Loss: 3.6179, Perplexity: 37.2593\n",
      "Epoch [1/3], Step [103200/138038], Loss: 2.9638, Perplexity: 19.3723\n",
      "Epoch [1/3], Step [103300/138038], Loss: 2.4076, Perplexity: 11.10719\n",
      "Epoch [1/3], Step [103400/138038], Loss: 2.9666, Perplexity: 19.42549\n",
      "Epoch [1/3], Step [103500/138038], Loss: 2.9323, Perplexity: 18.7708\n",
      "Epoch [1/3], Step [103600/138038], Loss: 2.0321, Perplexity: 7.630090\n",
      "Epoch [1/3], Step [103700/138038], Loss: 2.7351, Perplexity: 15.41074\n",
      "Epoch [1/3], Step [103800/138038], Loss: 2.5797, Perplexity: 13.19366\n",
      "Epoch [1/3], Step [103900/138038], Loss: 3.4453, Perplexity: 31.3540\n",
      "Epoch [1/3], Step [104000/138038], Loss: 2.1835, Perplexity: 8.87707\n",
      "Epoch [1/3], Step [104100/138038], Loss: 2.1645, Perplexity: 8.70989\n",
      "Epoch [1/3], Step [104200/138038], Loss: 3.2140, Perplexity: 24.8792\n",
      "Epoch [1/3], Step [104300/138038], Loss: 3.1062, Perplexity: 22.3357\n",
      "Epoch [1/3], Step [104400/138038], Loss: 2.4626, Perplexity: 11.7350\n",
      "Epoch [1/3], Step [104500/138038], Loss: 3.3759, Perplexity: 29.2519\n",
      "Epoch [1/3], Step [104600/138038], Loss: 3.1915, Perplexity: 24.32476\n",
      "Epoch [1/3], Step [104700/138038], Loss: 3.2493, Perplexity: 25.7725\n",
      "Epoch [1/3], Step [104800/138038], Loss: 3.3428, Perplexity: 28.29813\n",
      "Epoch [1/3], Step [104900/138038], Loss: 2.8918, Perplexity: 18.02609\n",
      "Epoch [1/3], Step [105000/138038], Loss: 2.4981, Perplexity: 12.1591\n",
      "Epoch [1/3], Step [105100/138038], Loss: 4.2205, Perplexity: 68.0701\n",
      "Epoch [1/3], Step [105200/138038], Loss: 3.1114, Perplexity: 22.45226\n",
      "Epoch [1/3], Step [105300/138038], Loss: 3.4513, Perplexity: 31.5426\n",
      "Epoch [1/3], Step [105400/138038], Loss: 2.8842, Perplexity: 17.8897\n",
      "Epoch [1/3], Step [105500/138038], Loss: 1.7596, Perplexity: 5.809813\n",
      "Epoch [1/3], Step [105600/138038], Loss: 2.1322, Perplexity: 8.43351\n",
      "Epoch [1/3], Step [105700/138038], Loss: 2.3053, Perplexity: 10.0269\n",
      "Epoch [1/3], Step [105800/138038], Loss: 1.9246, Perplexity: 6.85240\n",
      "Epoch [1/3], Step [105900/138038], Loss: 2.7286, Perplexity: 15.3109\n",
      "Epoch [1/3], Step [106000/138038], Loss: 2.1836, Perplexity: 8.877856\n",
      "Epoch [1/3], Step [106100/138038], Loss: 2.9208, Perplexity: 18.55534\n",
      "Epoch [1/3], Step [106200/138038], Loss: 2.2423, Perplexity: 9.415125\n",
      "Epoch [1/3], Step [106300/138038], Loss: 2.7971, Perplexity: 16.3977\n",
      "Epoch [1/3], Step [106400/138038], Loss: 4.5632, Perplexity: 95.8918\n",
      "Epoch [1/3], Step [106500/138038], Loss: 2.6615, Perplexity: 14.3176\n",
      "Epoch [1/3], Step [106600/138038], Loss: 3.0605, Perplexity: 21.33875\n",
      "Epoch [1/3], Step [106700/138038], Loss: 2.1906, Perplexity: 8.94083\n",
      "Epoch [1/3], Step [106800/138038], Loss: 2.1570, Perplexity: 8.645462\n",
      "Epoch [1/3], Step [106900/138038], Loss: 3.0324, Perplexity: 20.7463\n",
      "Epoch [1/3], Step [107000/138038], Loss: 2.3599, Perplexity: 10.5900\n",
      "Epoch [1/3], Step [107100/138038], Loss: 2.1238, Perplexity: 8.36289\n",
      "Epoch [1/3], Step [107200/138038], Loss: 1.9956, Perplexity: 7.356864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [107300/138038], Loss: 3.0222, Perplexity: 20.5371\n",
      "Epoch [1/3], Step [107400/138038], Loss: 2.3175, Perplexity: 10.1505\n",
      "Epoch [1/3], Step [107500/138038], Loss: 2.8638, Perplexity: 17.52723\n",
      "Epoch [1/3], Step [107600/138038], Loss: 3.5701, Perplexity: 35.5206\n",
      "Epoch [1/3], Step [107700/138038], Loss: 3.3533, Perplexity: 28.59837\n",
      "Epoch [1/3], Step [107800/138038], Loss: 1.9152, Perplexity: 6.788377\n",
      "Epoch [1/3], Step [107900/138038], Loss: 2.7581, Perplexity: 15.7702\n",
      "Epoch [1/3], Step [108000/138038], Loss: 2.7319, Perplexity: 15.3613\n",
      "Epoch [1/3], Step [108100/138038], Loss: 2.2395, Perplexity: 9.38849\n",
      "Epoch [1/3], Step [108200/138038], Loss: 3.5607, Perplexity: 35.1878\n",
      "Epoch [1/3], Step [108300/138038], Loss: 3.0536, Perplexity: 21.1910\n",
      "Epoch [1/3], Step [108400/138038], Loss: 2.0237, Perplexity: 7.566141\n",
      "Epoch [1/3], Step [108500/138038], Loss: 2.5817, Perplexity: 13.2192\n",
      "Epoch [1/3], Step [108600/138038], Loss: 2.9159, Perplexity: 18.4659\n",
      "Epoch [1/3], Step [108700/138038], Loss: 1.6835, Perplexity: 5.38446\n",
      "Epoch [1/3], Step [108800/138038], Loss: 2.4934, Perplexity: 12.10289\n",
      "Epoch [1/3], Step [108900/138038], Loss: 2.8404, Perplexity: 17.1219\n",
      "Epoch [1/3], Step [109000/138038], Loss: 2.9311, Perplexity: 18.74760\n",
      "Epoch [1/3], Step [109100/138038], Loss: 3.0784, Perplexity: 21.72365\n",
      "Epoch [1/3], Step [109200/138038], Loss: 2.5072, Perplexity: 12.27035\n",
      "Epoch [1/3], Step [109300/138038], Loss: 3.1060, Perplexity: 22.3314\n",
      "Epoch [1/3], Step [109400/138038], Loss: 2.6916, Perplexity: 14.75503\n",
      "Epoch [1/3], Step [109500/138038], Loss: 3.8362, Perplexity: 46.35032\n",
      "Epoch [1/3], Step [109600/138038], Loss: 2.2223, Perplexity: 9.22842\n",
      "Epoch [1/3], Step [109700/138038], Loss: 2.7463, Perplexity: 15.5856\n",
      "Epoch [1/3], Step [109800/138038], Loss: 2.2717, Perplexity: 9.695699\n",
      "Epoch [1/3], Step [109900/138038], Loss: 2.3188, Perplexity: 10.16399\n",
      "Epoch [1/3], Step [110000/138038], Loss: 1.8325, Perplexity: 6.24920\n",
      "Epoch [1/3], Step [110100/138038], Loss: 1.7177, Perplexity: 5.571915\n",
      "Epoch [1/3], Step [110200/138038], Loss: 3.1927, Perplexity: 24.3552\n",
      "Epoch [1/3], Step [110300/138038], Loss: 2.3022, Perplexity: 9.99592\n",
      "Epoch [1/3], Step [110400/138038], Loss: 1.8333, Perplexity: 6.25440\n",
      "Epoch [1/3], Step [110500/138038], Loss: 2.3104, Perplexity: 10.0787\n",
      "Epoch [1/3], Step [110600/138038], Loss: 3.2697, Perplexity: 26.30373\n",
      "Epoch [1/3], Step [110700/138038], Loss: 3.2083, Perplexity: 24.7380\n",
      "Epoch [1/3], Step [110800/138038], Loss: 2.5185, Perplexity: 12.4098\n",
      "Epoch [1/3], Step [110900/138038], Loss: 3.1385, Perplexity: 23.06852\n",
      "Epoch [1/3], Step [111000/138038], Loss: 2.5993, Perplexity: 13.4547\n",
      "Epoch [1/3], Step [111100/138038], Loss: 2.4995, Perplexity: 12.1761\n",
      "Epoch [1/3], Step [111200/138038], Loss: 2.8258, Perplexity: 16.8749\n",
      "Epoch [1/3], Step [111300/138038], Loss: 2.7714, Perplexity: 15.9814\n",
      "Epoch [1/3], Step [111400/138038], Loss: 4.3765, Perplexity: 79.55696\n",
      "Epoch [1/3], Step [111500/138038], Loss: 2.7545, Perplexity: 15.71270\n",
      "Epoch [1/3], Step [111600/138038], Loss: 3.6554, Perplexity: 38.6837\n",
      "Epoch [1/3], Step [111700/138038], Loss: 2.4159, Perplexity: 11.1997\n",
      "Epoch [1/3], Step [111800/138038], Loss: 2.7679, Perplexity: 15.9246\n",
      "Epoch [1/3], Step [111900/138038], Loss: 2.3176, Perplexity: 10.15102\n",
      "Epoch [1/3], Step [112000/138038], Loss: 2.4045, Perplexity: 11.07299\n",
      "Epoch [1/3], Step [112100/138038], Loss: 2.5879, Perplexity: 13.30190\n",
      "Epoch [1/3], Step [112200/138038], Loss: 1.9738, Perplexity: 7.19789\n",
      "Epoch [1/3], Step [112300/138038], Loss: 2.5006, Perplexity: 12.1900\n",
      "Epoch [1/3], Step [112400/138038], Loss: 2.7338, Perplexity: 15.3918\n",
      "Epoch [1/3], Step [112500/138038], Loss: 2.7527, Perplexity: 15.6843\n",
      "Epoch [1/3], Step [112600/138038], Loss: 2.6572, Perplexity: 14.2565\n",
      "Epoch [1/3], Step [112700/138038], Loss: 2.5230, Perplexity: 12.4664\n",
      "Epoch [1/3], Step [112800/138038], Loss: 3.1485, Perplexity: 23.3013\n",
      "Epoch [1/3], Step [112900/138038], Loss: 3.6323, Perplexity: 37.79914\n",
      "Epoch [1/3], Step [113000/138038], Loss: 3.4091, Perplexity: 30.23870\n",
      "Epoch [1/3], Step [113100/138038], Loss: 2.4481, Perplexity: 11.56667\n",
      "Epoch [1/3], Step [113200/138038], Loss: 3.8591, Perplexity: 47.42266\n",
      "Epoch [1/3], Step [113300/138038], Loss: 2.3640, Perplexity: 10.6335\n",
      "Epoch [1/3], Step [113400/138038], Loss: 1.9958, Perplexity: 7.35827\n",
      "Epoch [1/3], Step [113500/138038], Loss: 2.5283, Perplexity: 12.5319\n",
      "Epoch [1/3], Step [113600/138038], Loss: 3.4671, Perplexity: 32.0443\n",
      "Epoch [1/3], Step [113700/138038], Loss: 3.5495, Perplexity: 34.7965\n",
      "Epoch [1/3], Step [113800/138038], Loss: 2.4508, Perplexity: 11.5982\n",
      "Epoch [1/3], Step [113900/138038], Loss: 3.2732, Perplexity: 26.39499\n",
      "Epoch [1/3], Step [114000/138038], Loss: 3.9407, Perplexity: 51.4527\n",
      "Epoch [1/3], Step [114100/138038], Loss: 2.9636, Perplexity: 19.3683\n",
      "Epoch [1/3], Step [114200/138038], Loss: 3.4903, Perplexity: 32.7966\n",
      "Epoch [1/3], Step [114300/138038], Loss: 3.1530, Perplexity: 23.40641\n",
      "Epoch [1/3], Step [114400/138038], Loss: 3.4455, Perplexity: 31.35776\n",
      "Epoch [1/3], Step [114500/138038], Loss: 4.1242, Perplexity: 61.81567\n",
      "Epoch [1/3], Step [114600/138038], Loss: 2.9314, Perplexity: 18.7535\n",
      "Epoch [1/3], Step [114700/138038], Loss: 2.4660, Perplexity: 11.7756\n",
      "Epoch [1/3], Step [114800/138038], Loss: 2.8356, Perplexity: 17.0407\n",
      "Epoch [1/3], Step [114900/138038], Loss: 2.4203, Perplexity: 11.24913\n",
      "Epoch [1/3], Step [115000/138038], Loss: 4.4967, Perplexity: 89.7174\n",
      "Epoch [1/3], Step [115100/138038], Loss: 2.3425, Perplexity: 10.4069\n",
      "Epoch [1/3], Step [115200/138038], Loss: 2.4355, Perplexity: 11.42111\n",
      "Epoch [1/3], Step [115300/138038], Loss: 2.0853, Perplexity: 8.04700\n",
      "Epoch [1/3], Step [115400/138038], Loss: 1.8082, Perplexity: 6.099548\n",
      "Epoch [1/3], Step [115500/138038], Loss: 2.0973, Perplexity: 8.14404\n",
      "Epoch [1/3], Step [115600/138038], Loss: 3.2360, Perplexity: 25.4311\n",
      "Epoch [1/3], Step [115700/138038], Loss: 2.8383, Perplexity: 17.08754\n",
      "Epoch [1/3], Step [115800/138038], Loss: 2.6310, Perplexity: 13.88764\n",
      "Epoch [1/3], Step [115900/138038], Loss: 3.1327, Perplexity: 22.9355\n",
      "Epoch [1/3], Step [116000/138038], Loss: 3.4255, Perplexity: 30.7388\n",
      "Epoch [1/3], Step [116100/138038], Loss: 3.1780, Perplexity: 23.9980\n",
      "Epoch [1/3], Step [116200/138038], Loss: 2.4090, Perplexity: 11.1224\n",
      "Epoch [1/3], Step [116300/138038], Loss: 1.9725, Perplexity: 7.18866\n",
      "Epoch [1/3], Step [116400/138038], Loss: 2.0195, Perplexity: 7.534732\n",
      "Epoch [1/3], Step [116500/138038], Loss: 2.7074, Perplexity: 14.98973\n",
      "Epoch [1/3], Step [116600/138038], Loss: 1.9543, Perplexity: 7.059194\n",
      "Epoch [1/3], Step [116700/138038], Loss: 2.3614, Perplexity: 10.6062\n",
      "Epoch [1/3], Step [116800/138038], Loss: 2.0097, Perplexity: 7.46130\n",
      "Epoch [1/3], Step [116900/138038], Loss: 2.1195, Perplexity: 8.32690\n",
      "Epoch [1/3], Step [117000/138038], Loss: 2.4596, Perplexity: 11.7003\n",
      "Epoch [1/3], Step [117100/138038], Loss: 2.4683, Perplexity: 11.8028\n",
      "Epoch [1/3], Step [117200/138038], Loss: 2.6574, Perplexity: 14.2594\n",
      "Epoch [1/3], Step [117300/138038], Loss: 3.3401, Perplexity: 28.2218\n",
      "Epoch [1/3], Step [117400/138038], Loss: 2.4410, Perplexity: 11.48452\n",
      "Epoch [1/3], Step [117500/138038], Loss: 2.5985, Perplexity: 13.4438\n",
      "Epoch [1/3], Step [117600/138038], Loss: 2.9922, Perplexity: 19.92851\n",
      "Epoch [1/3], Step [117700/138038], Loss: 2.2578, Perplexity: 9.56179\n",
      "Epoch [1/3], Step [117800/138038], Loss: 3.2623, Perplexity: 26.11040\n",
      "Epoch [1/3], Step [117900/138038], Loss: 2.2845, Perplexity: 9.82060\n",
      "Epoch [1/3], Step [118000/138038], Loss: 2.5978, Perplexity: 13.4335\n",
      "Epoch [1/3], Step [118100/138038], Loss: 2.7079, Perplexity: 14.99776\n",
      "Epoch [1/3], Step [118200/138038], Loss: 2.7247, Perplexity: 15.2522\n",
      "Epoch [1/3], Step [118300/138038], Loss: 3.7891, Perplexity: 44.21876\n",
      "Epoch [1/3], Step [118400/138038], Loss: 3.6539, Perplexity: 38.6253\n",
      "Epoch [1/3], Step [118500/138038], Loss: 2.4950, Perplexity: 12.12174\n",
      "Epoch [1/3], Step [118600/138038], Loss: 3.0958, Perplexity: 22.1043\n",
      "Epoch [1/3], Step [118700/138038], Loss: 2.8322, Perplexity: 16.98369\n",
      "Epoch [1/3], Step [118800/138038], Loss: 2.1780, Perplexity: 8.828818\n",
      "Epoch [1/3], Step [118900/138038], Loss: 2.2175, Perplexity: 9.18457\n",
      "Epoch [1/3], Step [119000/138038], Loss: 2.3782, Perplexity: 10.78528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [119100/138038], Loss: 1.8442, Perplexity: 6.32307\n",
      "Epoch [1/3], Step [119200/138038], Loss: 4.3412, Perplexity: 76.8018\n",
      "Epoch [1/3], Step [119300/138038], Loss: 2.0200, Perplexity: 7.538691\n",
      "Epoch [1/3], Step [119400/138038], Loss: 2.3776, Perplexity: 10.7786\n",
      "Epoch [1/3], Step [119500/138038], Loss: 3.2245, Perplexity: 25.1415\n",
      "Epoch [1/3], Step [119600/138038], Loss: 2.0701, Perplexity: 7.92593\n",
      "Epoch [1/3], Step [119700/138038], Loss: 1.4634, Perplexity: 4.320786\n",
      "Epoch [1/3], Step [119800/138038], Loss: 4.0587, Perplexity: 57.9012\n",
      "Epoch [1/3], Step [119900/138038], Loss: 2.8352, Perplexity: 17.03330\n",
      "Epoch [1/3], Step [120000/138038], Loss: 3.4864, Perplexity: 32.6691\n",
      "Epoch [1/3], Step [120100/138038], Loss: 2.8369, Perplexity: 17.06241\n",
      "Epoch [1/3], Step [120200/138038], Loss: 2.9542, Perplexity: 19.1869\n",
      "Epoch [1/3], Step [120300/138038], Loss: 1.7118, Perplexity: 5.538886\n",
      "Epoch [1/3], Step [120400/138038], Loss: 3.6186, Perplexity: 37.2838\n",
      "Epoch [1/3], Step [120500/138038], Loss: 3.1505, Perplexity: 23.3480\n",
      "Epoch [1/3], Step [120600/138038], Loss: 3.4742, Perplexity: 32.27255\n",
      "Epoch [1/3], Step [120700/138038], Loss: 2.3540, Perplexity: 10.5272\n",
      "Epoch [1/3], Step [120800/138038], Loss: 3.1503, Perplexity: 23.3426\n",
      "Epoch [1/3], Step [120900/138038], Loss: 2.5823, Perplexity: 13.2274\n",
      "Epoch [1/3], Step [121000/138038], Loss: 3.1394, Perplexity: 23.0898\n",
      "Epoch [1/3], Step [121100/138038], Loss: 3.3558, Perplexity: 28.66732\n",
      "Epoch [1/3], Step [121200/138038], Loss: 2.1679, Perplexity: 8.739518\n",
      "Epoch [1/3], Step [121300/138038], Loss: 3.8506, Perplexity: 47.02345\n",
      "Epoch [1/3], Step [121400/138038], Loss: 2.3873, Perplexity: 10.8844\n",
      "Epoch [1/3], Step [121500/138038], Loss: 2.3561, Perplexity: 10.54994\n",
      "Epoch [1/3], Step [121600/138038], Loss: 1.7740, Perplexity: 5.89449\n",
      "Epoch [1/3], Step [121700/138038], Loss: 3.3345, Perplexity: 28.0644\n",
      "Epoch [1/3], Step [121800/138038], Loss: 2.8493, Perplexity: 17.2760\n",
      "Epoch [1/3], Step [121900/138038], Loss: 3.4740, Perplexity: 32.26602\n",
      "Epoch [1/3], Step [122000/138038], Loss: 3.2584, Perplexity: 26.00919\n",
      "Epoch [1/3], Step [122100/138038], Loss: 2.4940, Perplexity: 12.10915\n",
      "Epoch [1/3], Step [122200/138038], Loss: 2.2174, Perplexity: 9.183469\n",
      "Epoch [1/3], Step [122300/138038], Loss: 2.2195, Perplexity: 9.203120\n",
      "Epoch [1/3], Step [122400/138038], Loss: 2.3305, Perplexity: 10.2826\n",
      "Epoch [1/3], Step [122500/138038], Loss: 2.9944, Perplexity: 19.9737\n",
      "Epoch [1/3], Step [122600/138038], Loss: 2.7399, Perplexity: 15.48562\n",
      "Epoch [1/3], Step [122700/138038], Loss: 2.5708, Perplexity: 13.0767\n",
      "Epoch [1/3], Step [122800/138038], Loss: 3.0301, Perplexity: 20.6991\n",
      "Epoch [1/3], Step [122900/138038], Loss: 2.7440, Perplexity: 15.5489\n",
      "Epoch [1/3], Step [123000/138038], Loss: 2.2891, Perplexity: 9.865655\n",
      "Epoch [1/3], Step [123100/138038], Loss: 2.2749, Perplexity: 9.726594\n",
      "Epoch [1/3], Step [123200/138038], Loss: 4.1445, Perplexity: 63.0853\n",
      "Epoch [1/3], Step [123300/138038], Loss: 2.6603, Perplexity: 14.30034\n",
      "Epoch [1/3], Step [123400/138038], Loss: 2.2786, Perplexity: 9.76275\n",
      "Epoch [1/3], Step [123500/138038], Loss: 3.9387, Perplexity: 51.3529\n",
      "Epoch [1/3], Step [123600/138038], Loss: 4.1294, Perplexity: 62.1379\n",
      "Epoch [1/3], Step [123700/138038], Loss: 2.5432, Perplexity: 12.7197\n",
      "Epoch [1/3], Step [123800/138038], Loss: 3.8773, Perplexity: 48.29143\n",
      "Epoch [1/3], Step [123900/138038], Loss: 2.4701, Perplexity: 11.8237\n",
      "Epoch [1/3], Step [124000/138038], Loss: 2.7588, Perplexity: 15.78072\n",
      "Epoch [1/3], Step [124100/138038], Loss: 2.5501, Perplexity: 12.80889\n",
      "Epoch [1/3], Step [124200/138038], Loss: 2.7362, Perplexity: 15.42818\n",
      "Epoch [1/3], Step [124300/138038], Loss: 3.3115, Perplexity: 27.42659\n",
      "Epoch [1/3], Step [124400/138038], Loss: 2.5874, Perplexity: 13.2952\n",
      "Epoch [1/3], Step [124500/138038], Loss: 3.6996, Perplexity: 40.4310\n",
      "Epoch [1/3], Step [124600/138038], Loss: 1.9945, Perplexity: 7.348494\n",
      "Epoch [1/3], Step [124700/138038], Loss: 2.3506, Perplexity: 10.4915\n",
      "Epoch [1/3], Step [124800/138038], Loss: 3.1559, Perplexity: 23.4730\n",
      "Epoch [1/3], Step [124900/138038], Loss: 2.7165, Perplexity: 15.12683\n",
      "Epoch [1/3], Step [125000/138038], Loss: 1.5751, Perplexity: 4.831298\n",
      "Epoch [1/3], Step [125100/138038], Loss: 3.2146, Perplexity: 24.89337\n",
      "Epoch [1/3], Step [125200/138038], Loss: 2.4067, Perplexity: 11.09737\n",
      "Epoch [1/3], Step [125300/138038], Loss: 3.3162, Perplexity: 27.5564\n",
      "Epoch [1/3], Step [125400/138038], Loss: 2.5185, Perplexity: 12.40942\n",
      "Epoch [1/3], Step [125500/138038], Loss: 3.3562, Perplexity: 28.6793\n",
      "Epoch [1/3], Step [125600/138038], Loss: 3.1843, Perplexity: 24.1510\n",
      "Epoch [1/3], Step [125700/138038], Loss: 3.2453, Perplexity: 25.6705\n",
      "Epoch [1/3], Step [125800/138038], Loss: 2.6584, Perplexity: 14.2735\n",
      "Epoch [1/3], Step [125900/138038], Loss: 2.6204, Perplexity: 13.7416\n",
      "Epoch [1/3], Step [126000/138038], Loss: 2.2912, Perplexity: 9.887012\n",
      "Epoch [1/3], Step [126100/138038], Loss: 2.3717, Perplexity: 10.71556\n",
      "Epoch [1/3], Step [126200/138038], Loss: 3.1807, Perplexity: 24.0648\n",
      "Epoch [1/3], Step [126300/138038], Loss: 2.7718, Perplexity: 15.9880\n",
      "Epoch [1/3], Step [126400/138038], Loss: 3.3401, Perplexity: 28.2225\n",
      "Epoch [1/3], Step [126500/138038], Loss: 3.1873, Perplexity: 24.22274\n",
      "Epoch [1/3], Step [126600/138038], Loss: 3.4922, Perplexity: 32.85689\n",
      "Epoch [1/3], Step [126700/138038], Loss: 1.7952, Perplexity: 6.02094\n",
      "Epoch [1/3], Step [126800/138038], Loss: 3.6815, Perplexity: 39.7049\n",
      "Epoch [1/3], Step [126900/138038], Loss: 3.5976, Perplexity: 36.5103\n",
      "Epoch [1/3], Step [127000/138038], Loss: 1.7555, Perplexity: 5.786177\n",
      "Epoch [1/3], Step [127100/138038], Loss: 3.9269, Perplexity: 50.74721\n",
      "Epoch [1/3], Step [127200/138038], Loss: 3.1397, Perplexity: 23.09783\n",
      "Epoch [1/3], Step [127300/138038], Loss: 3.4650, Perplexity: 31.97514\n",
      "Epoch [1/3], Step [127400/138038], Loss: 2.7297, Perplexity: 15.3287\n",
      "Epoch [1/3], Step [127500/138038], Loss: 2.6637, Perplexity: 14.3489\n",
      "Epoch [1/3], Step [127600/138038], Loss: 1.3754, Perplexity: 3.95683\n",
      "Epoch [1/3], Step [127700/138038], Loss: 2.0934, Perplexity: 8.112426\n",
      "Epoch [1/3], Step [127800/138038], Loss: 2.0879, Perplexity: 8.06803\n",
      "Epoch [1/3], Step [127900/138038], Loss: 2.5942, Perplexity: 13.38622\n",
      "Epoch [1/3], Step [128000/138038], Loss: 2.7165, Perplexity: 15.1272\n",
      "Epoch [1/3], Step [128100/138038], Loss: 2.2091, Perplexity: 9.10754\n",
      "Epoch [1/3], Step [128200/138038], Loss: 3.7877, Perplexity: 44.1537\n",
      "Epoch [1/3], Step [128300/138038], Loss: 3.2323, Perplexity: 25.3378\n",
      "Epoch [1/3], Step [128400/138038], Loss: 1.9854, Perplexity: 7.28196\n",
      "Epoch [1/3], Step [128500/138038], Loss: 3.3543, Perplexity: 28.62672\n",
      "Epoch [1/3], Step [128600/138038], Loss: 2.7087, Perplexity: 15.01050\n",
      "Epoch [1/3], Step [128700/138038], Loss: 3.1392, Perplexity: 23.08543\n",
      "Epoch [1/3], Step [128800/138038], Loss: 2.4073, Perplexity: 11.1037\n",
      "Epoch [1/3], Step [128900/138038], Loss: 2.2026, Perplexity: 9.04869\n",
      "Epoch [1/3], Step [129000/138038], Loss: 3.3631, Perplexity: 28.8790\n",
      "Epoch [1/3], Step [129100/138038], Loss: 2.6105, Perplexity: 13.6061\n",
      "Epoch [1/3], Step [129200/138038], Loss: 2.2369, Perplexity: 9.36410\n",
      "Epoch [1/3], Step [129300/138038], Loss: 3.3411, Perplexity: 28.2497\n",
      "Epoch [1/3], Step [129400/138038], Loss: 2.7564, Perplexity: 15.7436\n",
      "Epoch [1/3], Step [129500/138038], Loss: 2.8696, Perplexity: 17.62975\n",
      "Epoch [1/3], Step [129600/138038], Loss: 3.1066, Perplexity: 22.34475\n",
      "Epoch [1/3], Step [129700/138038], Loss: 2.8803, Perplexity: 17.81941\n",
      "Epoch [1/3], Step [129800/138038], Loss: 2.7891, Perplexity: 16.26646\n",
      "Epoch [1/3], Step [129900/138038], Loss: 3.2816, Perplexity: 26.61840\n",
      "Epoch [1/3], Step [130000/138038], Loss: 3.0486, Perplexity: 21.08515\n",
      "Epoch [1/3], Step [130100/138038], Loss: 2.4536, Perplexity: 11.63002\n",
      "Epoch [1/3], Step [130200/138038], Loss: 2.6744, Perplexity: 14.50301\n",
      "Epoch [1/3], Step [130300/138038], Loss: 2.5443, Perplexity: 12.7345\n",
      "Epoch [1/3], Step [130400/138038], Loss: 2.8116, Perplexity: 16.63688\n",
      "Epoch [1/3], Step [130500/138038], Loss: 2.2923, Perplexity: 9.89806\n",
      "Epoch [1/3], Step [130600/138038], Loss: 2.4497, Perplexity: 11.5849\n",
      "Epoch [1/3], Step [130700/138038], Loss: 2.3013, Perplexity: 9.987305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [130800/138038], Loss: 1.7676, Perplexity: 5.85650\n",
      "Epoch [1/3], Step [130900/138038], Loss: 2.6828, Perplexity: 14.6261\n",
      "Epoch [1/3], Step [131000/138038], Loss: 2.2434, Perplexity: 9.42574\n",
      "Epoch [1/3], Step [131100/138038], Loss: 4.1742, Perplexity: 64.98581\n",
      "Epoch [1/3], Step [131200/138038], Loss: 2.2762, Perplexity: 9.739354\n",
      "Epoch [1/3], Step [131300/138038], Loss: 2.4422, Perplexity: 11.4985\n",
      "Epoch [1/3], Step [131400/138038], Loss: 2.0511, Perplexity: 7.77626\n",
      "Epoch [1/3], Step [131500/138038], Loss: 2.9723, Perplexity: 19.53670\n",
      "Epoch [1/3], Step [131600/138038], Loss: 2.4325, Perplexity: 11.3877\n",
      "Epoch [1/3], Step [131700/138038], Loss: 2.7145, Perplexity: 15.0978\n",
      "Epoch [1/3], Step [131800/138038], Loss: 2.2172, Perplexity: 9.181576\n",
      "Epoch [1/3], Step [131900/138038], Loss: 2.5380, Perplexity: 12.6540\n",
      "Epoch [1/3], Step [132000/138038], Loss: 2.7307, Perplexity: 15.34435\n",
      "Epoch [1/3], Step [132100/138038], Loss: 3.0874, Perplexity: 21.92082\n",
      "Epoch [1/3], Step [132200/138038], Loss: 3.0218, Perplexity: 20.5276\n",
      "Epoch [1/3], Step [132300/138038], Loss: 2.8184, Perplexity: 16.75056\n",
      "Epoch [1/3], Step [132400/138038], Loss: 2.9567, Perplexity: 19.23355\n",
      "Epoch [1/3], Step [132500/138038], Loss: 1.8805, Perplexity: 6.55686\n",
      "Epoch [1/3], Step [132600/138038], Loss: 2.4727, Perplexity: 11.8549\n",
      "Epoch [1/3], Step [132700/138038], Loss: 4.0407, Perplexity: 56.8671\n",
      "Epoch [1/3], Step [132800/138038], Loss: 2.1000, Perplexity: 8.166593\n",
      "Epoch [1/3], Step [132900/138038], Loss: 2.3512, Perplexity: 10.49810\n",
      "Epoch [1/3], Step [133000/138038], Loss: 2.9953, Perplexity: 19.9914\n",
      "Epoch [1/3], Step [133100/138038], Loss: 2.0022, Perplexity: 7.40568\n",
      "Epoch [1/3], Step [133200/138038], Loss: 3.0735, Perplexity: 21.6185\n",
      "Epoch [1/3], Step [133300/138038], Loss: 2.3374, Perplexity: 10.3546\n",
      "Epoch [1/3], Step [133400/138038], Loss: 3.1142, Perplexity: 22.51641\n",
      "Epoch [1/3], Step [133500/138038], Loss: 3.2058, Perplexity: 24.6760\n",
      "Epoch [1/3], Step [133600/138038], Loss: 2.6455, Perplexity: 14.0905\n",
      "Epoch [1/3], Step [133700/138038], Loss: 4.2294, Perplexity: 68.6784\n",
      "Epoch [1/3], Step [133800/138038], Loss: 2.4932, Perplexity: 12.10007\n",
      "Epoch [1/3], Step [133900/138038], Loss: 3.1145, Perplexity: 22.5219\n",
      "Epoch [1/3], Step [134000/138038], Loss: 2.4076, Perplexity: 11.1073\n",
      "Epoch [1/3], Step [134100/138038], Loss: 4.1280, Perplexity: 62.05384\n",
      "Epoch [1/3], Step [134200/138038], Loss: 2.3385, Perplexity: 10.3654\n",
      "Epoch [1/3], Step [134300/138038], Loss: 3.8927, Perplexity: 49.04104\n",
      "Epoch [1/3], Step [134400/138038], Loss: 3.9156, Perplexity: 50.17807\n",
      "Epoch [1/3], Step [134500/138038], Loss: 2.4958, Perplexity: 12.1316\n",
      "Epoch [1/3], Step [134600/138038], Loss: 2.9590, Perplexity: 19.27905\n",
      "Epoch [1/3], Step [134700/138038], Loss: 2.6734, Perplexity: 14.48850\n",
      "Epoch [1/3], Step [134800/138038], Loss: 1.9641, Perplexity: 7.128498\n",
      "Epoch [1/3], Step [134900/138038], Loss: 2.3970, Perplexity: 10.99013\n",
      "Epoch [1/3], Step [135000/138038], Loss: 2.6455, Perplexity: 14.0909\n",
      "Epoch [1/3], Step [135100/138038], Loss: 1.6356, Perplexity: 5.13249\n",
      "Epoch [1/3], Step [135200/138038], Loss: 2.4656, Perplexity: 11.77052\n",
      "Epoch [1/3], Step [135300/138038], Loss: 1.8766, Perplexity: 6.53166\n",
      "Epoch [1/3], Step [135400/138038], Loss: 2.7601, Perplexity: 15.80128\n",
      "Epoch [1/3], Step [135500/138038], Loss: 2.7727, Perplexity: 16.0015\n",
      "Epoch [1/3], Step [135600/138038], Loss: 2.5647, Perplexity: 12.9972\n",
      "Epoch [1/3], Step [135700/138038], Loss: 2.4429, Perplexity: 11.5069\n",
      "Epoch [1/3], Step [135800/138038], Loss: 3.1158, Perplexity: 22.5517\n",
      "Epoch [1/3], Step [135900/138038], Loss: 2.5755, Perplexity: 13.1380\n",
      "Epoch [1/3], Step [136000/138038], Loss: 3.0528, Perplexity: 21.17404\n",
      "Epoch [1/3], Step [136100/138038], Loss: 2.3954, Perplexity: 10.9730\n",
      "Epoch [1/3], Step [136200/138038], Loss: 3.7988, Perplexity: 44.6475\n",
      "Epoch [1/3], Step [136300/138038], Loss: 2.8808, Perplexity: 17.8290\n",
      "Epoch [1/3], Step [136400/138038], Loss: 3.1341, Perplexity: 22.96759\n",
      "Epoch [1/3], Step [136500/138038], Loss: 2.5386, Perplexity: 12.66194\n",
      "Epoch [1/3], Step [136600/138038], Loss: 2.7824, Perplexity: 16.1583\n",
      "Epoch [1/3], Step [136700/138038], Loss: 2.4840, Perplexity: 11.98876\n",
      "Epoch [1/3], Step [136800/138038], Loss: 3.3858, Perplexity: 29.5405\n",
      "Epoch [1/3], Step [136900/138038], Loss: 2.4682, Perplexity: 11.8014\n",
      "Epoch [1/3], Step [137000/138038], Loss: 2.7062, Perplexity: 14.9728\n",
      "Epoch [1/3], Step [137100/138038], Loss: 3.2000, Perplexity: 24.5332\n",
      "Epoch [1/3], Step [137200/138038], Loss: 2.6860, Perplexity: 14.67324\n",
      "Epoch [1/3], Step [137300/138038], Loss: 2.1972, Perplexity: 8.99956\n",
      "Epoch [1/3], Step [137400/138038], Loss: 2.5319, Perplexity: 12.5773\n",
      "Epoch [1/3], Step [137500/138038], Loss: 3.0457, Perplexity: 21.0256\n",
      "Epoch [1/3], Step [137600/138038], Loss: 2.5039, Perplexity: 12.2306\n",
      "Epoch [1/3], Step [137700/138038], Loss: 3.5330, Perplexity: 34.22678\n",
      "Epoch [1/3], Step [137800/138038], Loss: 4.0743, Perplexity: 58.81176\n",
      "Epoch [1/3], Step [137900/138038], Loss: 2.9780, Perplexity: 19.64874\n",
      "Epoch [1/3], Step [138000/138038], Loss: 3.1017, Perplexity: 22.2353\n",
      "Epoch [2/3], Step [100/138038], Loss: 2.1519, Perplexity: 8.60169614\n",
      "Epoch [2/3], Step [200/138038], Loss: 2.6888, Perplexity: 14.7144\n",
      "Epoch [2/3], Step [300/138038], Loss: 3.7474, Perplexity: 42.41260\n",
      "Epoch [2/3], Step [400/138038], Loss: 2.4728, Perplexity: 11.85626\n",
      "Epoch [2/3], Step [500/138038], Loss: 3.4700, Perplexity: 32.1379\n",
      "Epoch [2/3], Step [600/138038], Loss: 3.3962, Perplexity: 29.8510\n",
      "Epoch [2/3], Step [700/138038], Loss: 1.4453, Perplexity: 4.24303\n",
      "Epoch [2/3], Step [800/138038], Loss: 2.4062, Perplexity: 11.09199\n",
      "Epoch [2/3], Step [900/138038], Loss: 2.0104, Perplexity: 7.46658\n",
      "Epoch [2/3], Step [1000/138038], Loss: 3.4839, Perplexity: 32.5873\n",
      "Epoch [2/3], Step [1100/138038], Loss: 1.7962, Perplexity: 6.027015\n",
      "Epoch [2/3], Step [1200/138038], Loss: 2.6353, Perplexity: 13.94696\n",
      "Epoch [2/3], Step [1300/138038], Loss: 2.4934, Perplexity: 12.1029\n",
      "Epoch [2/3], Step [1400/138038], Loss: 2.3361, Perplexity: 10.3413\n",
      "Epoch [2/3], Step [1500/138038], Loss: 2.7961, Perplexity: 16.3808\n",
      "Epoch [2/3], Step [1600/138038], Loss: 2.4714, Perplexity: 11.8389\n",
      "Epoch [2/3], Step [1700/138038], Loss: 3.0942, Perplexity: 22.0685\n",
      "Epoch [2/3], Step [1800/138038], Loss: 2.6209, Perplexity: 13.7487\n",
      "Epoch [2/3], Step [1900/138038], Loss: 2.8639, Perplexity: 17.5306\n",
      "Epoch [2/3], Step [2000/138038], Loss: 3.3675, Perplexity: 29.00633\n",
      "Epoch [2/3], Step [2100/138038], Loss: 3.4171, Perplexity: 30.47990\n",
      "Epoch [2/3], Step [2200/138038], Loss: 3.1433, Perplexity: 23.1793\n",
      "Epoch [2/3], Step [2300/138038], Loss: 3.2530, Perplexity: 25.8690\n",
      "Epoch [2/3], Step [2400/138038], Loss: 3.6784, Perplexity: 39.58279\n",
      "Epoch [2/3], Step [2500/138038], Loss: 3.4030, Perplexity: 30.0533\n",
      "Epoch [2/3], Step [2600/138038], Loss: 1.0798, Perplexity: 2.94424\n",
      "Epoch [2/3], Step [2700/138038], Loss: 2.2341, Perplexity: 9.33762\n",
      "Epoch [2/3], Step [2800/138038], Loss: 2.9065, Perplexity: 18.2920\n",
      "Epoch [2/3], Step [2900/138038], Loss: 1.8658, Perplexity: 6.461265\n",
      "Epoch [2/3], Step [3000/138038], Loss: 2.1345, Perplexity: 8.45255\n",
      "Epoch [2/3], Step [3100/138038], Loss: 3.2439, Perplexity: 25.63473\n",
      "Epoch [2/3], Step [3200/138038], Loss: 2.3399, Perplexity: 10.3800\n",
      "Epoch [2/3], Step [3300/138038], Loss: 3.1620, Perplexity: 23.6179\n",
      "Epoch [2/3], Step [3400/138038], Loss: 2.5297, Perplexity: 12.5503\n",
      "Epoch [2/3], Step [3500/138038], Loss: 3.0506, Perplexity: 21.1275\n",
      "Epoch [2/3], Step [3600/138038], Loss: 2.0435, Perplexity: 7.71786\n",
      "Epoch [2/3], Step [3700/138038], Loss: 2.5977, Perplexity: 13.43328\n",
      "Epoch [2/3], Step [3800/138038], Loss: 2.8117, Perplexity: 16.6387\n",
      "Epoch [2/3], Step [3900/138038], Loss: 3.2992, Perplexity: 27.0922\n",
      "Epoch [2/3], Step [4000/138038], Loss: 4.2558, Perplexity: 70.51442\n",
      "Epoch [2/3], Step [4100/138038], Loss: 3.1057, Perplexity: 22.32425\n",
      "Epoch [2/3], Step [4200/138038], Loss: 2.5675, Perplexity: 13.0327\n",
      "Epoch [2/3], Step [4300/138038], Loss: 3.6051, Perplexity: 36.78655\n",
      "Epoch [2/3], Step [4400/138038], Loss: 3.4047, Perplexity: 30.1066\n",
      "Epoch [2/3], Step [4500/138038], Loss: 3.7956, Perplexity: 44.50435\n",
      "Epoch [2/3], Step [4600/138038], Loss: 3.8604, Perplexity: 47.4823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [4700/138038], Loss: 2.5281, Perplexity: 12.52945\n",
      "Epoch [2/3], Step [4800/138038], Loss: 2.2221, Perplexity: 9.226685\n",
      "Epoch [2/3], Step [4900/138038], Loss: 2.4637, Perplexity: 11.7479\n",
      "Epoch [2/3], Step [5000/138038], Loss: 1.7254, Perplexity: 5.61466\n",
      "Epoch [2/3], Step [5100/138038], Loss: 2.6297, Perplexity: 13.8700\n",
      "Epoch [2/3], Step [5200/138038], Loss: 3.3082, Perplexity: 27.33685\n",
      "Epoch [2/3], Step [5300/138038], Loss: 3.0612, Perplexity: 21.35265\n",
      "Epoch [2/3], Step [5400/138038], Loss: 2.7871, Perplexity: 16.23397\n",
      "Epoch [2/3], Step [5500/138038], Loss: 2.0847, Perplexity: 8.04247\n",
      "Epoch [2/3], Step [5600/138038], Loss: 2.8580, Perplexity: 17.42716\n",
      "Epoch [2/3], Step [5700/138038], Loss: 2.6960, Perplexity: 14.82103\n",
      "Epoch [2/3], Step [5800/138038], Loss: 2.5084, Perplexity: 12.28570\n",
      "Epoch [2/3], Step [5900/138038], Loss: 2.5937, Perplexity: 13.3789\n",
      "Epoch [2/3], Step [6000/138038], Loss: 2.1368, Perplexity: 8.47246\n",
      "Epoch [2/3], Step [6100/138038], Loss: 2.4801, Perplexity: 11.9424\n",
      "Epoch [2/3], Step [6200/138038], Loss: 3.6295, Perplexity: 37.69455\n",
      "Epoch [2/3], Step [6300/138038], Loss: 2.1348, Perplexity: 8.45558\n",
      "Epoch [2/3], Step [6400/138038], Loss: 3.3539, Perplexity: 28.61373\n",
      "Epoch [2/3], Step [6500/138038], Loss: 3.2288, Perplexity: 25.2498\n",
      "Epoch [2/3], Step [6600/138038], Loss: 2.4974, Perplexity: 12.1510\n",
      "Epoch [2/3], Step [6700/138038], Loss: 2.9701, Perplexity: 19.49316\n",
      "Epoch [2/3], Step [6800/138038], Loss: 2.3538, Perplexity: 10.52568\n",
      "Epoch [2/3], Step [6900/138038], Loss: 3.2524, Perplexity: 25.85277\n",
      "Epoch [2/3], Step [7000/138038], Loss: 3.8616, Perplexity: 47.5418\n",
      "Epoch [2/3], Step [7100/138038], Loss: 3.8241, Perplexity: 45.7906\n",
      "Epoch [2/3], Step [7200/138038], Loss: 2.5049, Perplexity: 12.2429\n",
      "Epoch [2/3], Step [7300/138038], Loss: 2.4598, Perplexity: 11.70212\n",
      "Epoch [2/3], Step [7400/138038], Loss: 2.1307, Perplexity: 8.42057\n",
      "Epoch [2/3], Step [7500/138038], Loss: 2.3171, Perplexity: 10.1459\n",
      "Epoch [2/3], Step [7600/138038], Loss: 2.4835, Perplexity: 11.98346\n",
      "Epoch [2/3], Step [7700/138038], Loss: 3.7113, Perplexity: 40.9052\n",
      "Epoch [2/3], Step [7800/138038], Loss: 3.0195, Perplexity: 20.48017\n",
      "Epoch [2/3], Step [7900/138038], Loss: 2.4043, Perplexity: 11.07076\n",
      "Epoch [2/3], Step [8000/138038], Loss: 2.8280, Perplexity: 16.91157\n",
      "Epoch [2/3], Step [8100/138038], Loss: 2.9935, Perplexity: 19.9549\n",
      "Epoch [2/3], Step [8200/138038], Loss: 2.8651, Perplexity: 17.5516\n",
      "Epoch [2/3], Step [8300/138038], Loss: 2.8630, Perplexity: 17.51360\n",
      "Epoch [2/3], Step [8400/138038], Loss: 2.4565, Perplexity: 11.6645\n",
      "Epoch [2/3], Step [8500/138038], Loss: 1.7552, Perplexity: 5.78449\n",
      "Epoch [2/3], Step [8600/138038], Loss: 2.3841, Perplexity: 10.8496\n",
      "Epoch [2/3], Step [8700/138038], Loss: 2.6923, Perplexity: 14.7656\n",
      "Epoch [2/3], Step [8800/138038], Loss: 3.4232, Perplexity: 30.66739\n",
      "Epoch [2/3], Step [8900/138038], Loss: 2.8900, Perplexity: 17.9934\n",
      "Epoch [2/3], Step [9000/138038], Loss: 3.0207, Perplexity: 20.5064\n",
      "Epoch [2/3], Step [9100/138038], Loss: 2.9865, Perplexity: 19.8156\n",
      "Epoch [2/3], Step [9200/138038], Loss: 1.6338, Perplexity: 5.12317\n",
      "Epoch [2/3], Step [9300/138038], Loss: 2.3833, Perplexity: 10.8401\n",
      "Epoch [2/3], Step [9400/138038], Loss: 1.4607, Perplexity: 4.309254\n",
      "Epoch [2/3], Step [9500/138038], Loss: 2.2051, Perplexity: 9.07107\n",
      "Epoch [2/3], Step [9600/138038], Loss: 3.1418, Perplexity: 23.1445\n",
      "Epoch [2/3], Step [9700/138038], Loss: 3.1300, Perplexity: 22.87428\n",
      "Epoch [2/3], Step [9800/138038], Loss: 1.9511, Perplexity: 7.03658\n",
      "Epoch [2/3], Step [9900/138038], Loss: 2.4578, Perplexity: 11.6790\n",
      "Epoch [2/3], Step [10000/138038], Loss: 2.0087, Perplexity: 7.4533\n",
      "Epoch [2/3], Step [10100/138038], Loss: 2.7920, Perplexity: 16.3136\n",
      "Epoch [2/3], Step [10200/138038], Loss: 3.2724, Perplexity: 26.3742\n",
      "Epoch [2/3], Step [10300/138038], Loss: 2.4255, Perplexity: 11.30847\n",
      "Epoch [2/3], Step [10400/138038], Loss: 3.3760, Perplexity: 29.2522\n",
      "Epoch [2/3], Step [10500/138038], Loss: 1.8069, Perplexity: 6.091440\n",
      "Epoch [2/3], Step [10600/138038], Loss: 2.3716, Perplexity: 10.71482\n",
      "Epoch [2/3], Step [10700/138038], Loss: 4.2101, Perplexity: 67.3601\n",
      "Epoch [2/3], Step [10800/138038], Loss: 2.9770, Perplexity: 19.6283\n",
      "Epoch [2/3], Step [10900/138038], Loss: 3.2725, Perplexity: 26.3785\n",
      "Epoch [2/3], Step [11000/138038], Loss: 2.8273, Perplexity: 16.90031\n",
      "Epoch [2/3], Step [11100/138038], Loss: 2.0587, Perplexity: 7.835897\n",
      "Epoch [2/3], Step [11200/138038], Loss: 3.0765, Perplexity: 21.6815\n",
      "Epoch [2/3], Step [11300/138038], Loss: 3.0906, Perplexity: 21.9913\n",
      "Epoch [2/3], Step [11400/138038], Loss: 2.4576, Perplexity: 11.6764\n",
      "Epoch [2/3], Step [11500/138038], Loss: 3.7997, Perplexity: 44.68806\n",
      "Epoch [2/3], Step [11600/138038], Loss: 2.2669, Perplexity: 9.649793\n",
      "Epoch [2/3], Step [11700/138038], Loss: 3.0833, Perplexity: 21.82960\n",
      "Epoch [2/3], Step [11800/138038], Loss: 2.9015, Perplexity: 18.20208\n",
      "Epoch [2/3], Step [11900/138038], Loss: 2.6806, Perplexity: 14.5946\n",
      "Epoch [2/3], Step [12000/138038], Loss: 2.3151, Perplexity: 10.12559\n",
      "Epoch [2/3], Step [12100/138038], Loss: 2.9397, Perplexity: 18.91107\n",
      "Epoch [2/3], Step [12200/138038], Loss: 3.6175, Perplexity: 37.2447\n",
      "Epoch [2/3], Step [12300/138038], Loss: 2.1710, Perplexity: 8.76675\n",
      "Epoch [2/3], Step [12400/138038], Loss: 4.3819, Perplexity: 79.99048\n",
      "Epoch [2/3], Step [12500/138038], Loss: 1.9143, Perplexity: 6.78191\n",
      "Epoch [2/3], Step [12600/138038], Loss: 2.0909, Perplexity: 8.09192\n",
      "Epoch [2/3], Step [12700/138038], Loss: 2.8993, Perplexity: 18.16109\n",
      "Epoch [2/3], Step [12800/138038], Loss: 3.6366, Perplexity: 37.9641\n",
      "Epoch [2/3], Step [12900/138038], Loss: 1.9442, Perplexity: 6.988119\n",
      "Epoch [2/3], Step [13000/138038], Loss: 2.6967, Perplexity: 14.8311\n",
      "Epoch [2/3], Step [13100/138038], Loss: 2.8246, Perplexity: 16.8549\n",
      "Epoch [2/3], Step [13200/138038], Loss: 2.8276, Perplexity: 16.9042\n",
      "Epoch [2/3], Step [13300/138038], Loss: 3.3730, Perplexity: 29.1664\n",
      "Epoch [2/3], Step [13400/138038], Loss: 2.2864, Perplexity: 9.83938\n",
      "Epoch [2/3], Step [13500/138038], Loss: 2.5921, Perplexity: 13.3572\n",
      "Epoch [2/3], Step [13600/138038], Loss: 3.0428, Perplexity: 20.9642\n",
      "Epoch [2/3], Step [13700/138038], Loss: 2.3240, Perplexity: 10.21601\n",
      "Epoch [2/3], Step [13800/138038], Loss: 4.7435, Perplexity: 114.8311\n",
      "Epoch [2/3], Step [13900/138038], Loss: 2.7475, Perplexity: 15.6035\n",
      "Epoch [2/3], Step [14000/138038], Loss: 2.6240, Perplexity: 13.7910\n",
      "Epoch [2/3], Step [14100/138038], Loss: 3.4181, Perplexity: 30.5105\n",
      "Epoch [2/3], Step [14200/138038], Loss: 2.4961, Perplexity: 12.1356\n",
      "Epoch [2/3], Step [14300/138038], Loss: 1.8118, Perplexity: 6.121424\n",
      "Epoch [2/3], Step [14400/138038], Loss: 1.9853, Perplexity: 7.281275\n",
      "Epoch [2/3], Step [14500/138038], Loss: 2.4403, Perplexity: 11.47605\n",
      "Epoch [2/3], Step [14600/138038], Loss: 2.8765, Perplexity: 17.7524\n",
      "Epoch [2/3], Step [14700/138038], Loss: 2.9395, Perplexity: 18.9065\n",
      "Epoch [2/3], Step [14800/138038], Loss: 2.3702, Perplexity: 10.7001\n",
      "Epoch [2/3], Step [14900/138038], Loss: 2.4576, Perplexity: 11.67727\n",
      "Epoch [2/3], Step [15000/138038], Loss: 2.7603, Perplexity: 15.8039\n",
      "Epoch [2/3], Step [15100/138038], Loss: 2.5413, Perplexity: 12.6964\n",
      "Epoch [2/3], Step [15200/138038], Loss: 2.1661, Perplexity: 8.72425\n",
      "Epoch [2/3], Step [15300/138038], Loss: 2.6527, Perplexity: 14.1922\n",
      "Epoch [2/3], Step [15400/138038], Loss: 2.4267, Perplexity: 11.3213\n",
      "Epoch [2/3], Step [15500/138038], Loss: 2.2303, Perplexity: 9.302556\n",
      "Epoch [2/3], Step [15600/138038], Loss: 2.4007, Perplexity: 11.0309\n",
      "Epoch [2/3], Step [15700/138038], Loss: 2.8136, Perplexity: 16.66925\n",
      "Epoch [2/3], Step [15800/138038], Loss: 2.0302, Perplexity: 7.61591\n",
      "Epoch [2/3], Step [15900/138038], Loss: 1.8456, Perplexity: 6.33215\n",
      "Epoch [2/3], Step [16000/138038], Loss: 1.7729, Perplexity: 5.88812\n",
      "Epoch [2/3], Step [16100/138038], Loss: 3.2685, Perplexity: 26.2730\n",
      "Epoch [2/3], Step [16200/138038], Loss: 4.5544, Perplexity: 95.0541\n",
      "Epoch [2/3], Step [16300/138038], Loss: 2.4243, Perplexity: 11.29389\n",
      "Epoch [2/3], Step [16400/138038], Loss: 2.8355, Perplexity: 17.0396\n",
      "Epoch [2/3], Step [16500/138038], Loss: 1.9597, Perplexity: 7.09755\n",
      "Epoch [2/3], Step [16600/138038], Loss: 2.4823, Perplexity: 11.9689\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [16700/138038], Loss: 2.6508, Perplexity: 14.16564\n",
      "Epoch [2/3], Step [16800/138038], Loss: 2.0660, Perplexity: 7.892872\n",
      "Epoch [2/3], Step [16900/138038], Loss: 3.0679, Perplexity: 21.4958\n",
      "Epoch [2/3], Step [17000/138038], Loss: 2.4934, Perplexity: 12.1021\n",
      "Epoch [2/3], Step [17100/138038], Loss: 2.7705, Perplexity: 15.9661\n",
      "Epoch [2/3], Step [17200/138038], Loss: 2.2204, Perplexity: 9.21118\n",
      "Epoch [2/3], Step [17300/138038], Loss: 3.1906, Perplexity: 24.30282\n",
      "Epoch [2/3], Step [17400/138038], Loss: 2.7437, Perplexity: 15.54379\n",
      "Epoch [2/3], Step [17500/138038], Loss: 3.1544, Perplexity: 23.4391\n",
      "Epoch [2/3], Step [17600/138038], Loss: 3.1698, Perplexity: 23.8018\n",
      "Epoch [2/3], Step [17700/138038], Loss: 3.1641, Perplexity: 23.6668\n",
      "Epoch [2/3], Step [17800/138038], Loss: 2.6489, Perplexity: 14.13819\n",
      "Epoch [2/3], Step [17900/138038], Loss: 2.1914, Perplexity: 8.94799\n",
      "Epoch [2/3], Step [18000/138038], Loss: 2.0446, Perplexity: 7.72576\n",
      "Epoch [2/3], Step [18100/138038], Loss: 3.2093, Perplexity: 24.76236\n",
      "Epoch [2/3], Step [18200/138038], Loss: 2.4490, Perplexity: 11.5762\n",
      "Epoch [2/3], Step [18300/138038], Loss: 3.6132, Perplexity: 37.08393\n",
      "Epoch [2/3], Step [18400/138038], Loss: 1.6381, Perplexity: 5.145502\n",
      "Epoch [2/3], Step [18500/138038], Loss: 2.9555, Perplexity: 19.21150\n",
      "Epoch [2/3], Step [18600/138038], Loss: 2.7527, Perplexity: 15.6843\n",
      "Epoch [2/3], Step [18700/138038], Loss: 3.0963, Perplexity: 22.11541\n",
      "Epoch [2/3], Step [18800/138038], Loss: 3.8370, Perplexity: 46.3881\n",
      "Epoch [2/3], Step [18900/138038], Loss: 1.7544, Perplexity: 5.780218\n",
      "Epoch [2/3], Step [19000/138038], Loss: 2.2546, Perplexity: 9.53146\n",
      "Epoch [2/3], Step [19100/138038], Loss: 2.8646, Perplexity: 17.5423\n",
      "Epoch [2/3], Step [19200/138038], Loss: 2.4582, Perplexity: 11.6837\n",
      "Epoch [2/3], Step [19300/138038], Loss: 3.0943, Perplexity: 22.0722\n",
      "Epoch [2/3], Step [19400/138038], Loss: 2.7495, Perplexity: 15.63408\n",
      "Epoch [2/3], Step [19500/138038], Loss: 2.9277, Perplexity: 18.6842\n",
      "Epoch [2/3], Step [19600/138038], Loss: 2.2847, Perplexity: 9.822882\n",
      "Epoch [2/3], Step [19700/138038], Loss: 3.3174, Perplexity: 27.5876\n",
      "Epoch [2/3], Step [19800/138038], Loss: 2.9506, Perplexity: 19.1170\n",
      "Epoch [2/3], Step [19900/138038], Loss: 2.8325, Perplexity: 16.98777\n",
      "Epoch [2/3], Step [20000/138038], Loss: 2.9428, Perplexity: 18.9680\n",
      "Epoch [2/3], Step [20100/138038], Loss: 1.6892, Perplexity: 5.41514\n",
      "Epoch [2/3], Step [20200/138038], Loss: 2.4762, Perplexity: 11.89570\n",
      "Epoch [2/3], Step [20300/138038], Loss: 3.8567, Perplexity: 47.31076\n",
      "Epoch [2/3], Step [20400/138038], Loss: 2.2846, Perplexity: 9.82176\n",
      "Epoch [2/3], Step [20500/138038], Loss: 2.8948, Perplexity: 18.07986\n",
      "Epoch [2/3], Step [20600/138038], Loss: 2.3734, Perplexity: 10.73348\n",
      "Epoch [2/3], Step [20700/138038], Loss: 2.7520, Perplexity: 15.6743\n",
      "Epoch [2/3], Step [20800/138038], Loss: 2.7990, Perplexity: 16.4279\n",
      "Epoch [2/3], Step [20900/138038], Loss: 2.3191, Perplexity: 10.1662\n",
      "Epoch [2/3], Step [21000/138038], Loss: 2.3468, Perplexity: 10.4516\n",
      "Epoch [2/3], Step [21100/138038], Loss: 3.7257, Perplexity: 41.5002\n",
      "Epoch [2/3], Step [21200/138038], Loss: 2.7954, Perplexity: 16.3689\n",
      "Epoch [2/3], Step [21300/138038], Loss: 2.8022, Perplexity: 16.4812\n",
      "Epoch [2/3], Step [21400/138038], Loss: 3.1692, Perplexity: 23.78800\n",
      "Epoch [2/3], Step [21500/138038], Loss: 2.4153, Perplexity: 11.19285\n",
      "Epoch [2/3], Step [21600/138038], Loss: 3.2335, Perplexity: 25.3672\n",
      "Epoch [2/3], Step [21700/138038], Loss: 3.6773, Perplexity: 39.54111\n",
      "Epoch [2/3], Step [21800/138038], Loss: 2.3554, Perplexity: 10.54259\n",
      "Epoch [2/3], Step [21900/138038], Loss: 2.7717, Perplexity: 15.9859\n",
      "Epoch [2/3], Step [22000/138038], Loss: 3.0765, Perplexity: 21.6824\n",
      "Epoch [2/3], Step [22100/138038], Loss: 2.3902, Perplexity: 10.9161\n",
      "Epoch [2/3], Step [22200/138038], Loss: 2.4862, Perplexity: 12.0158\n",
      "Epoch [2/3], Step [22300/138038], Loss: 2.3591, Perplexity: 10.5814\n",
      "Epoch [2/3], Step [22400/138038], Loss: 2.5631, Perplexity: 12.9755\n",
      "Epoch [2/3], Step [22500/138038], Loss: 3.0329, Perplexity: 20.7567\n",
      "Epoch [2/3], Step [22600/138038], Loss: 1.8791, Perplexity: 6.547772\n",
      "Epoch [2/3], Step [22700/138038], Loss: 2.5164, Perplexity: 12.3841\n",
      "Epoch [2/3], Step [22800/138038], Loss: 4.8644, Perplexity: 129.5967\n",
      "Epoch [2/3], Step [22900/138038], Loss: 2.3496, Perplexity: 10.4809\n",
      "Epoch [2/3], Step [23000/138038], Loss: 1.9054, Perplexity: 6.72197\n",
      "Epoch [2/3], Step [23100/138038], Loss: 3.2591, Perplexity: 26.0272\n",
      "Epoch [2/3], Step [23200/138038], Loss: 2.2042, Perplexity: 9.06278\n",
      "Epoch [2/3], Step [23300/138038], Loss: 2.7065, Perplexity: 14.9767\n",
      "Epoch [2/3], Step [23400/138038], Loss: 2.4045, Perplexity: 11.0731\n",
      "Epoch [2/3], Step [23500/138038], Loss: 2.7772, Perplexity: 16.07372\n",
      "Epoch [2/3], Step [23600/138038], Loss: 3.3536, Perplexity: 28.6042\n",
      "Epoch [2/3], Step [23700/138038], Loss: 2.4871, Perplexity: 12.02651\n",
      "Epoch [2/3], Step [23800/138038], Loss: 2.4221, Perplexity: 11.2695\n",
      "Epoch [2/3], Step [23900/138038], Loss: 2.4713, Perplexity: 11.83767\n",
      "Epoch [2/3], Step [24000/138038], Loss: 2.5350, Perplexity: 12.61632\n",
      "Epoch [2/3], Step [24100/138038], Loss: 3.6948, Perplexity: 40.23899\n",
      "Epoch [2/3], Step [24200/138038], Loss: 1.7465, Perplexity: 5.73463\n",
      "Epoch [2/3], Step [24300/138038], Loss: 2.1073, Perplexity: 8.226383\n",
      "Epoch [2/3], Step [24400/138038], Loss: 3.1928, Perplexity: 24.3577\n",
      "Epoch [2/3], Step [24500/138038], Loss: 3.2368, Perplexity: 25.4531\n",
      "Epoch [2/3], Step [24600/138038], Loss: 3.0153, Perplexity: 20.3957\n",
      "Epoch [2/3], Step [24700/138038], Loss: 1.7162, Perplexity: 5.56322\n",
      "Epoch [2/3], Step [24800/138038], Loss: 3.6121, Perplexity: 37.04346\n",
      "Epoch [2/3], Step [24900/138038], Loss: 2.4510, Perplexity: 11.6003\n",
      "Epoch [2/3], Step [25000/138038], Loss: 3.4711, Perplexity: 32.1719\n",
      "Epoch [2/3], Step [25100/138038], Loss: 1.9709, Perplexity: 7.17704\n",
      "Epoch [2/3], Step [25200/138038], Loss: 2.4569, Perplexity: 11.66901\n",
      "Epoch [2/3], Step [25300/138038], Loss: 1.9348, Perplexity: 6.92290\n",
      "Epoch [2/3], Step [25400/138038], Loss: 3.5511, Perplexity: 34.85244\n",
      "Epoch [2/3], Step [25500/138038], Loss: 3.7858, Perplexity: 44.0727\n",
      "Epoch [2/3], Step [25600/138038], Loss: 2.8372, Perplexity: 17.0686\n",
      "Epoch [2/3], Step [25700/138038], Loss: 2.8728, Perplexity: 17.68608\n",
      "Epoch [2/3], Step [25800/138038], Loss: 2.6063, Perplexity: 13.5492\n",
      "Epoch [2/3], Step [25900/138038], Loss: 2.5974, Perplexity: 13.4293\n",
      "Epoch [2/3], Step [26000/138038], Loss: 2.8061, Perplexity: 16.5447\n",
      "Epoch [2/3], Step [26100/138038], Loss: 2.4917, Perplexity: 12.0814\n",
      "Epoch [2/3], Step [26200/138038], Loss: 3.1189, Perplexity: 22.6224\n",
      "Epoch [2/3], Step [26300/138038], Loss: 1.8772, Perplexity: 6.53534\n",
      "Epoch [2/3], Step [26400/138038], Loss: 2.9765, Perplexity: 19.6196\n",
      "Epoch [2/3], Step [26500/138038], Loss: 2.5586, Perplexity: 12.9179\n",
      "Epoch [2/3], Step [26600/138038], Loss: 2.6652, Perplexity: 14.3714\n",
      "Epoch [2/3], Step [26700/138038], Loss: 3.3407, Perplexity: 28.24011\n",
      "Epoch [2/3], Step [26800/138038], Loss: 3.9076, Perplexity: 49.7798\n",
      "Epoch [2/3], Step [26900/138038], Loss: 3.1713, Perplexity: 23.8394\n",
      "Epoch [2/3], Step [27000/138038], Loss: 2.7076, Perplexity: 14.9938\n",
      "Epoch [2/3], Step [27100/138038], Loss: 3.2090, Perplexity: 24.7545\n",
      "Epoch [2/3], Step [27200/138038], Loss: 2.7280, Perplexity: 15.30299\n",
      "Epoch [2/3], Step [27300/138038], Loss: 3.5992, Perplexity: 36.57004\n",
      "Epoch [2/3], Step [27400/138038], Loss: 3.1313, Perplexity: 22.9038\n",
      "Epoch [2/3], Step [27500/138038], Loss: 2.0205, Perplexity: 7.541930\n",
      "Epoch [2/3], Step [27600/138038], Loss: 3.8236, Perplexity: 45.76734\n",
      "Epoch [2/3], Step [27700/138038], Loss: 3.9298, Perplexity: 50.8946\n",
      "Epoch [2/3], Step [27800/138038], Loss: 3.8150, Perplexity: 45.3754\n",
      "Epoch [2/3], Step [27900/138038], Loss: 3.2140, Perplexity: 24.8794\n",
      "Epoch [2/3], Step [28000/138038], Loss: 2.2638, Perplexity: 9.619264\n",
      "Epoch [2/3], Step [28100/138038], Loss: 2.6512, Perplexity: 14.1705\n",
      "Epoch [2/3], Step [28200/138038], Loss: 3.1148, Perplexity: 22.52882\n",
      "Epoch [2/3], Step [28300/138038], Loss: 2.6783, Perplexity: 14.5604\n",
      "Epoch [2/3], Step [28400/138038], Loss: 3.2051, Perplexity: 24.65730\n",
      "Epoch [2/3], Step [28500/138038], Loss: 4.1027, Perplexity: 60.5038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [28600/138038], Loss: 3.3922, Perplexity: 29.73080\n",
      "Epoch [2/3], Step [28700/138038], Loss: 2.0734, Perplexity: 7.951853\n",
      "Epoch [2/3], Step [28800/138038], Loss: 2.5838, Perplexity: 13.2474\n",
      "Epoch [2/3], Step [28900/138038], Loss: 4.1845, Perplexity: 65.66350\n",
      "Epoch [2/3], Step [29000/138038], Loss: 2.6159, Perplexity: 13.67903\n",
      "Epoch [2/3], Step [29100/138038], Loss: 3.5835, Perplexity: 35.9987\n",
      "Epoch [2/3], Step [29200/138038], Loss: 2.4869, Perplexity: 12.0244\n",
      "Epoch [2/3], Step [29300/138038], Loss: 3.0964, Perplexity: 22.11777\n",
      "Epoch [2/3], Step [29400/138038], Loss: 2.5703, Perplexity: 13.07000\n",
      "Epoch [2/3], Step [29500/138038], Loss: 2.7682, Perplexity: 15.9297\n",
      "Epoch [2/3], Step [29600/138038], Loss: 2.9047, Perplexity: 18.25905\n",
      "Epoch [2/3], Step [29700/138038], Loss: 2.6009, Perplexity: 13.4761\n",
      "Epoch [2/3], Step [29800/138038], Loss: 3.6260, Perplexity: 37.56076\n",
      "Epoch [2/3], Step [29900/138038], Loss: 2.1097, Perplexity: 8.245745\n",
      "Epoch [2/3], Step [30000/138038], Loss: 2.3602, Perplexity: 10.59355\n",
      "Epoch [2/3], Step [30100/138038], Loss: 3.2911, Perplexity: 26.8720\n",
      "Epoch [2/3], Step [30200/138038], Loss: 2.3108, Perplexity: 10.0821\n",
      "Epoch [2/3], Step [30300/138038], Loss: 2.2824, Perplexity: 9.79987\n",
      "Epoch [2/3], Step [30400/138038], Loss: 2.7036, Perplexity: 14.9336\n",
      "Epoch [2/3], Step [30500/138038], Loss: 3.7625, Perplexity: 43.05548\n",
      "Epoch [2/3], Step [30600/138038], Loss: 2.2449, Perplexity: 9.439116\n",
      "Epoch [2/3], Step [30700/138038], Loss: 3.2519, Perplexity: 25.8390\n",
      "Epoch [2/3], Step [30800/138038], Loss: 3.3526, Perplexity: 28.57723\n",
      "Epoch [2/3], Step [30900/138038], Loss: 2.2864, Perplexity: 9.83900\n",
      "Epoch [2/3], Step [31000/138038], Loss: 2.7229, Perplexity: 15.2245\n",
      "Epoch [2/3], Step [31100/138038], Loss: 2.5009, Perplexity: 12.19357\n",
      "Epoch [2/3], Step [31200/138038], Loss: 2.5422, Perplexity: 12.7074\n",
      "Epoch [2/3], Step [31300/138038], Loss: 3.2364, Perplexity: 25.4413\n",
      "Epoch [2/3], Step [31400/138038], Loss: 2.0328, Perplexity: 7.635349\n",
      "Epoch [2/3], Step [31500/138038], Loss: 2.9300, Perplexity: 18.7273\n",
      "Epoch [2/3], Step [31600/138038], Loss: 2.3614, Perplexity: 10.6058\n",
      "Epoch [2/3], Step [31700/138038], Loss: 2.0387, Perplexity: 7.68044\n",
      "Epoch [2/3], Step [31800/138038], Loss: 2.7651, Perplexity: 15.8814\n",
      "Epoch [2/3], Step [31900/138038], Loss: 1.5781, Perplexity: 4.84595\n",
      "Epoch [2/3], Step [32000/138038], Loss: 2.7040, Perplexity: 14.93937\n",
      "Epoch [2/3], Step [32100/138038], Loss: 1.5285, Perplexity: 4.611384\n",
      "Epoch [2/3], Step [32200/138038], Loss: 2.0275, Perplexity: 7.595348\n",
      "Epoch [2/3], Step [32300/138038], Loss: 3.5289, Perplexity: 34.0873\n",
      "Epoch [2/3], Step [32400/138038], Loss: 2.5664, Perplexity: 13.0183\n",
      "Epoch [2/3], Step [32500/138038], Loss: 5.1560, Perplexity: 173.4678\n",
      "Epoch [2/3], Step [32600/138038], Loss: 2.9062, Perplexity: 18.2865\n",
      "Epoch [2/3], Step [32700/138038], Loss: 2.6616, Perplexity: 14.3188\n",
      "Epoch [2/3], Step [32800/138038], Loss: 2.0947, Perplexity: 8.122837\n",
      "Epoch [2/3], Step [32900/138038], Loss: 2.5771, Perplexity: 13.15916\n",
      "Epoch [2/3], Step [33000/138038], Loss: 3.6231, Perplexity: 37.4542\n",
      "Epoch [2/3], Step [33100/138038], Loss: 2.4879, Perplexity: 12.03590\n",
      "Epoch [2/3], Step [33200/138038], Loss: 3.1108, Perplexity: 22.4388\n",
      "Epoch [2/3], Step [33300/138038], Loss: 2.3969, Perplexity: 10.9894\n",
      "Epoch [2/3], Step [33400/138038], Loss: 3.4859, Perplexity: 32.6502\n",
      "Epoch [2/3], Step [33500/138038], Loss: 2.6973, Perplexity: 14.8400\n",
      "Epoch [2/3], Step [33600/138038], Loss: 2.6358, Perplexity: 13.95444\n",
      "Epoch [2/3], Step [33700/138038], Loss: 2.8240, Perplexity: 16.8441\n",
      "Epoch [2/3], Step [33800/138038], Loss: 3.0303, Perplexity: 20.70375\n",
      "Epoch [2/3], Step [33900/138038], Loss: 2.6441, Perplexity: 14.07126\n",
      "Epoch [2/3], Step [34000/138038], Loss: 1.4666, Perplexity: 4.33449\n",
      "Epoch [2/3], Step [34100/138038], Loss: 2.3194, Perplexity: 10.16910\n",
      "Epoch [2/3], Step [34200/138038], Loss: 3.2664, Perplexity: 26.21754\n",
      "Epoch [2/3], Step [34300/138038], Loss: 4.3926, Perplexity: 80.84728\n",
      "Epoch [2/3], Step [34400/138038], Loss: 2.3521, Perplexity: 10.5081\n",
      "Epoch [2/3], Step [34500/138038], Loss: 2.7637, Perplexity: 15.85840\n",
      "Epoch [2/3], Step [34600/138038], Loss: 2.1706, Perplexity: 8.76325\n",
      "Epoch [2/3], Step [34700/138038], Loss: 3.7462, Perplexity: 42.35869\n",
      "Epoch [2/3], Step [34800/138038], Loss: 3.1141, Perplexity: 22.5129\n",
      "Epoch [2/3], Step [34900/138038], Loss: 2.5236, Perplexity: 12.4733\n",
      "Epoch [2/3], Step [35000/138038], Loss: 3.0186, Perplexity: 20.46198\n",
      "Epoch [2/3], Step [35100/138038], Loss: 3.0684, Perplexity: 21.50737\n",
      "Epoch [2/3], Step [35200/138038], Loss: 3.0765, Perplexity: 21.6819\n",
      "Epoch [2/3], Step [35300/138038], Loss: 2.6400, Perplexity: 14.0136\n",
      "Epoch [2/3], Step [35400/138038], Loss: 3.0622, Perplexity: 21.3750\n",
      "Epoch [2/3], Step [35500/138038], Loss: 1.3764, Perplexity: 3.960500\n",
      "Epoch [2/3], Step [35600/138038], Loss: 2.5365, Perplexity: 12.63555\n",
      "Epoch [2/3], Step [35700/138038], Loss: 1.9574, Perplexity: 7.080609\n",
      "Epoch [2/3], Step [35800/138038], Loss: 1.8463, Perplexity: 6.33644\n",
      "Epoch [2/3], Step [35900/138038], Loss: 3.3399, Perplexity: 28.2157\n",
      "Epoch [2/3], Step [36000/138038], Loss: 3.4966, Perplexity: 33.00335\n",
      "Epoch [2/3], Step [36100/138038], Loss: 3.2253, Perplexity: 25.15994\n",
      "Epoch [2/3], Step [36200/138038], Loss: 2.9817, Perplexity: 19.7215\n",
      "Epoch [2/3], Step [36300/138038], Loss: 2.3153, Perplexity: 10.1281\n",
      "Epoch [2/3], Step [36400/138038], Loss: 2.4774, Perplexity: 11.91029\n",
      "Epoch [2/3], Step [36500/138038], Loss: 2.8679, Perplexity: 17.6006\n",
      "Epoch [2/3], Step [36600/138038], Loss: 3.1981, Perplexity: 24.4853\n",
      "Epoch [2/3], Step [36700/138038], Loss: 2.0003, Perplexity: 7.39158\n",
      "Epoch [2/3], Step [36800/138038], Loss: 3.1992, Perplexity: 24.5140\n",
      "Epoch [2/3], Step [36900/138038], Loss: 2.7473, Perplexity: 15.6001\n",
      "Epoch [2/3], Step [37000/138038], Loss: 1.8961, Perplexity: 6.65981\n",
      "Epoch [2/3], Step [37100/138038], Loss: 2.6547, Perplexity: 14.2201\n",
      "Epoch [2/3], Step [37200/138038], Loss: 2.0576, Perplexity: 7.826889\n",
      "Epoch [2/3], Step [37300/138038], Loss: 2.7069, Perplexity: 14.9829\n",
      "Epoch [2/3], Step [37400/138038], Loss: 2.6064, Perplexity: 13.5507\n",
      "Epoch [2/3], Step [37500/138038], Loss: 3.3325, Perplexity: 28.00731\n",
      "Epoch [2/3], Step [37600/138038], Loss: 2.6388, Perplexity: 13.9967\n",
      "Epoch [2/3], Step [37700/138038], Loss: 2.2790, Perplexity: 9.76744\n",
      "Epoch [2/3], Step [37800/138038], Loss: 3.4069, Perplexity: 30.17078\n",
      "Epoch [2/3], Step [37900/138038], Loss: 3.2154, Perplexity: 24.91214\n",
      "Epoch [2/3], Step [38000/138038], Loss: 2.7743, Perplexity: 16.02685\n",
      "Epoch [2/3], Step [38100/138038], Loss: 2.5836, Perplexity: 13.24468\n",
      "Epoch [2/3], Step [38200/138038], Loss: 2.9852, Perplexity: 19.7901\n",
      "Epoch [2/3], Step [38300/138038], Loss: 2.5799, Perplexity: 13.1961\n",
      "Epoch [2/3], Step [38400/138038], Loss: 2.3275, Perplexity: 10.25206\n",
      "Epoch [2/3], Step [38500/138038], Loss: 1.9440, Perplexity: 6.986912\n",
      "Epoch [2/3], Step [38600/138038], Loss: 2.7328, Perplexity: 15.37537\n",
      "Epoch [2/3], Step [38700/138038], Loss: 1.6325, Perplexity: 5.11640\n",
      "Epoch [2/3], Step [38800/138038], Loss: 2.5467, Perplexity: 12.7653\n",
      "Epoch [2/3], Step [38900/138038], Loss: 3.2843, Perplexity: 26.6907\n",
      "Epoch [2/3], Step [39000/138038], Loss: 2.0617, Perplexity: 7.859572\n",
      "Epoch [2/3], Step [39100/138038], Loss: 2.6615, Perplexity: 14.31826\n",
      "Epoch [2/3], Step [39200/138038], Loss: 1.9951, Perplexity: 7.35304\n",
      "Epoch [2/3], Step [39300/138038], Loss: 2.9067, Perplexity: 18.29719\n",
      "Epoch [2/3], Step [39400/138038], Loss: 2.8938, Perplexity: 18.0623\n",
      "Epoch [2/3], Step [39500/138038], Loss: 3.7282, Perplexity: 41.60456\n",
      "Epoch [2/3], Step [39600/138038], Loss: 2.6136, Perplexity: 13.64822\n",
      "Epoch [2/3], Step [39700/138038], Loss: 2.1545, Perplexity: 8.62328\n",
      "Epoch [2/3], Step [39800/138038], Loss: 3.3595, Perplexity: 28.7734\n",
      "Epoch [2/3], Step [39900/138038], Loss: 3.0065, Perplexity: 20.2171\n",
      "Epoch [2/3], Step [40000/138038], Loss: 3.2275, Perplexity: 25.21702\n",
      "Epoch [2/3], Step [40100/138038], Loss: 1.8847, Perplexity: 6.58419\n",
      "Epoch [2/3], Step [40200/138038], Loss: 1.5050, Perplexity: 4.50391\n",
      "Epoch [2/3], Step [40300/138038], Loss: 2.7466, Perplexity: 15.5894\n",
      "Epoch [2/3], Step [40400/138038], Loss: 2.9529, Perplexity: 19.16175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [40500/138038], Loss: 1.6614, Perplexity: 5.26691\n",
      "Epoch [2/3], Step [40600/138038], Loss: 1.5782, Perplexity: 4.846389\n",
      "Epoch [2/3], Step [40700/138038], Loss: 3.6318, Perplexity: 37.7806\n",
      "Epoch [2/3], Step [40800/138038], Loss: 2.7026, Perplexity: 14.9183\n",
      "Epoch [2/3], Step [40900/138038], Loss: 2.6445, Perplexity: 14.0759\n",
      "Epoch [2/3], Step [41000/138038], Loss: 2.4649, Perplexity: 11.76184\n",
      "Epoch [2/3], Step [41100/138038], Loss: 3.3579, Perplexity: 28.72827\n",
      "Epoch [2/3], Step [41200/138038], Loss: 2.5000, Perplexity: 12.1830\n",
      "Epoch [2/3], Step [41300/138038], Loss: 4.0382, Perplexity: 56.7265\n",
      "Epoch [2/3], Step [41400/138038], Loss: 2.2634, Perplexity: 9.61544\n",
      "Epoch [2/3], Step [41500/138038], Loss: 2.0985, Perplexity: 8.154092\n",
      "Epoch [2/3], Step [41600/138038], Loss: 3.4430, Perplexity: 31.2804\n",
      "Epoch [2/3], Step [41700/138038], Loss: 3.5671, Perplexity: 35.41322\n",
      "Epoch [2/3], Step [41800/138038], Loss: 3.2958, Perplexity: 26.9990\n",
      "Epoch [2/3], Step [41900/138038], Loss: 2.4704, Perplexity: 11.8268\n",
      "Epoch [2/3], Step [42000/138038], Loss: 2.9399, Perplexity: 18.9135\n",
      "Epoch [2/3], Step [42100/138038], Loss: 2.9470, Perplexity: 19.04933\n",
      "Epoch [2/3], Step [42200/138038], Loss: 2.6911, Perplexity: 14.7475\n",
      "Epoch [2/3], Step [42300/138038], Loss: 3.1996, Perplexity: 24.52388\n",
      "Epoch [2/3], Step [42400/138038], Loss: 2.7245, Perplexity: 15.2481\n",
      "Epoch [2/3], Step [42500/138038], Loss: 2.6125, Perplexity: 13.63368\n",
      "Epoch [2/3], Step [42600/138038], Loss: 3.3025, Perplexity: 27.18151\n",
      "Epoch [2/3], Step [42700/138038], Loss: 3.4034, Perplexity: 30.06493\n",
      "Epoch [2/3], Step [42800/138038], Loss: 1.5561, Perplexity: 4.74059\n",
      "Epoch [2/3], Step [42900/138038], Loss: 2.5774, Perplexity: 13.16279\n",
      "Epoch [2/3], Step [43000/138038], Loss: 2.8998, Perplexity: 18.1708\n",
      "Epoch [2/3], Step [43100/138038], Loss: 2.6648, Perplexity: 14.3657\n",
      "Epoch [2/3], Step [43200/138038], Loss: 2.0359, Perplexity: 7.659568\n",
      "Epoch [2/3], Step [43300/138038], Loss: 3.4096, Perplexity: 30.25345\n",
      "Epoch [2/3], Step [43400/138038], Loss: 2.4738, Perplexity: 11.86716\n",
      "Epoch [2/3], Step [43500/138038], Loss: 2.4587, Perplexity: 11.6895\n",
      "Epoch [2/3], Step [43600/138038], Loss: 2.9959, Perplexity: 20.00321\n",
      "Epoch [2/3], Step [43700/138038], Loss: 2.9445, Perplexity: 19.00077\n",
      "Epoch [2/3], Step [43800/138038], Loss: 2.4883, Perplexity: 12.0405\n",
      "Epoch [2/3], Step [43900/138038], Loss: 2.7008, Perplexity: 14.89160\n",
      "Epoch [2/3], Step [44000/138038], Loss: 2.6004, Perplexity: 13.4698\n",
      "Epoch [2/3], Step [44100/138038], Loss: 3.2569, Perplexity: 25.9683\n",
      "Epoch [2/3], Step [44200/138038], Loss: 2.6251, Perplexity: 13.8061\n",
      "Epoch [2/3], Step [44300/138038], Loss: 2.9832, Perplexity: 19.75179\n",
      "Epoch [2/3], Step [44400/138038], Loss: 2.3438, Perplexity: 10.4207\n",
      "Epoch [2/3], Step [44500/138038], Loss: 3.3315, Perplexity: 27.9790\n",
      "Epoch [2/3], Step [44600/138038], Loss: 2.3577, Perplexity: 10.56657\n",
      "Epoch [2/3], Step [44700/138038], Loss: 2.3197, Perplexity: 10.17299\n",
      "Epoch [2/3], Step [44800/138038], Loss: 3.2067, Perplexity: 24.6982\n",
      "Epoch [2/3], Step [44900/138038], Loss: 3.0888, Perplexity: 21.9517\n",
      "Epoch [2/3], Step [45000/138038], Loss: 3.2915, Perplexity: 26.8824\n",
      "Epoch [2/3], Step [45100/138038], Loss: 2.4515, Perplexity: 11.60610\n",
      "Epoch [2/3], Step [45200/138038], Loss: 3.1106, Perplexity: 22.43531\n",
      "Epoch [2/3], Step [45300/138038], Loss: 2.7168, Perplexity: 15.1314\n",
      "Epoch [2/3], Step [45400/138038], Loss: 3.1159, Perplexity: 22.55434\n",
      "Epoch [2/3], Step [45500/138038], Loss: 3.4886, Perplexity: 32.7388\n",
      "Epoch [2/3], Step [45600/138038], Loss: 3.2668, Perplexity: 26.2278\n",
      "Epoch [2/3], Step [45700/138038], Loss: 1.8664, Perplexity: 6.465342\n",
      "Epoch [2/3], Step [45800/138038], Loss: 3.3095, Perplexity: 27.3712\n",
      "Epoch [2/3], Step [45900/138038], Loss: 2.8468, Perplexity: 17.2321\n",
      "Epoch [2/3], Step [46000/138038], Loss: 3.9944, Perplexity: 54.29090\n",
      "Epoch [2/3], Step [46100/138038], Loss: 2.2846, Perplexity: 9.82136\n",
      "Epoch [2/3], Step [46200/138038], Loss: 3.0552, Perplexity: 21.2247\n",
      "Epoch [2/3], Step [46300/138038], Loss: 1.7614, Perplexity: 5.82069\n",
      "Epoch [2/3], Step [46400/138038], Loss: 1.5430, Perplexity: 4.678819\n",
      "Epoch [2/3], Step [46500/138038], Loss: 3.0707, Perplexity: 21.55776\n",
      "Epoch [2/3], Step [46600/138038], Loss: 3.7257, Perplexity: 41.50165\n",
      "Epoch [2/3], Step [46700/138038], Loss: 2.9911, Perplexity: 19.90759\n",
      "Epoch [2/3], Step [46800/138038], Loss: 2.6730, Perplexity: 14.4830\n",
      "Epoch [2/3], Step [46900/138038], Loss: 1.9516, Perplexity: 7.04021\n",
      "Epoch [2/3], Step [47000/138038], Loss: 3.0788, Perplexity: 21.73182\n",
      "Epoch [2/3], Step [47100/138038], Loss: 3.1454, Perplexity: 23.2294\n",
      "Epoch [2/3], Step [47200/138038], Loss: 3.2269, Perplexity: 25.2021\n",
      "Epoch [2/3], Step [47300/138038], Loss: 1.7007, Perplexity: 5.47790\n",
      "Epoch [2/3], Step [47400/138038], Loss: 2.1877, Perplexity: 8.91508\n",
      "Epoch [2/3], Step [47500/138038], Loss: 1.8625, Perplexity: 6.44000\n",
      "Epoch [2/3], Step [47600/138038], Loss: 1.7322, Perplexity: 5.65301\n",
      "Epoch [2/3], Step [47700/138038], Loss: 2.3511, Perplexity: 10.4967\n",
      "Epoch [2/3], Step [47800/138038], Loss: 2.9207, Perplexity: 18.5536\n",
      "Epoch [2/3], Step [47900/138038], Loss: 3.9012, Perplexity: 49.46390\n",
      "Epoch [2/3], Step [48000/138038], Loss: 3.7898, Perplexity: 44.2479\n",
      "Epoch [2/3], Step [48100/138038], Loss: 2.7238, Perplexity: 15.2385\n",
      "Epoch [2/3], Step [48200/138038], Loss: 2.3658, Perplexity: 10.6529\n",
      "Epoch [2/3], Step [48300/138038], Loss: 4.6478, Perplexity: 104.3504\n",
      "Epoch [2/3], Step [48400/138038], Loss: 2.8768, Perplexity: 17.75735\n",
      "Epoch [2/3], Step [48500/138038], Loss: 2.5272, Perplexity: 12.5189\n",
      "Epoch [2/3], Step [48600/138038], Loss: 2.7087, Perplexity: 15.01036\n",
      "Epoch [2/3], Step [48700/138038], Loss: 2.9185, Perplexity: 18.5135\n",
      "Epoch [2/3], Step [48800/138038], Loss: 2.4770, Perplexity: 11.9060\n",
      "Epoch [2/3], Step [48900/138038], Loss: 3.2020, Perplexity: 24.58150\n",
      "Epoch [2/3], Step [49000/138038], Loss: 2.3582, Perplexity: 10.5714\n",
      "Epoch [2/3], Step [49100/138038], Loss: 2.9443, Perplexity: 18.9982\n",
      "Epoch [2/3], Step [49200/138038], Loss: 2.8827, Perplexity: 17.86168\n",
      "Epoch [2/3], Step [49300/138038], Loss: 3.4307, Perplexity: 30.8986\n",
      "Epoch [2/3], Step [49400/138038], Loss: 2.9172, Perplexity: 18.4896\n",
      "Epoch [2/3], Step [49500/138038], Loss: 3.1930, Perplexity: 24.36142\n",
      "Epoch [2/3], Step [49600/138038], Loss: 2.4474, Perplexity: 11.5580\n",
      "Epoch [2/3], Step [49700/138038], Loss: 2.8661, Perplexity: 17.5682\n",
      "Epoch [2/3], Step [49800/138038], Loss: 1.6222, Perplexity: 5.064219\n",
      "Epoch [2/3], Step [49900/138038], Loss: 2.1551, Perplexity: 8.628888\n",
      "Epoch [2/3], Step [50000/138038], Loss: 3.0372, Perplexity: 20.84754\n",
      "Epoch [2/3], Step [50100/138038], Loss: 2.4644, Perplexity: 11.75665\n",
      "Epoch [2/3], Step [50200/138038], Loss: 2.9137, Perplexity: 18.42458\n",
      "Epoch [2/3], Step [50300/138038], Loss: 3.4185, Perplexity: 30.52231\n",
      "Epoch [2/3], Step [50400/138038], Loss: 3.9655, Perplexity: 52.7458\n",
      "Epoch [2/3], Step [50500/138038], Loss: 1.7641, Perplexity: 5.83618\n",
      "Epoch [2/3], Step [50600/138038], Loss: 2.5364, Perplexity: 12.63380\n",
      "Epoch [2/3], Step [50700/138038], Loss: 1.8637, Perplexity: 6.44750\n",
      "Epoch [2/3], Step [50800/138038], Loss: 3.1434, Perplexity: 23.1819\n",
      "Epoch [2/3], Step [50900/138038], Loss: 2.4315, Perplexity: 11.3757\n",
      "Epoch [2/3], Step [51000/138038], Loss: 2.1438, Perplexity: 8.532191\n",
      "Epoch [2/3], Step [51100/138038], Loss: 2.9330, Perplexity: 18.7843\n",
      "Epoch [2/3], Step [51200/138038], Loss: 2.2908, Perplexity: 9.882994\n",
      "Epoch [2/3], Step [51300/138038], Loss: 3.8877, Perplexity: 48.7992\n",
      "Epoch [2/3], Step [51400/138038], Loss: 1.6314, Perplexity: 5.11117\n",
      "Epoch [2/3], Step [51500/138038], Loss: 2.9387, Perplexity: 18.89042\n",
      "Epoch [2/3], Step [51600/138038], Loss: 1.9309, Perplexity: 6.89609\n",
      "Epoch [2/3], Step [51700/138038], Loss: 1.8503, Perplexity: 6.362084\n",
      "Epoch [2/3], Step [51800/138038], Loss: 2.1550, Perplexity: 8.62832\n",
      "Epoch [2/3], Step [51900/138038], Loss: 2.8664, Perplexity: 17.5729\n",
      "Epoch [2/3], Step [52000/138038], Loss: 3.2027, Perplexity: 24.5992\n",
      "Epoch [2/3], Step [52100/138038], Loss: 3.1892, Perplexity: 24.2685\n",
      "Epoch [2/3], Step [52200/138038], Loss: 2.7465, Perplexity: 15.58813\n",
      "Epoch [2/3], Step [52300/138038], Loss: 2.2090, Perplexity: 9.10648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [52400/138038], Loss: 3.0091, Perplexity: 20.2701\n",
      "Epoch [2/3], Step [52500/138038], Loss: 3.1870, Perplexity: 24.21514\n",
      "Epoch [2/3], Step [52600/138038], Loss: 2.1860, Perplexity: 8.899666\n",
      "Epoch [2/3], Step [52700/138038], Loss: 3.0594, Perplexity: 21.31395\n",
      "Epoch [2/3], Step [52800/138038], Loss: 2.4490, Perplexity: 11.5768\n",
      "Epoch [2/3], Step [52900/138038], Loss: 3.5409, Perplexity: 34.4995\n",
      "Epoch [2/3], Step [53000/138038], Loss: 3.5407, Perplexity: 34.49018\n",
      "Epoch [2/3], Step [53100/138038], Loss: 3.4144, Perplexity: 30.39914\n",
      "Epoch [2/3], Step [53200/138038], Loss: 4.1429, Perplexity: 62.98615\n",
      "Epoch [2/3], Step [53300/138038], Loss: 2.0595, Perplexity: 7.84178\n",
      "Epoch [2/3], Step [53400/138038], Loss: 3.3678, Perplexity: 29.0154\n",
      "Epoch [2/3], Step [53500/138038], Loss: 3.5726, Perplexity: 35.6105\n",
      "Epoch [2/3], Step [53600/138038], Loss: 2.8647, Perplexity: 17.5444\n",
      "Epoch [2/3], Step [53700/138038], Loss: 1.8044, Perplexity: 6.07611\n",
      "Epoch [2/3], Step [53800/138038], Loss: 3.1643, Perplexity: 23.67320\n",
      "Epoch [2/3], Step [53900/138038], Loss: 3.7921, Perplexity: 44.3488\n",
      "Epoch [2/3], Step [54000/138038], Loss: 2.0175, Perplexity: 7.51926\n",
      "Epoch [2/3], Step [54100/138038], Loss: 2.6152, Perplexity: 13.66961\n",
      "Epoch [2/3], Step [54200/138038], Loss: 3.9452, Perplexity: 51.6882\n",
      "Epoch [2/3], Step [54300/138038], Loss: 2.7151, Perplexity: 15.1065\n",
      "Epoch [2/3], Step [54400/138038], Loss: 1.9989, Perplexity: 7.38117\n",
      "Epoch [2/3], Step [54500/138038], Loss: 3.6716, Perplexity: 39.31661\n",
      "Epoch [2/3], Step [54600/138038], Loss: 1.5735, Perplexity: 4.82367\n",
      "Epoch [2/3], Step [54700/138038], Loss: 2.8223, Perplexity: 16.81557\n",
      "Epoch [2/3], Step [54800/138038], Loss: 2.3825, Perplexity: 10.8317\n",
      "Epoch [2/3], Step [54900/138038], Loss: 2.8614, Perplexity: 17.48647\n",
      "Epoch [2/3], Step [55000/138038], Loss: 2.8149, Perplexity: 16.6912\n",
      "Epoch [2/3], Step [55100/138038], Loss: 2.0817, Perplexity: 8.01834\n",
      "Epoch [2/3], Step [55200/138038], Loss: 2.1462, Perplexity: 8.552040\n",
      "Epoch [2/3], Step [55300/138038], Loss: 3.2114, Perplexity: 24.8138\n",
      "Epoch [2/3], Step [55400/138038], Loss: 1.6574, Perplexity: 5.24574\n",
      "Epoch [2/3], Step [55500/138038], Loss: 4.4154, Perplexity: 82.7147\n",
      "Epoch [2/3], Step [55600/138038], Loss: 3.5570, Perplexity: 35.0595\n",
      "Epoch [2/3], Step [55700/138038], Loss: 2.9463, Perplexity: 19.03495\n",
      "Epoch [2/3], Step [55800/138038], Loss: 3.1952, Perplexity: 24.4148\n",
      "Epoch [2/3], Step [55900/138038], Loss: 3.0443, Perplexity: 20.9959\n",
      "Epoch [2/3], Step [56000/138038], Loss: 2.1340, Perplexity: 8.44849\n",
      "Epoch [2/3], Step [56100/138038], Loss: 3.2879, Perplexity: 26.7870\n",
      "Epoch [2/3], Step [56200/138038], Loss: 3.4140, Perplexity: 30.3877\n",
      "Epoch [2/3], Step [56300/138038], Loss: 3.3850, Perplexity: 29.51902\n",
      "Epoch [2/3], Step [56400/138038], Loss: 2.2469, Perplexity: 9.458312\n",
      "Epoch [2/3], Step [56500/138038], Loss: 2.7378, Perplexity: 15.45347\n",
      "Epoch [2/3], Step [56600/138038], Loss: 2.8627, Perplexity: 17.5081\n",
      "Epoch [2/3], Step [56700/138038], Loss: 1.9673, Perplexity: 7.151121\n",
      "Epoch [2/3], Step [56800/138038], Loss: 1.8627, Perplexity: 6.441117\n",
      "Epoch [2/3], Step [56900/138038], Loss: 1.8359, Perplexity: 6.27056\n",
      "Epoch [2/3], Step [57000/138038], Loss: 1.8448, Perplexity: 6.32709\n",
      "Epoch [2/3], Step [57100/138038], Loss: 2.7368, Perplexity: 15.43792\n",
      "Epoch [2/3], Step [57200/138038], Loss: 2.9545, Perplexity: 19.1929\n",
      "Epoch [2/3], Step [57300/138038], Loss: 3.2890, Perplexity: 26.81625\n",
      "Epoch [2/3], Step [57400/138038], Loss: 2.4545, Perplexity: 11.6410\n",
      "Epoch [2/3], Step [57500/138038], Loss: 3.5997, Perplexity: 36.58741\n",
      "Epoch [2/3], Step [57600/138038], Loss: 2.6321, Perplexity: 13.9026\n",
      "Epoch [2/3], Step [57700/138038], Loss: 3.5876, Perplexity: 36.1479\n",
      "Epoch [2/3], Step [57800/138038], Loss: 2.4731, Perplexity: 11.85889\n",
      "Epoch [2/3], Step [57900/138038], Loss: 2.9457, Perplexity: 19.0232\n",
      "Epoch [2/3], Step [58000/138038], Loss: 1.6589, Perplexity: 5.25358\n",
      "Epoch [2/3], Step [58100/138038], Loss: 2.6024, Perplexity: 13.4955\n",
      "Epoch [2/3], Step [58200/138038], Loss: 3.1603, Perplexity: 23.57780\n",
      "Epoch [2/3], Step [58300/138038], Loss: 4.4046, Perplexity: 81.82602\n",
      "Epoch [2/3], Step [58400/138038], Loss: 2.2060, Perplexity: 9.07939\n",
      "Epoch [2/3], Step [58500/138038], Loss: 2.5942, Perplexity: 13.38569\n",
      "Epoch [2/3], Step [58600/138038], Loss: 2.5421, Perplexity: 12.7062\n",
      "Epoch [2/3], Step [58700/138038], Loss: 3.1805, Perplexity: 24.0599\n",
      "Epoch [2/3], Step [58800/138038], Loss: 1.6975, Perplexity: 5.46032\n",
      "Epoch [2/3], Step [58900/138038], Loss: 2.3487, Perplexity: 10.47188\n",
      "Epoch [2/3], Step [59000/138038], Loss: 2.1048, Perplexity: 8.205487\n",
      "Epoch [2/3], Step [59100/138038], Loss: 2.8359, Perplexity: 17.04649\n",
      "Epoch [2/3], Step [59200/138038], Loss: 4.3160, Perplexity: 74.88789\n",
      "Epoch [2/3], Step [59300/138038], Loss: 1.6362, Perplexity: 5.13545\n",
      "Epoch [2/3], Step [59400/138038], Loss: 2.3449, Perplexity: 10.43180\n",
      "Epoch [2/3], Step [59500/138038], Loss: 3.2198, Perplexity: 25.0227\n",
      "Epoch [2/3], Step [59600/138038], Loss: 2.7950, Perplexity: 16.3619\n",
      "Epoch [2/3], Step [59700/138038], Loss: 1.6496, Perplexity: 5.205232\n",
      "Epoch [2/3], Step [59800/138038], Loss: 2.7489, Perplexity: 15.62587\n",
      "Epoch [2/3], Step [59900/138038], Loss: 2.6088, Perplexity: 13.5832\n",
      "Epoch [2/3], Step [60000/138038], Loss: 1.9590, Perplexity: 7.092537\n",
      "Epoch [2/3], Step [60100/138038], Loss: 2.4035, Perplexity: 11.06144\n",
      "Epoch [2/3], Step [60200/138038], Loss: 2.0389, Perplexity: 7.682592\n",
      "Epoch [2/3], Step [60300/138038], Loss: 2.2115, Perplexity: 9.129740\n",
      "Epoch [2/3], Step [60400/138038], Loss: 2.7329, Perplexity: 15.37769\n",
      "Epoch [2/3], Step [60500/138038], Loss: 3.1623, Perplexity: 23.6243\n",
      "Epoch [2/3], Step [60600/138038], Loss: 2.1272, Perplexity: 8.391204\n",
      "Epoch [2/3], Step [60700/138038], Loss: 2.3618, Perplexity: 10.6098\n",
      "Epoch [2/3], Step [60800/138038], Loss: 2.2607, Perplexity: 9.58954\n",
      "Epoch [2/3], Step [60900/138038], Loss: 2.9494, Perplexity: 19.0942\n",
      "Epoch [2/3], Step [61000/138038], Loss: 3.1865, Perplexity: 24.2032\n",
      "Epoch [2/3], Step [61100/138038], Loss: 2.5696, Perplexity: 13.0603\n",
      "Epoch [2/3], Step [61200/138038], Loss: 2.1630, Perplexity: 8.69703\n",
      "Epoch [2/3], Step [61300/138038], Loss: 4.2107, Perplexity: 67.40472\n",
      "Epoch [2/3], Step [61400/138038], Loss: 2.1307, Perplexity: 8.42089\n",
      "Epoch [2/3], Step [61500/138038], Loss: 3.5628, Perplexity: 35.2627\n",
      "Epoch [2/3], Step [61600/138038], Loss: 2.6380, Perplexity: 13.98594\n",
      "Epoch [2/3], Step [61700/138038], Loss: 3.0597, Perplexity: 21.3222\n",
      "Epoch [2/3], Step [61800/138038], Loss: 2.8678, Perplexity: 17.5985\n",
      "Epoch [2/3], Step [61900/138038], Loss: 2.5549, Perplexity: 12.8703\n",
      "Epoch [2/3], Step [62000/138038], Loss: 3.7388, Perplexity: 42.0472\n",
      "Epoch [2/3], Step [62100/138038], Loss: 3.9960, Perplexity: 54.37792\n",
      "Epoch [2/3], Step [62200/138038], Loss: 2.8043, Perplexity: 16.5149\n",
      "Epoch [2/3], Step [62300/138038], Loss: 2.4503, Perplexity: 11.5916\n",
      "Epoch [2/3], Step [62400/138038], Loss: 2.7320, Perplexity: 15.3639\n",
      "Epoch [2/3], Step [62500/138038], Loss: 3.0118, Perplexity: 20.3249\n",
      "Epoch [2/3], Step [62600/138038], Loss: 2.1450, Perplexity: 8.54225\n",
      "Epoch [2/3], Step [62700/138038], Loss: 2.6822, Perplexity: 14.6175\n",
      "Epoch [2/3], Step [62800/138038], Loss: 2.8385, Perplexity: 17.09041\n",
      "Epoch [2/3], Step [62900/138038], Loss: 1.9633, Perplexity: 7.12270\n",
      "Epoch [2/3], Step [63000/138038], Loss: 2.8769, Perplexity: 17.7583\n",
      "Epoch [2/3], Step [63100/138038], Loss: 3.0250, Perplexity: 20.5931\n",
      "Epoch [2/3], Step [63200/138038], Loss: 2.9547, Perplexity: 19.19667\n",
      "Epoch [2/3], Step [63300/138038], Loss: 2.6368, Perplexity: 13.96852\n",
      "Epoch [2/3], Step [63400/138038], Loss: 1.7722, Perplexity: 5.88392\n",
      "Epoch [2/3], Step [63500/138038], Loss: 2.3582, Perplexity: 10.57150\n",
      "Epoch [2/3], Step [63600/138038], Loss: 3.1341, Perplexity: 22.96881\n",
      "Epoch [2/3], Step [63700/138038], Loss: 2.9526, Perplexity: 19.1551\n",
      "Epoch [2/3], Step [63800/138038], Loss: 2.6390, Perplexity: 13.99983\n",
      "Epoch [2/3], Step [63900/138038], Loss: 1.7283, Perplexity: 5.63090\n",
      "Epoch [2/3], Step [64000/138038], Loss: 1.9178, Perplexity: 6.80632\n",
      "Epoch [2/3], Step [64100/138038], Loss: 1.8821, Perplexity: 6.567168\n",
      "Epoch [2/3], Step [64200/138038], Loss: 2.8153, Perplexity: 16.6983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [64300/138038], Loss: 3.1843, Perplexity: 24.1505\n",
      "Epoch [2/3], Step [64400/138038], Loss: 2.5833, Perplexity: 13.2406\n",
      "Epoch [2/3], Step [64500/138038], Loss: 3.7468, Perplexity: 42.3854\n",
      "Epoch [2/3], Step [64600/138038], Loss: 2.2878, Perplexity: 9.85360\n",
      "Epoch [2/3], Step [64700/138038], Loss: 3.2545, Perplexity: 25.9056\n",
      "Epoch [2/3], Step [64800/138038], Loss: 2.7060, Perplexity: 14.9699\n",
      "Epoch [2/3], Step [64900/138038], Loss: 2.5114, Perplexity: 12.3223\n",
      "Epoch [2/3], Step [65000/138038], Loss: 2.7384, Perplexity: 15.4618\n",
      "Epoch [2/3], Step [65100/138038], Loss: 3.3509, Perplexity: 28.52794\n",
      "Epoch [2/3], Step [65200/138038], Loss: 2.5825, Perplexity: 13.23042\n",
      "Epoch [2/3], Step [65300/138038], Loss: 2.6034, Perplexity: 13.5101\n",
      "Epoch [2/3], Step [65400/138038], Loss: 2.3100, Perplexity: 10.0746\n",
      "Epoch [2/3], Step [65500/138038], Loss: 3.1881, Perplexity: 24.24122\n",
      "Epoch [2/3], Step [65600/138038], Loss: 2.1781, Perplexity: 8.829627\n",
      "Epoch [2/3], Step [65700/138038], Loss: 4.1626, Perplexity: 64.2383\n",
      "Epoch [2/3], Step [65800/138038], Loss: 2.7153, Perplexity: 15.1086\n",
      "Epoch [2/3], Step [65900/138038], Loss: 4.4573, Perplexity: 86.2536\n",
      "Epoch [2/3], Step [66000/138038], Loss: 2.8541, Perplexity: 17.3582\n",
      "Epoch [2/3], Step [66100/138038], Loss: 2.7776, Perplexity: 16.0800\n",
      "Epoch [2/3], Step [66200/138038], Loss: 2.1619, Perplexity: 8.68776\n",
      "Epoch [2/3], Step [66300/138038], Loss: 1.9852, Perplexity: 7.28046\n",
      "Epoch [2/3], Step [66400/138038], Loss: 2.5403, Perplexity: 12.6833\n",
      "Epoch [2/3], Step [66500/138038], Loss: 2.0107, Perplexity: 7.46824\n",
      "Epoch [2/3], Step [66600/138038], Loss: 2.3058, Perplexity: 10.0320\n",
      "Epoch [2/3], Step [66700/138038], Loss: 2.4099, Perplexity: 11.1328\n",
      "Epoch [2/3], Step [66800/138038], Loss: 2.8264, Perplexity: 16.8844\n",
      "Epoch [2/3], Step [66900/138038], Loss: 3.6029, Perplexity: 36.70546\n",
      "Epoch [2/3], Step [67000/138038], Loss: 3.5630, Perplexity: 35.2677\n",
      "Epoch [2/3], Step [67100/138038], Loss: 2.1063, Perplexity: 8.21756\n",
      "Epoch [2/3], Step [67200/138038], Loss: 2.6125, Perplexity: 13.6329\n",
      "Epoch [2/3], Step [67300/138038], Loss: 3.5574, Perplexity: 35.0732\n",
      "Epoch [2/3], Step [67400/138038], Loss: 3.1637, Perplexity: 23.65860\n",
      "Epoch [2/3], Step [67500/138038], Loss: 1.8005, Perplexity: 6.05295\n",
      "Epoch [2/3], Step [67600/138038], Loss: 2.5687, Perplexity: 13.0485\n",
      "Epoch [2/3], Step [67700/138038], Loss: 2.7953, Perplexity: 16.3671\n",
      "Epoch [2/3], Step [67800/138038], Loss: 2.9888, Perplexity: 19.8628\n",
      "Epoch [2/3], Step [67900/138038], Loss: 2.9267, Perplexity: 18.66610\n",
      "Epoch [2/3], Step [68000/138038], Loss: 4.2034, Perplexity: 66.91496\n",
      "Epoch [2/3], Step [68100/138038], Loss: 4.2818, Perplexity: 72.36729\n",
      "Epoch [2/3], Step [68200/138038], Loss: 2.4079, Perplexity: 11.1106\n",
      "Epoch [2/3], Step [68300/138038], Loss: 2.0472, Perplexity: 7.746547\n",
      "Epoch [2/3], Step [68400/138038], Loss: 2.1562, Perplexity: 8.63786\n",
      "Epoch [2/3], Step [68500/138038], Loss: 2.6670, Perplexity: 14.3967\n",
      "Epoch [2/3], Step [68600/138038], Loss: 1.9421, Perplexity: 6.97372\n",
      "Epoch [2/3], Step [68700/138038], Loss: 2.8665, Perplexity: 17.5748\n",
      "Epoch [2/3], Step [68800/138038], Loss: 1.8928, Perplexity: 6.637982\n",
      "Epoch [2/3], Step [68900/138038], Loss: 2.6389, Perplexity: 13.9974\n",
      "Epoch [2/3], Step [69000/138038], Loss: 3.0171, Perplexity: 20.43113\n",
      "Epoch [2/3], Step [69100/138038], Loss: 2.4505, Perplexity: 11.59374\n",
      "Epoch [2/3], Step [69200/138038], Loss: 2.4840, Perplexity: 11.98906\n",
      "Epoch [2/3], Step [69300/138038], Loss: 3.3720, Perplexity: 29.1380\n",
      "Epoch [2/3], Step [69400/138038], Loss: 2.4715, Perplexity: 11.8398\n",
      "Epoch [2/3], Step [69500/138038], Loss: 2.9504, Perplexity: 19.1135\n",
      "Epoch [2/3], Step [69600/138038], Loss: 2.2462, Perplexity: 9.45148\n",
      "Epoch [2/3], Step [69700/138038], Loss: 2.9642, Perplexity: 19.3800\n",
      "Epoch [2/3], Step [69800/138038], Loss: 2.9537, Perplexity: 19.1768\n",
      "Epoch [2/3], Step [69900/138038], Loss: 2.4936, Perplexity: 12.10470\n",
      "Epoch [2/3], Step [70000/138038], Loss: 2.7349, Perplexity: 15.40804\n",
      "Epoch [2/3], Step [70100/138038], Loss: 2.1171, Perplexity: 8.30709\n",
      "Epoch [2/3], Step [70200/138038], Loss: 2.1211, Perplexity: 8.34054\n",
      "Epoch [2/3], Step [70300/138038], Loss: 2.4412, Perplexity: 11.4870\n",
      "Epoch [2/3], Step [70400/138038], Loss: 2.0894, Perplexity: 8.07995\n",
      "Epoch [2/3], Step [70500/138038], Loss: 2.9787, Perplexity: 19.6629\n",
      "Epoch [2/3], Step [70600/138038], Loss: 2.3611, Perplexity: 10.60247\n",
      "Epoch [2/3], Step [70700/138038], Loss: 2.3919, Perplexity: 10.9337\n",
      "Epoch [2/3], Step [70800/138038], Loss: 2.5790, Perplexity: 13.1841\n",
      "Epoch [2/3], Step [70900/138038], Loss: 2.3195, Perplexity: 10.1710\n",
      "Epoch [2/3], Step [71000/138038], Loss: 1.8677, Perplexity: 6.47326\n",
      "Epoch [2/3], Step [71100/138038], Loss: 3.2079, Perplexity: 24.7266\n",
      "Epoch [2/3], Step [71200/138038], Loss: 2.8533, Perplexity: 17.34525\n",
      "Epoch [2/3], Step [71300/138038], Loss: 4.0251, Perplexity: 55.9835\n",
      "Epoch [2/3], Step [71400/138038], Loss: 3.4559, Perplexity: 31.6866\n",
      "Epoch [2/3], Step [71500/138038], Loss: 2.9831, Perplexity: 19.74873\n",
      "Epoch [2/3], Step [71600/138038], Loss: 2.6954, Perplexity: 14.81100\n",
      "Epoch [2/3], Step [71700/138038], Loss: 3.5448, Perplexity: 34.63274\n",
      "Epoch [2/3], Step [71800/138038], Loss: 3.0962, Perplexity: 22.1139\n",
      "Epoch [2/3], Step [71900/138038], Loss: 2.3638, Perplexity: 10.6308\n",
      "Epoch [2/3], Step [72000/138038], Loss: 2.5451, Perplexity: 12.74458\n",
      "Epoch [2/3], Step [72100/138038], Loss: 1.8620, Perplexity: 6.43698\n",
      "Epoch [2/3], Step [72200/138038], Loss: 2.4762, Perplexity: 11.89601\n",
      "Epoch [2/3], Step [72300/138038], Loss: 2.8301, Perplexity: 16.94789\n",
      "Epoch [2/3], Step [72400/138038], Loss: 2.3990, Perplexity: 11.0117\n",
      "Epoch [2/3], Step [72500/138038], Loss: 2.4628, Perplexity: 11.7378\n",
      "Epoch [2/3], Step [72600/138038], Loss: 2.9489, Perplexity: 19.0845\n",
      "Epoch [2/3], Step [72700/138038], Loss: 2.4958, Perplexity: 12.13097\n",
      "Epoch [2/3], Step [72800/138038], Loss: 1.3314, Perplexity: 3.78642\n",
      "Epoch [2/3], Step [72900/138038], Loss: 2.9255, Perplexity: 18.64407\n",
      "Epoch [2/3], Step [73000/138038], Loss: 2.2412, Perplexity: 9.40455\n",
      "Epoch [2/3], Step [73100/138038], Loss: 3.2112, Perplexity: 24.8076\n",
      "Epoch [2/3], Step [73200/138038], Loss: 2.5241, Perplexity: 12.47919\n",
      "Epoch [2/3], Step [73300/138038], Loss: 3.7375, Perplexity: 41.99198\n",
      "Epoch [2/3], Step [73400/138038], Loss: 3.5323, Perplexity: 34.2035\n",
      "Epoch [2/3], Step [73500/138038], Loss: 2.8907, Perplexity: 18.0056\n",
      "Epoch [2/3], Step [73600/138038], Loss: 2.6920, Perplexity: 14.7608\n",
      "Epoch [2/3], Step [73700/138038], Loss: 3.1831, Perplexity: 24.1210\n",
      "Epoch [2/3], Step [73800/138038], Loss: 4.8983, Perplexity: 134.0611\n",
      "Epoch [2/3], Step [73900/138038], Loss: 3.2548, Perplexity: 25.91541\n",
      "Epoch [2/3], Step [74000/138038], Loss: 3.3755, Perplexity: 29.2389\n",
      "Epoch [2/3], Step [74100/138038], Loss: 1.5963, Perplexity: 4.93489\n",
      "Epoch [2/3], Step [74200/138038], Loss: 3.3087, Perplexity: 27.3495\n",
      "Epoch [2/3], Step [74300/138038], Loss: 2.6879, Perplexity: 14.7010\n",
      "Epoch [2/3], Step [74400/138038], Loss: 1.7298, Perplexity: 5.63962\n",
      "Epoch [2/3], Step [74500/138038], Loss: 3.0988, Perplexity: 22.1723\n",
      "Epoch [2/3], Step [74600/138038], Loss: 2.4769, Perplexity: 11.9044\n",
      "Epoch [2/3], Step [74700/138038], Loss: 2.6720, Perplexity: 14.46897\n",
      "Epoch [2/3], Step [74800/138038], Loss: 2.3175, Perplexity: 10.1508\n",
      "Epoch [2/3], Step [74900/138038], Loss: 3.0435, Perplexity: 20.97955\n",
      "Epoch [2/3], Step [75000/138038], Loss: 2.5178, Perplexity: 12.40143\n",
      "Epoch [2/3], Step [75100/138038], Loss: 2.7242, Perplexity: 15.2443\n",
      "Epoch [2/3], Step [75200/138038], Loss: 2.6798, Perplexity: 14.58156\n",
      "Epoch [2/3], Step [75300/138038], Loss: 1.9364, Perplexity: 6.933940\n",
      "Epoch [2/3], Step [75400/138038], Loss: 4.2725, Perplexity: 71.69815\n",
      "Epoch [2/3], Step [75500/138038], Loss: 3.6973, Perplexity: 40.33835\n",
      "Epoch [2/3], Step [75600/138038], Loss: 2.6458, Perplexity: 14.09440\n",
      "Epoch [2/3], Step [75700/138038], Loss: 2.8288, Perplexity: 16.9251\n",
      "Epoch [2/3], Step [75800/138038], Loss: 2.4067, Perplexity: 11.0977\n",
      "Epoch [2/3], Step [75900/138038], Loss: 3.1387, Perplexity: 23.07385\n",
      "Epoch [2/3], Step [76000/138038], Loss: 2.8434, Perplexity: 17.1749\n",
      "Epoch [2/3], Step [76100/138038], Loss: 5.3159, Perplexity: 203.5432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [76200/138038], Loss: 2.8745, Perplexity: 17.7160\n",
      "Epoch [2/3], Step [76300/138038], Loss: 2.3160, Perplexity: 10.13504\n",
      "Epoch [2/3], Step [76400/138038], Loss: 2.8712, Perplexity: 17.6583\n",
      "Epoch [2/3], Step [76500/138038], Loss: 3.8064, Perplexity: 44.9872\n",
      "Epoch [2/3], Step [76600/138038], Loss: 3.5171, Perplexity: 33.68682\n",
      "Epoch [2/3], Step [76700/138038], Loss: 1.6824, Perplexity: 5.378634\n",
      "Epoch [2/3], Step [76800/138038], Loss: 2.9470, Perplexity: 19.0480\n",
      "Epoch [2/3], Step [76900/138038], Loss: 4.0745, Perplexity: 58.8229\n",
      "Epoch [2/3], Step [77000/138038], Loss: 1.7872, Perplexity: 5.97258\n",
      "Epoch [2/3], Step [77100/138038], Loss: 2.5804, Perplexity: 13.20319\n",
      "Epoch [2/3], Step [77200/138038], Loss: 1.7502, Perplexity: 5.75552\n",
      "Epoch [2/3], Step [77300/138038], Loss: 1.7115, Perplexity: 5.53737\n",
      "Epoch [2/3], Step [77400/138038], Loss: 2.8209, Perplexity: 16.79154\n",
      "Epoch [2/3], Step [77500/138038], Loss: 2.7382, Perplexity: 15.45872\n",
      "Epoch [2/3], Step [77600/138038], Loss: 3.5002, Perplexity: 33.12150\n",
      "Epoch [2/3], Step [77700/138038], Loss: 2.4721, Perplexity: 11.8475\n",
      "Epoch [2/3], Step [77800/138038], Loss: 2.8066, Perplexity: 16.5542\n",
      "Epoch [2/3], Step [77900/138038], Loss: 2.0986, Perplexity: 8.15496\n",
      "Epoch [2/3], Step [78000/138038], Loss: 1.8706, Perplexity: 6.492498\n",
      "Epoch [2/3], Step [78100/138038], Loss: 3.1349, Perplexity: 22.9854\n",
      "Epoch [2/3], Step [78200/138038], Loss: 2.6177, Perplexity: 13.70403\n",
      "Epoch [2/3], Step [78300/138038], Loss: 1.7908, Perplexity: 5.99410\n",
      "Epoch [2/3], Step [78400/138038], Loss: 3.3103, Perplexity: 27.3935\n",
      "Epoch [2/3], Step [78500/138038], Loss: 2.0456, Perplexity: 7.73362\n",
      "Epoch [2/3], Step [78600/138038], Loss: 2.6054, Perplexity: 13.5371\n",
      "Epoch [2/3], Step [78700/138038], Loss: 3.2216, Perplexity: 25.0671\n",
      "Epoch [2/3], Step [78800/138038], Loss: 2.5149, Perplexity: 12.3655\n",
      "Epoch [2/3], Step [78900/138038], Loss: 3.1026, Perplexity: 22.25598\n",
      "Epoch [2/3], Step [79000/138038], Loss: 1.8149, Perplexity: 6.14043\n",
      "Epoch [2/3], Step [79100/138038], Loss: 2.1404, Perplexity: 8.502885\n",
      "Epoch [2/3], Step [79200/138038], Loss: 2.3105, Perplexity: 10.0795\n",
      "Epoch [2/3], Step [79300/138038], Loss: 2.7344, Perplexity: 15.4007\n",
      "Epoch [2/3], Step [79400/138038], Loss: 4.0253, Perplexity: 55.99942\n",
      "Epoch [2/3], Step [79500/138038], Loss: 2.6650, Perplexity: 14.36751\n",
      "Epoch [2/3], Step [79600/138038], Loss: 2.8538, Perplexity: 17.35320\n",
      "Epoch [2/3], Step [79700/138038], Loss: 1.8700, Perplexity: 6.48859\n",
      "Epoch [2/3], Step [79800/138038], Loss: 2.2696, Perplexity: 9.67560\n",
      "Epoch [2/3], Step [79900/138038], Loss: 2.8170, Perplexity: 16.7265\n",
      "Epoch [2/3], Step [80000/138038], Loss: 2.6540, Perplexity: 14.2105\n",
      "Epoch [2/3], Step [80100/138038], Loss: 3.4225, Perplexity: 30.64496\n",
      "Epoch [2/3], Step [80200/138038], Loss: 2.9861, Perplexity: 19.8087\n",
      "Epoch [2/3], Step [80300/138038], Loss: 2.6263, Perplexity: 13.8224\n",
      "Epoch [2/3], Step [80400/138038], Loss: 2.6490, Perplexity: 14.1392\n",
      "Epoch [2/3], Step [80500/138038], Loss: 2.4684, Perplexity: 11.80332\n",
      "Epoch [2/3], Step [80600/138038], Loss: 2.1253, Perplexity: 8.375813\n",
      "Epoch [2/3], Step [80700/138038], Loss: 2.0890, Perplexity: 8.077186\n",
      "Epoch [2/3], Step [80800/138038], Loss: 2.8955, Perplexity: 18.0923\n",
      "Epoch [2/3], Step [80900/138038], Loss: 2.4006, Perplexity: 11.0300\n",
      "Epoch [2/3], Step [81000/138038], Loss: 2.2461, Perplexity: 9.45054\n",
      "Epoch [2/3], Step [81100/138038], Loss: 1.7835, Perplexity: 5.95065\n",
      "Epoch [2/3], Step [81200/138038], Loss: 2.0831, Perplexity: 8.02973\n",
      "Epoch [2/3], Step [81300/138038], Loss: 2.2513, Perplexity: 9.50048\n",
      "Epoch [2/3], Step [81400/138038], Loss: 3.5758, Perplexity: 35.72491\n",
      "Epoch [2/3], Step [81500/138038], Loss: 2.1791, Perplexity: 8.83872\n",
      "Epoch [2/3], Step [81600/138038], Loss: 3.1324, Perplexity: 22.92902\n",
      "Epoch [2/3], Step [81700/138038], Loss: 2.3772, Perplexity: 10.77521\n",
      "Epoch [2/3], Step [81800/138038], Loss: 3.3449, Perplexity: 28.3590\n",
      "Epoch [2/3], Step [81900/138038], Loss: 2.4843, Perplexity: 11.9933\n",
      "Epoch [2/3], Step [82000/138038], Loss: 2.9539, Perplexity: 19.1812\n",
      "Epoch [2/3], Step [82100/138038], Loss: 3.1603, Perplexity: 23.5766\n",
      "Epoch [2/3], Step [82200/138038], Loss: 3.0112, Perplexity: 20.3116\n",
      "Epoch [2/3], Step [82300/138038], Loss: 2.2584, Perplexity: 9.56776\n",
      "Epoch [2/3], Step [82400/138038], Loss: 2.4810, Perplexity: 11.9537\n",
      "Epoch [2/3], Step [82500/138038], Loss: 3.6617, Perplexity: 38.9278\n",
      "Epoch [2/3], Step [82600/138038], Loss: 2.6572, Perplexity: 14.2564\n",
      "Epoch [2/3], Step [82700/138038], Loss: 3.2565, Perplexity: 25.9585\n",
      "Epoch [2/3], Step [82800/138038], Loss: 2.4559, Perplexity: 11.6566\n",
      "Epoch [2/3], Step [82900/138038], Loss: 2.4980, Perplexity: 12.15795\n",
      "Epoch [2/3], Step [83000/138038], Loss: 3.7868, Perplexity: 44.1156\n",
      "Epoch [2/3], Step [83100/138038], Loss: 3.5935, Perplexity: 36.3598\n",
      "Epoch [2/3], Step [83200/138038], Loss: 2.2556, Perplexity: 9.54089\n",
      "Epoch [2/3], Step [83300/138038], Loss: 2.6935, Perplexity: 14.7835\n",
      "Epoch [2/3], Step [83400/138038], Loss: 4.0032, Perplexity: 54.7714\n",
      "Epoch [2/3], Step [83500/138038], Loss: 2.9522, Perplexity: 19.14838\n",
      "Epoch [2/3], Step [83600/138038], Loss: 2.9025, Perplexity: 18.22009\n",
      "Epoch [2/3], Step [83700/138038], Loss: 2.3954, Perplexity: 10.97228\n",
      "Epoch [2/3], Step [83800/138038], Loss: 2.3715, Perplexity: 10.7131\n",
      "Epoch [2/3], Step [83900/138038], Loss: 1.6192, Perplexity: 5.04905\n",
      "Epoch [2/3], Step [84000/138038], Loss: 3.6744, Perplexity: 39.42480\n",
      "Epoch [2/3], Step [84100/138038], Loss: 2.5290, Perplexity: 12.54095\n",
      "Epoch [2/3], Step [84200/138038], Loss: 2.0387, Perplexity: 7.680675\n",
      "Epoch [2/3], Step [84300/138038], Loss: 1.5346, Perplexity: 4.63931\n",
      "Epoch [2/3], Step [84400/138038], Loss: 2.7847, Perplexity: 16.1946\n",
      "Epoch [2/3], Step [84500/138038], Loss: 2.2671, Perplexity: 9.65141\n",
      "Epoch [2/3], Step [84600/138038], Loss: 2.0108, Perplexity: 7.46943\n",
      "Epoch [2/3], Step [84700/138038], Loss: 2.0777, Perplexity: 7.98584\n",
      "Epoch [2/3], Step [84800/138038], Loss: 2.5091, Perplexity: 12.29333\n",
      "Epoch [2/3], Step [84900/138038], Loss: 2.6213, Perplexity: 13.75399\n",
      "Epoch [2/3], Step [85000/138038], Loss: 2.6827, Perplexity: 14.62447\n",
      "Epoch [2/3], Step [85100/138038], Loss: 3.4419, Perplexity: 31.2456\n",
      "Epoch [2/3], Step [85200/138038], Loss: 3.4129, Perplexity: 30.3524\n",
      "Epoch [2/3], Step [85300/138038], Loss: 2.5704, Perplexity: 13.0715\n",
      "Epoch [2/3], Step [85400/138038], Loss: 2.9998, Perplexity: 20.0823\n",
      "Epoch [2/3], Step [85500/138038], Loss: 2.2908, Perplexity: 9.883229\n",
      "Epoch [2/3], Step [85600/138038], Loss: 3.3055, Perplexity: 27.2627\n",
      "Epoch [2/3], Step [85700/138038], Loss: 2.4359, Perplexity: 11.4263\n",
      "Epoch [2/3], Step [85800/138038], Loss: 1.5334, Perplexity: 4.63406\n",
      "Epoch [2/3], Step [85900/138038], Loss: 2.6088, Perplexity: 13.5828\n",
      "Epoch [2/3], Step [86000/138038], Loss: 2.6233, Perplexity: 13.78104\n",
      "Epoch [2/3], Step [86100/138038], Loss: 2.7014, Perplexity: 14.9004\n",
      "Epoch [2/3], Step [86200/138038], Loss: 2.6819, Perplexity: 14.6126\n",
      "Epoch [2/3], Step [86300/138038], Loss: 2.9626, Perplexity: 19.3474\n",
      "Epoch [2/3], Step [86400/138038], Loss: 2.5894, Perplexity: 13.32166\n",
      "Epoch [2/3], Step [86500/138038], Loss: 4.4582, Perplexity: 86.3283\n",
      "Epoch [2/3], Step [86600/138038], Loss: 4.1297, Perplexity: 62.15718\n",
      "Epoch [2/3], Step [86700/138038], Loss: 3.8321, Perplexity: 46.15978\n",
      "Epoch [2/3], Step [86800/138038], Loss: 2.6312, Perplexity: 13.8911\n",
      "Epoch [2/3], Step [86900/138038], Loss: 3.3657, Perplexity: 28.95452\n",
      "Epoch [2/3], Step [87000/138038], Loss: 2.5797, Perplexity: 13.1934\n",
      "Epoch [2/3], Step [87100/138038], Loss: 1.6213, Perplexity: 5.05967\n",
      "Epoch [2/3], Step [87200/138038], Loss: 2.8543, Perplexity: 17.3626\n",
      "Epoch [2/3], Step [87300/138038], Loss: 2.8959, Perplexity: 18.10064\n",
      "Epoch [2/3], Step [87400/138038], Loss: 1.8214, Perplexity: 6.180268\n",
      "Epoch [2/3], Step [87500/138038], Loss: 2.8429, Perplexity: 17.1657\n",
      "Epoch [2/3], Step [87600/138038], Loss: 2.0372, Perplexity: 7.66940\n",
      "Epoch [2/3], Step [87700/138038], Loss: 1.8563, Perplexity: 6.40029\n",
      "Epoch [2/3], Step [87800/138038], Loss: 3.7039, Perplexity: 40.60663\n",
      "Epoch [2/3], Step [87900/138038], Loss: 3.0838, Perplexity: 21.84032\n",
      "Epoch [2/3], Step [88000/138038], Loss: 2.3955, Perplexity: 10.9732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [88100/138038], Loss: 2.0502, Perplexity: 7.76925\n",
      "Epoch [2/3], Step [88200/138038], Loss: 2.4383, Perplexity: 11.4533\n",
      "Epoch [2/3], Step [88300/138038], Loss: 3.4521, Perplexity: 31.56618\n",
      "Epoch [2/3], Step [88400/138038], Loss: 3.6936, Perplexity: 40.1905\n",
      "Epoch [2/3], Step [88500/138038], Loss: 3.4441, Perplexity: 31.31430\n",
      "Epoch [2/3], Step [88600/138038], Loss: 3.4029, Perplexity: 30.05042\n",
      "Epoch [2/3], Step [88700/138038], Loss: 3.0762, Perplexity: 21.67644\n",
      "Epoch [2/3], Step [88800/138038], Loss: 2.5073, Perplexity: 12.2715\n",
      "Epoch [2/3], Step [88900/138038], Loss: 2.1320, Perplexity: 8.431376\n",
      "Epoch [2/3], Step [89000/138038], Loss: 2.9136, Perplexity: 18.42359\n",
      "Epoch [2/3], Step [89100/138038], Loss: 3.0550, Perplexity: 21.2213\n",
      "Epoch [2/3], Step [89200/138038], Loss: 2.1878, Perplexity: 8.915763\n",
      "Epoch [2/3], Step [89300/138038], Loss: 3.0315, Perplexity: 20.72777\n",
      "Epoch [2/3], Step [89400/138038], Loss: 2.6406, Perplexity: 14.0219\n",
      "Epoch [2/3], Step [89500/138038], Loss: 2.2156, Perplexity: 9.16668\n",
      "Epoch [2/3], Step [89600/138038], Loss: 2.1354, Perplexity: 8.46016\n",
      "Epoch [2/3], Step [89700/138038], Loss: 2.9245, Perplexity: 18.6247\n",
      "Epoch [2/3], Step [89800/138038], Loss: 3.0044, Perplexity: 20.17320\n",
      "Epoch [2/3], Step [89900/138038], Loss: 2.4488, Perplexity: 11.5742\n",
      "Epoch [2/3], Step [90000/138038], Loss: 2.6521, Perplexity: 14.1834\n",
      "Epoch [2/3], Step [90100/138038], Loss: 3.1051, Perplexity: 22.3124\n",
      "Epoch [2/3], Step [90200/138038], Loss: 2.1397, Perplexity: 8.496978\n",
      "Epoch [2/3], Step [90300/138038], Loss: 3.4259, Perplexity: 30.75115\n",
      "Epoch [2/3], Step [90400/138038], Loss: 2.9821, Perplexity: 19.7293\n",
      "Epoch [2/3], Step [90500/138038], Loss: 2.5903, Perplexity: 13.33363\n",
      "Epoch [2/3], Step [90600/138038], Loss: 3.1683, Perplexity: 23.7665\n",
      "Epoch [2/3], Step [90700/138038], Loss: 2.7810, Perplexity: 16.13547\n",
      "Epoch [2/3], Step [90800/138038], Loss: 1.7484, Perplexity: 5.74547\n",
      "Epoch [2/3], Step [90900/138038], Loss: 2.3763, Perplexity: 10.7652\n",
      "Epoch [2/3], Step [91000/138038], Loss: 2.2350, Perplexity: 9.34688\n",
      "Epoch [2/3], Step [91100/138038], Loss: 2.8781, Perplexity: 17.7799\n",
      "Epoch [2/3], Step [91200/138038], Loss: 2.8902, Perplexity: 17.99691\n",
      "Epoch [2/3], Step [91300/138038], Loss: 2.7088, Perplexity: 15.0116\n",
      "Epoch [2/3], Step [91400/138038], Loss: 2.8127, Perplexity: 16.6548\n",
      "Epoch [2/3], Step [91500/138038], Loss: 2.4361, Perplexity: 11.4287\n",
      "Epoch [2/3], Step [91600/138038], Loss: 3.3225, Perplexity: 27.7301\n",
      "Epoch [2/3], Step [91700/138038], Loss: 2.5972, Perplexity: 13.4255\n",
      "Epoch [2/3], Step [91800/138038], Loss: 2.3409, Perplexity: 10.3902\n",
      "Epoch [2/3], Step [91900/138038], Loss: 1.6845, Perplexity: 5.38983\n",
      "Epoch [2/3], Step [92000/138038], Loss: 2.5202, Perplexity: 12.43073\n",
      "Epoch [2/3], Step [92100/138038], Loss: 2.9073, Perplexity: 18.3077\n",
      "Epoch [2/3], Step [92200/138038], Loss: 2.0566, Perplexity: 7.819551\n",
      "Epoch [2/3], Step [92300/138038], Loss: 2.5014, Perplexity: 12.1997\n",
      "Epoch [2/3], Step [92400/138038], Loss: 2.6138, Perplexity: 13.6512\n",
      "Epoch [2/3], Step [92500/138038], Loss: 3.0421, Perplexity: 20.9495\n",
      "Epoch [2/3], Step [92600/138038], Loss: 2.4336, Perplexity: 11.3993\n",
      "Epoch [2/3], Step [92700/138038], Loss: 2.2196, Perplexity: 9.20342\n",
      "Epoch [2/3], Step [92800/138038], Loss: 2.0058, Perplexity: 7.432085\n",
      "Epoch [2/3], Step [92900/138038], Loss: 2.4906, Perplexity: 12.06859\n",
      "Epoch [2/3], Step [93000/138038], Loss: 2.2922, Perplexity: 9.89718\n",
      "Epoch [2/3], Step [93100/138038], Loss: 3.6730, Perplexity: 39.3691\n",
      "Epoch [2/3], Step [93200/138038], Loss: 3.1880, Perplexity: 24.2404\n",
      "Epoch [2/3], Step [93300/138038], Loss: 3.1929, Perplexity: 24.35893\n",
      "Epoch [2/3], Step [93400/138038], Loss: 2.4886, Perplexity: 12.0439\n",
      "Epoch [2/3], Step [93500/138038], Loss: 1.8507, Perplexity: 6.364359\n",
      "Epoch [2/3], Step [93600/138038], Loss: 2.2394, Perplexity: 9.387603\n",
      "Epoch [2/3], Step [93700/138038], Loss: 3.3572, Perplexity: 28.7075\n",
      "Epoch [2/3], Step [93800/138038], Loss: 3.9468, Perplexity: 51.7699\n",
      "Epoch [2/3], Step [93900/138038], Loss: 3.0274, Perplexity: 20.64340\n",
      "Epoch [2/3], Step [94000/138038], Loss: 2.2243, Perplexity: 9.24702\n",
      "Epoch [2/3], Step [94100/138038], Loss: 4.6518, Perplexity: 104.7756\n",
      "Epoch [2/3], Step [94200/138038], Loss: 2.5005, Perplexity: 12.1890\n",
      "Epoch [2/3], Step [94300/138038], Loss: 3.0131, Perplexity: 20.35076\n",
      "Epoch [2/3], Step [94400/138038], Loss: 2.2490, Perplexity: 9.478203\n",
      "Epoch [2/3], Step [94500/138038], Loss: 2.5105, Perplexity: 12.3111\n",
      "Epoch [2/3], Step [94600/138038], Loss: 2.0903, Perplexity: 8.08724\n",
      "Epoch [2/3], Step [94700/138038], Loss: 2.9682, Perplexity: 19.4572\n",
      "Epoch [2/3], Step [94800/138038], Loss: 2.9086, Perplexity: 18.3304\n",
      "Epoch [2/3], Step [94900/138038], Loss: 1.9853, Perplexity: 7.28169\n",
      "Epoch [2/3], Step [95000/138038], Loss: 3.5040, Perplexity: 33.2469\n",
      "Epoch [2/3], Step [95100/138038], Loss: 2.4398, Perplexity: 11.4706\n",
      "Epoch [2/3], Step [95200/138038], Loss: 1.8991, Perplexity: 6.67972\n",
      "Epoch [2/3], Step [95300/138038], Loss: 2.8056, Perplexity: 16.5372\n",
      "Epoch [2/3], Step [95400/138038], Loss: 2.5539, Perplexity: 12.8569\n",
      "Epoch [2/3], Step [95500/138038], Loss: 3.5752, Perplexity: 35.7018\n",
      "Epoch [2/3], Step [95600/138038], Loss: 1.4779, Perplexity: 4.383939\n",
      "Epoch [2/3], Step [95700/138038], Loss: 3.1798, Perplexity: 24.0419\n",
      "Epoch [2/3], Step [95800/138038], Loss: 2.7016, Perplexity: 14.9039\n",
      "Epoch [2/3], Step [95900/138038], Loss: 2.2677, Perplexity: 9.65686\n",
      "Epoch [2/3], Step [96000/138038], Loss: 2.8432, Perplexity: 17.1710\n",
      "Epoch [2/3], Step [96100/138038], Loss: 4.4786, Perplexity: 88.10966\n",
      "Epoch [2/3], Step [96200/138038], Loss: 2.0886, Perplexity: 8.07346\n",
      "Epoch [2/3], Step [96300/138038], Loss: 2.4466, Perplexity: 11.5489\n",
      "Epoch [2/3], Step [96400/138038], Loss: 3.0831, Perplexity: 21.82581\n",
      "Epoch [2/3], Step [96500/138038], Loss: 2.4436, Perplexity: 11.5142\n",
      "Epoch [2/3], Step [96600/138038], Loss: 2.6321, Perplexity: 13.90266\n",
      "Epoch [2/3], Step [96700/138038], Loss: 2.1941, Perplexity: 8.97196\n",
      "Epoch [2/3], Step [96800/138038], Loss: 2.8748, Perplexity: 17.7225\n",
      "Epoch [2/3], Step [96900/138038], Loss: 3.6775, Perplexity: 39.5493\n",
      "Epoch [2/3], Step [97000/138038], Loss: 3.0749, Perplexity: 21.64840\n",
      "Epoch [2/3], Step [97100/138038], Loss: 1.6286, Perplexity: 5.09682\n",
      "Epoch [2/3], Step [97200/138038], Loss: 1.8976, Perplexity: 6.66966\n",
      "Epoch [2/3], Step [97300/138038], Loss: 1.4888, Perplexity: 4.431693\n",
      "Epoch [2/3], Step [97400/138038], Loss: 2.5952, Perplexity: 13.39895\n",
      "Epoch [2/3], Step [97500/138038], Loss: 2.6895, Perplexity: 14.72437\n",
      "Epoch [2/3], Step [97600/138038], Loss: 2.5414, Perplexity: 12.6968\n",
      "Epoch [2/3], Step [97700/138038], Loss: 3.1768, Perplexity: 23.9690\n",
      "Epoch [2/3], Step [97800/138038], Loss: 2.9234, Perplexity: 18.6037\n",
      "Epoch [2/3], Step [97900/138038], Loss: 2.6161, Perplexity: 13.68182\n",
      "Epoch [2/3], Step [98000/138038], Loss: 2.9942, Perplexity: 19.9694\n",
      "Epoch [2/3], Step [98100/138038], Loss: 2.5429, Perplexity: 12.7169\n",
      "Epoch [2/3], Step [98200/138038], Loss: 2.2994, Perplexity: 9.96863\n",
      "Epoch [2/3], Step [98300/138038], Loss: 4.4806, Perplexity: 88.28775\n",
      "Epoch [2/3], Step [98400/138038], Loss: 1.8587, Perplexity: 6.415482\n",
      "Epoch [2/3], Step [98500/138038], Loss: 2.2230, Perplexity: 9.23485\n",
      "Epoch [2/3], Step [98600/138038], Loss: 2.6680, Perplexity: 14.4113\n",
      "Epoch [2/3], Step [98700/138038], Loss: 3.3555, Perplexity: 28.6598\n",
      "Epoch [2/3], Step [98800/138038], Loss: 2.5569, Perplexity: 12.8959\n",
      "Epoch [2/3], Step [98900/138038], Loss: 4.5385, Perplexity: 93.5511\n",
      "Epoch [2/3], Step [99000/138038], Loss: 2.8691, Perplexity: 17.62145\n",
      "Epoch [2/3], Step [99100/138038], Loss: 2.4206, Perplexity: 11.25231\n",
      "Epoch [2/3], Step [99200/138038], Loss: 2.5413, Perplexity: 12.6956\n",
      "Epoch [2/3], Step [99300/138038], Loss: 2.6613, Perplexity: 14.3151\n",
      "Epoch [2/3], Step [99400/138038], Loss: 3.6016, Perplexity: 36.6551\n",
      "Epoch [2/3], Step [99500/138038], Loss: 2.9999, Perplexity: 20.08325\n",
      "Epoch [2/3], Step [99600/138038], Loss: 1.9996, Perplexity: 7.38592\n",
      "Epoch [2/3], Step [99700/138038], Loss: 2.9900, Perplexity: 19.8855\n",
      "Epoch [2/3], Step [99800/138038], Loss: 2.3272, Perplexity: 10.2491\n",
      "Epoch [2/3], Step [99900/138038], Loss: 2.3225, Perplexity: 10.20077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [100000/138038], Loss: 2.2314, Perplexity: 9.31293\n",
      "Epoch [2/3], Step [100100/138038], Loss: 2.4013, Perplexity: 11.0376\n",
      "Epoch [2/3], Step [100200/138038], Loss: 2.4336, Perplexity: 11.39965\n",
      "Epoch [2/3], Step [100300/138038], Loss: 2.5493, Perplexity: 12.7984\n",
      "Epoch [2/3], Step [100400/138038], Loss: 2.4947, Perplexity: 12.1184\n",
      "Epoch [2/3], Step [100500/138038], Loss: 2.7269, Perplexity: 15.28501\n",
      "Epoch [2/3], Step [100600/138038], Loss: 2.6673, Perplexity: 14.4010\n",
      "Epoch [2/3], Step [100700/138038], Loss: 4.4276, Perplexity: 83.7300\n",
      "Epoch [2/3], Step [100800/138038], Loss: 3.2386, Perplexity: 25.4985\n",
      "Epoch [2/3], Step [100900/138038], Loss: 3.7351, Perplexity: 41.8936\n",
      "Epoch [2/3], Step [101000/138038], Loss: 2.1908, Perplexity: 8.94249\n",
      "Epoch [2/3], Step [101100/138038], Loss: 2.7199, Perplexity: 15.17836\n",
      "Epoch [2/3], Step [101200/138038], Loss: 2.8392, Perplexity: 17.1026\n",
      "Epoch [2/3], Step [101300/138038], Loss: 2.3829, Perplexity: 10.83578\n",
      "Epoch [2/3], Step [101400/138038], Loss: 2.5905, Perplexity: 13.3360\n",
      "Epoch [2/3], Step [101500/138038], Loss: 3.1220, Perplexity: 22.6927\n",
      "Epoch [2/3], Step [101600/138038], Loss: 1.8513, Perplexity: 6.36799\n",
      "Epoch [2/3], Step [101700/138038], Loss: 2.3121, Perplexity: 10.09533\n",
      "Epoch [2/3], Step [101800/138038], Loss: 3.0059, Perplexity: 20.20443\n",
      "Epoch [2/3], Step [101900/138038], Loss: 3.4575, Perplexity: 31.7386\n",
      "Epoch [2/3], Step [102000/138038], Loss: 2.8244, Perplexity: 16.8515\n",
      "Epoch [2/3], Step [102100/138038], Loss: 1.9292, Perplexity: 6.88384\n",
      "Epoch [2/3], Step [102200/138038], Loss: 1.8966, Perplexity: 6.66350\n",
      "Epoch [2/3], Step [102300/138038], Loss: 3.3295, Perplexity: 27.92443\n",
      "Epoch [2/3], Step [102400/138038], Loss: 2.7275, Perplexity: 15.2949\n",
      "Epoch [2/3], Step [102500/138038], Loss: 1.8758, Perplexity: 6.52631\n",
      "Epoch [2/3], Step [102600/138038], Loss: 3.0148, Perplexity: 20.38488\n",
      "Epoch [2/3], Step [102700/138038], Loss: 3.0126, Perplexity: 20.3405\n",
      "Epoch [2/3], Step [102800/138038], Loss: 2.8097, Perplexity: 16.6051\n",
      "Epoch [2/3], Step [102900/138038], Loss: 4.0609, Perplexity: 58.0264\n",
      "Epoch [2/3], Step [103000/138038], Loss: 2.2881, Perplexity: 9.855993\n",
      "Epoch [2/3], Step [103100/138038], Loss: 4.2139, Perplexity: 67.61909\n",
      "Epoch [2/3], Step [103200/138038], Loss: 2.9620, Perplexity: 19.3375\n",
      "Epoch [2/3], Step [103300/138038], Loss: 1.9596, Perplexity: 7.09640\n",
      "Epoch [2/3], Step [103400/138038], Loss: 2.9440, Perplexity: 18.9926\n",
      "Epoch [2/3], Step [103500/138038], Loss: 2.5526, Perplexity: 12.8403\n",
      "Epoch [2/3], Step [103600/138038], Loss: 2.8272, Perplexity: 16.89743\n",
      "Epoch [2/3], Step [103700/138038], Loss: 1.9393, Perplexity: 6.95390\n",
      "Epoch [2/3], Step [103800/138038], Loss: 3.0316, Perplexity: 20.7298\n",
      "Epoch [2/3], Step [103900/138038], Loss: 2.4257, Perplexity: 11.3106\n",
      "Epoch [2/3], Step [104000/138038], Loss: 2.5973, Perplexity: 13.4274\n",
      "Epoch [2/3], Step [104100/138038], Loss: 3.4627, Perplexity: 31.90355\n",
      "Epoch [2/3], Step [104200/138038], Loss: 2.1554, Perplexity: 8.63132\n",
      "Epoch [2/3], Step [104300/138038], Loss: 2.6429, Perplexity: 14.0534\n",
      "Epoch [2/3], Step [104400/138038], Loss: 2.9606, Perplexity: 19.31011\n",
      "Epoch [2/3], Step [104500/138038], Loss: 2.6510, Perplexity: 14.1685\n",
      "Epoch [2/3], Step [104600/138038], Loss: 3.1338, Perplexity: 22.9617\n",
      "Epoch [2/3], Step [104700/138038], Loss: 3.1208, Perplexity: 22.66396\n",
      "Epoch [2/3], Step [104800/138038], Loss: 3.2261, Perplexity: 25.1814\n",
      "Epoch [2/3], Step [104900/138038], Loss: 2.2855, Perplexity: 9.83067\n",
      "Epoch [2/3], Step [105000/138038], Loss: 3.2185, Perplexity: 24.9913\n",
      "Epoch [2/3], Step [105100/138038], Loss: 2.8766, Perplexity: 17.7547\n",
      "Epoch [2/3], Step [105200/138038], Loss: 2.5194, Perplexity: 12.42178\n",
      "Epoch [2/3], Step [105300/138038], Loss: 2.2624, Perplexity: 9.605864\n",
      "Epoch [2/3], Step [105400/138038], Loss: 3.1441, Perplexity: 23.19992\n",
      "Epoch [2/3], Step [105500/138038], Loss: 1.4529, Perplexity: 4.275585\n",
      "Epoch [2/3], Step [105600/138038], Loss: 1.8464, Perplexity: 6.33679\n",
      "Epoch [2/3], Step [105700/138038], Loss: 2.2874, Perplexity: 9.84932\n",
      "Epoch [2/3], Step [105800/138038], Loss: 2.1167, Perplexity: 8.303339\n",
      "Epoch [2/3], Step [105900/138038], Loss: 4.0876, Perplexity: 59.5984\n",
      "Epoch [2/3], Step [106000/138038], Loss: 1.2581, Perplexity: 3.51874\n",
      "Epoch [2/3], Step [106100/138038], Loss: 1.6890, Perplexity: 5.41432\n",
      "Epoch [2/3], Step [106200/138038], Loss: 3.1702, Perplexity: 23.81191\n",
      "Epoch [2/3], Step [106300/138038], Loss: 2.2222, Perplexity: 9.227607\n",
      "Epoch [2/3], Step [106400/138038], Loss: 2.6819, Perplexity: 14.61250\n",
      "Epoch [2/3], Step [106500/138038], Loss: 2.1502, Perplexity: 8.586453\n",
      "Epoch [2/3], Step [106600/138038], Loss: 2.2994, Perplexity: 9.968245\n",
      "Epoch [2/3], Step [106700/138038], Loss: 2.5671, Perplexity: 13.0280\n",
      "Epoch [2/3], Step [106800/138038], Loss: 2.6794, Perplexity: 14.5764\n",
      "Epoch [2/3], Step [106900/138038], Loss: 3.3467, Perplexity: 28.40928\n",
      "Epoch [2/3], Step [107000/138038], Loss: 4.0771, Perplexity: 58.9719\n",
      "Epoch [2/3], Step [107100/138038], Loss: 1.6900, Perplexity: 5.41953\n",
      "Epoch [2/3], Step [107200/138038], Loss: 2.4762, Perplexity: 11.89576\n",
      "Epoch [2/3], Step [107300/138038], Loss: 3.2352, Perplexity: 25.4108\n",
      "Epoch [2/3], Step [107400/138038], Loss: 3.9684, Perplexity: 52.90020\n",
      "Epoch [2/3], Step [107500/138038], Loss: 3.0045, Perplexity: 20.17650\n",
      "Epoch [2/3], Step [107600/138038], Loss: 2.1429, Perplexity: 8.52432\n",
      "Epoch [2/3], Step [107700/138038], Loss: 2.1740, Perplexity: 8.79356\n",
      "Epoch [2/3], Step [107800/138038], Loss: 3.7530, Perplexity: 42.65026\n",
      "Epoch [2/3], Step [107900/138038], Loss: 3.0998, Perplexity: 22.1927\n",
      "Epoch [2/3], Step [108000/138038], Loss: 3.2824, Perplexity: 26.6388\n",
      "Epoch [2/3], Step [108100/138038], Loss: 3.0770, Perplexity: 21.6925\n",
      "Epoch [2/3], Step [108200/138038], Loss: 1.9497, Perplexity: 7.02632\n",
      "Epoch [2/3], Step [108300/138038], Loss: 2.4321, Perplexity: 11.3829\n",
      "Epoch [2/3], Step [108400/138038], Loss: 3.0811, Perplexity: 21.7813\n",
      "Epoch [2/3], Step [108500/138038], Loss: 1.9466, Perplexity: 7.004626\n",
      "Epoch [2/3], Step [108600/138038], Loss: 2.2398, Perplexity: 9.39140\n",
      "Epoch [2/3], Step [108700/138038], Loss: 1.7176, Perplexity: 5.571353\n",
      "Epoch [2/3], Step [108800/138038], Loss: 2.3082, Perplexity: 10.0560\n",
      "Epoch [2/3], Step [108900/138038], Loss: 2.7223, Perplexity: 15.21587\n",
      "Epoch [2/3], Step [109000/138038], Loss: 2.2975, Perplexity: 9.94936\n",
      "Epoch [2/3], Step [109100/138038], Loss: 1.6916, Perplexity: 5.428358\n",
      "Epoch [2/3], Step [109200/138038], Loss: 2.2176, Perplexity: 9.18531\n",
      "Epoch [2/3], Step [109300/138038], Loss: 3.3575, Perplexity: 28.71667\n",
      "Epoch [2/3], Step [109400/138038], Loss: 2.7845, Perplexity: 16.19128\n",
      "Epoch [2/3], Step [109500/138038], Loss: 3.2575, Perplexity: 25.9841\n",
      "Epoch [2/3], Step [109600/138038], Loss: 1.7934, Perplexity: 6.009779\n",
      "Epoch [2/3], Step [109700/138038], Loss: 2.6712, Perplexity: 14.4573\n",
      "Epoch [2/3], Step [109800/138038], Loss: 2.0013, Perplexity: 7.39903\n",
      "Epoch [2/3], Step [109900/138038], Loss: 2.1229, Perplexity: 8.35530\n",
      "Epoch [2/3], Step [110000/138038], Loss: 2.5258, Perplexity: 12.50064\n",
      "Epoch [2/3], Step [110100/138038], Loss: 2.6483, Perplexity: 14.1306\n",
      "Epoch [2/3], Step [110200/138038], Loss: 2.3602, Perplexity: 10.5928\n",
      "Epoch [2/3], Step [110300/138038], Loss: 2.3681, Perplexity: 10.67745\n",
      "Epoch [2/3], Step [110400/138038], Loss: 2.3324, Perplexity: 10.3029\n",
      "Epoch [2/3], Step [110500/138038], Loss: 2.5447, Perplexity: 12.73905\n",
      "Epoch [2/3], Step [110600/138038], Loss: 3.0071, Perplexity: 20.22899\n",
      "Epoch [2/3], Step [110700/138038], Loss: 2.8736, Perplexity: 17.7005\n",
      "Epoch [2/3], Step [110800/138038], Loss: 2.2476, Perplexity: 9.46558\n",
      "Epoch [2/3], Step [110900/138038], Loss: 2.8310, Perplexity: 16.96180\n",
      "Epoch [2/3], Step [111000/138038], Loss: 3.6973, Perplexity: 40.3388\n",
      "Epoch [2/3], Step [111100/138038], Loss: 2.4416, Perplexity: 11.49114\n",
      "Epoch [2/3], Step [111200/138038], Loss: 3.7398, Perplexity: 42.09067\n",
      "Epoch [2/3], Step [111300/138038], Loss: 2.4844, Perplexity: 11.99378\n",
      "Epoch [2/3], Step [111400/138038], Loss: 3.3329, Perplexity: 28.0194\n",
      "Epoch [2/3], Step [111500/138038], Loss: 2.6135, Perplexity: 13.6467\n",
      "Epoch [2/3], Step [111600/138038], Loss: 3.1127, Perplexity: 22.4808\n",
      "Epoch [2/3], Step [111700/138038], Loss: 1.7952, Perplexity: 6.02056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [111800/138038], Loss: 3.4439, Perplexity: 31.31021\n",
      "Epoch [2/3], Step [111900/138038], Loss: 2.6066, Perplexity: 13.5534\n",
      "Epoch [2/3], Step [112000/138038], Loss: 2.2757, Perplexity: 9.73460\n",
      "Epoch [2/3], Step [112100/138038], Loss: 2.1149, Perplexity: 8.28874\n",
      "Epoch [2/3], Step [112200/138038], Loss: 2.4243, Perplexity: 11.2946\n",
      "Epoch [2/3], Step [112300/138038], Loss: 2.1509, Perplexity: 8.59299\n",
      "Epoch [2/3], Step [112400/138038], Loss: 2.6479, Perplexity: 14.1238\n",
      "Epoch [2/3], Step [112500/138038], Loss: 2.1693, Perplexity: 8.75210\n",
      "Epoch [2/3], Step [112600/138038], Loss: 2.1320, Perplexity: 8.43181\n",
      "Epoch [2/3], Step [112700/138038], Loss: 2.6653, Perplexity: 14.3727\n",
      "Epoch [2/3], Step [112800/138038], Loss: 2.9157, Perplexity: 18.4626\n",
      "Epoch [2/3], Step [112900/138038], Loss: 1.9173, Perplexity: 6.80272\n",
      "Epoch [2/3], Step [113000/138038], Loss: 3.0276, Perplexity: 20.6473\n",
      "Epoch [2/3], Step [113100/138038], Loss: 3.1480, Perplexity: 23.2887\n",
      "Epoch [2/3], Step [113200/138038], Loss: 2.8610, Perplexity: 17.4795\n",
      "Epoch [2/3], Step [113300/138038], Loss: 2.8594, Perplexity: 17.45081\n",
      "Epoch [2/3], Step [113400/138038], Loss: 3.6912, Perplexity: 40.09424\n",
      "Epoch [2/3], Step [113500/138038], Loss: 2.4002, Perplexity: 11.0257\n",
      "Epoch [2/3], Step [113600/138038], Loss: 2.4208, Perplexity: 11.25430\n",
      "Epoch [2/3], Step [113700/138038], Loss: 2.9382, Perplexity: 18.88107\n",
      "Epoch [2/3], Step [113800/138038], Loss: 3.0650, Perplexity: 21.4350\n",
      "Epoch [2/3], Step [113900/138038], Loss: 2.8242, Perplexity: 16.84693\n",
      "Epoch [2/3], Step [114000/138038], Loss: 2.3329, Perplexity: 10.3076\n",
      "Epoch [2/3], Step [114100/138038], Loss: 3.1835, Perplexity: 24.1308\n",
      "Epoch [2/3], Step [114200/138038], Loss: 2.3516, Perplexity: 10.50276\n",
      "Epoch [2/3], Step [114300/138038], Loss: 1.6034, Perplexity: 4.969726\n",
      "Epoch [2/3], Step [114400/138038], Loss: 2.7479, Perplexity: 15.6097\n",
      "Epoch [2/3], Step [114500/138038], Loss: 1.9421, Perplexity: 6.973563\n",
      "Epoch [2/3], Step [114600/138038], Loss: 1.9395, Perplexity: 6.95511\n",
      "Epoch [2/3], Step [114700/138038], Loss: 2.1813, Perplexity: 8.85766\n",
      "Epoch [2/3], Step [114800/138038], Loss: 2.9974, Perplexity: 20.0337\n",
      "Epoch [2/3], Step [114900/138038], Loss: 2.3504, Perplexity: 10.4895\n",
      "Epoch [2/3], Step [115000/138038], Loss: 3.5975, Perplexity: 36.5073\n",
      "Epoch [2/3], Step [115100/138038], Loss: 2.2135, Perplexity: 9.14739\n",
      "Epoch [2/3], Step [115200/138038], Loss: 2.4644, Perplexity: 11.7570\n",
      "Epoch [2/3], Step [115300/138038], Loss: 2.0022, Perplexity: 7.40533\n",
      "Epoch [2/3], Step [115400/138038], Loss: 2.3309, Perplexity: 10.2872\n",
      "Epoch [2/3], Step [115500/138038], Loss: 3.3792, Perplexity: 29.3468\n",
      "Epoch [2/3], Step [115600/138038], Loss: 2.0854, Perplexity: 8.04798\n",
      "Epoch [2/3], Step [115700/138038], Loss: 2.1557, Perplexity: 8.63418\n",
      "Epoch [2/3], Step [115800/138038], Loss: 3.1955, Perplexity: 24.4221\n",
      "Epoch [2/3], Step [115900/138038], Loss: 2.8473, Perplexity: 17.2418\n",
      "Epoch [2/3], Step [116000/138038], Loss: 3.2743, Perplexity: 26.4252\n",
      "Epoch [2/3], Step [116100/138038], Loss: 1.9256, Perplexity: 6.85911\n",
      "Epoch [2/3], Step [116200/138038], Loss: 2.1265, Perplexity: 8.385185\n",
      "Epoch [2/3], Step [116300/138038], Loss: 2.0158, Perplexity: 7.506575\n",
      "Epoch [2/3], Step [116400/138038], Loss: 2.3323, Perplexity: 10.30146\n",
      "Epoch [2/3], Step [116500/138038], Loss: 2.2820, Perplexity: 9.79613\n",
      "Epoch [2/3], Step [116600/138038], Loss: 2.7397, Perplexity: 15.4816\n",
      "Epoch [2/3], Step [116700/138038], Loss: 3.4470, Perplexity: 31.4046\n",
      "Epoch [2/3], Step [116800/138038], Loss: 1.6806, Perplexity: 5.368942\n",
      "Epoch [2/3], Step [116900/138038], Loss: 3.1465, Perplexity: 23.2542\n",
      "Epoch [2/3], Step [117000/138038], Loss: 3.1518, Perplexity: 23.3779\n",
      "Epoch [2/3], Step [117100/138038], Loss: 1.1775, Perplexity: 3.24631\n",
      "Epoch [2/3], Step [117200/138038], Loss: 2.1887, Perplexity: 8.92378\n",
      "Epoch [2/3], Step [117300/138038], Loss: 3.4345, Perplexity: 31.0165\n",
      "Epoch [2/3], Step [117400/138038], Loss: 3.2640, Perplexity: 26.15403\n",
      "Epoch [2/3], Step [117500/138038], Loss: 2.9411, Perplexity: 18.93685\n",
      "Epoch [2/3], Step [117600/138038], Loss: 1.8835, Perplexity: 6.57650\n",
      "Epoch [2/3], Step [117700/138038], Loss: 2.3504, Perplexity: 10.4901\n",
      "Epoch [2/3], Step [117800/138038], Loss: 2.1689, Perplexity: 8.74833\n",
      "Epoch [2/3], Step [117900/138038], Loss: 2.1248, Perplexity: 8.37093\n",
      "Epoch [2/3], Step [118000/138038], Loss: 1.6664, Perplexity: 5.292816\n",
      "Epoch [2/3], Step [118100/138038], Loss: 2.6007, Perplexity: 13.4735\n",
      "Epoch [2/3], Step [118200/138038], Loss: 3.2729, Perplexity: 26.38774\n",
      "Epoch [2/3], Step [118300/138038], Loss: 2.5836, Perplexity: 13.24485\n",
      "Epoch [2/3], Step [118400/138038], Loss: 2.7173, Perplexity: 15.1396\n",
      "Epoch [2/3], Step [118500/138038], Loss: 2.8251, Perplexity: 16.8631\n",
      "Epoch [2/3], Step [118600/138038], Loss: 3.5186, Perplexity: 33.7378\n",
      "Epoch [2/3], Step [118700/138038], Loss: 2.3783, Perplexity: 10.7870\n",
      "Epoch [2/3], Step [118800/138038], Loss: 2.1131, Perplexity: 8.27397\n",
      "Epoch [2/3], Step [118900/138038], Loss: 2.3565, Perplexity: 10.5541\n",
      "Epoch [2/3], Step [119000/138038], Loss: 3.1101, Perplexity: 22.42295\n",
      "Epoch [2/3], Step [119100/138038], Loss: 2.8344, Perplexity: 17.0210\n",
      "Epoch [2/3], Step [119200/138038], Loss: 2.3051, Perplexity: 10.02489\n",
      "Epoch [2/3], Step [119300/138038], Loss: 2.5668, Perplexity: 13.02370\n",
      "Epoch [2/3], Step [119400/138038], Loss: 4.5048, Perplexity: 90.44937\n",
      "Epoch [2/3], Step [119500/138038], Loss: 2.3078, Perplexity: 10.0519\n",
      "Epoch [2/3], Step [119600/138038], Loss: 3.6553, Perplexity: 38.6809\n",
      "Epoch [2/3], Step [119700/138038], Loss: 2.9056, Perplexity: 18.27693\n",
      "Epoch [2/3], Step [119800/138038], Loss: 2.6279, Perplexity: 13.84412\n",
      "Epoch [2/3], Step [119900/138038], Loss: 2.5833, Perplexity: 13.24105\n",
      "Epoch [2/3], Step [120000/138038], Loss: 2.9645, Perplexity: 19.38444\n",
      "Epoch [2/3], Step [120100/138038], Loss: 1.9450, Perplexity: 6.99372\n",
      "Epoch [2/3], Step [120200/138038], Loss: 1.6206, Perplexity: 5.056341\n",
      "Epoch [2/3], Step [120300/138038], Loss: 2.0315, Perplexity: 7.62550\n",
      "Epoch [2/3], Step [120400/138038], Loss: 4.3758, Perplexity: 79.5072\n",
      "Epoch [2/3], Step [120500/138038], Loss: 1.4702, Perplexity: 4.35029\n",
      "Epoch [2/3], Step [120600/138038], Loss: 1.5520, Perplexity: 4.72118\n",
      "Epoch [2/3], Step [120700/138038], Loss: 2.6089, Perplexity: 13.5846\n",
      "Epoch [2/3], Step [120800/138038], Loss: 3.7054, Perplexity: 40.66654\n",
      "Epoch [2/3], Step [120900/138038], Loss: 2.2979, Perplexity: 9.953535\n",
      "Epoch [2/3], Step [121000/138038], Loss: 2.9504, Perplexity: 19.1133\n",
      "Epoch [2/3], Step [121100/138038], Loss: 4.4147, Perplexity: 82.6564\n",
      "Epoch [2/3], Step [121200/138038], Loss: 3.8525, Perplexity: 47.1125\n",
      "Epoch [2/3], Step [121300/138038], Loss: 2.7748, Perplexity: 16.03621\n",
      "Epoch [2/3], Step [121400/138038], Loss: 2.2973, Perplexity: 9.94751\n",
      "Epoch [2/3], Step [121500/138038], Loss: 1.7968, Perplexity: 6.03020\n",
      "Epoch [2/3], Step [121600/138038], Loss: 2.0426, Perplexity: 7.710987\n",
      "Epoch [2/3], Step [121700/138038], Loss: 2.3068, Perplexity: 10.04248\n",
      "Epoch [2/3], Step [121800/138038], Loss: 1.8356, Perplexity: 6.26929\n",
      "Epoch [2/3], Step [121900/138038], Loss: 3.2541, Perplexity: 25.8963\n",
      "Epoch [2/3], Step [122000/138038], Loss: 3.3906, Perplexity: 29.6845\n",
      "Epoch [2/3], Step [122100/138038], Loss: 2.3623, Perplexity: 10.61520\n",
      "Epoch [2/3], Step [122200/138038], Loss: 3.0974, Perplexity: 22.1399\n",
      "Epoch [2/3], Step [122300/138038], Loss: 4.8819, Perplexity: 131.8856\n",
      "Epoch [2/3], Step [122400/138038], Loss: 1.8061, Perplexity: 6.086611\n",
      "Epoch [2/3], Step [122500/138038], Loss: 3.5381, Perplexity: 34.4019\n",
      "Epoch [2/3], Step [122600/138038], Loss: 2.8700, Perplexity: 17.6375\n",
      "Epoch [2/3], Step [122700/138038], Loss: 3.2090, Perplexity: 24.7537\n",
      "Epoch [2/3], Step [122800/138038], Loss: 2.6247, Perplexity: 13.8008\n",
      "Epoch [2/3], Step [122900/138038], Loss: 2.2775, Perplexity: 9.75270\n",
      "Epoch [2/3], Step [123000/138038], Loss: 3.3707, Perplexity: 29.09871\n",
      "Epoch [2/3], Step [123100/138038], Loss: 2.9267, Perplexity: 18.66522\n",
      "Epoch [2/3], Step [123200/138038], Loss: 2.8954, Perplexity: 18.09132\n",
      "Epoch [2/3], Step [123300/138038], Loss: 3.5658, Perplexity: 35.3668\n",
      "Epoch [2/3], Step [123400/138038], Loss: 3.6502, Perplexity: 38.4807\n",
      "Epoch [2/3], Step [123500/138038], Loss: 2.8105, Perplexity: 16.6178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [123600/138038], Loss: 3.2354, Perplexity: 25.4171\n",
      "Epoch [2/3], Step [123700/138038], Loss: 2.4669, Perplexity: 11.7853\n",
      "Epoch [2/3], Step [123800/138038], Loss: 3.0333, Perplexity: 20.7648\n",
      "Epoch [2/3], Step [123900/138038], Loss: 3.9931, Perplexity: 54.22365\n",
      "Epoch [2/3], Step [124000/138038], Loss: 2.2842, Perplexity: 9.817693\n",
      "Epoch [2/3], Step [124100/138038], Loss: 3.3461, Perplexity: 28.3913\n",
      "Epoch [2/3], Step [124200/138038], Loss: 2.9862, Perplexity: 19.8102\n",
      "Epoch [2/3], Step [124300/138038], Loss: 2.9547, Perplexity: 19.1967\n",
      "Epoch [2/3], Step [124400/138038], Loss: 3.6868, Perplexity: 39.9159\n",
      "Epoch [2/3], Step [124500/138038], Loss: 2.0163, Perplexity: 7.51017\n",
      "Epoch [2/3], Step [124600/138038], Loss: 3.4030, Perplexity: 30.05412\n",
      "Epoch [2/3], Step [124700/138038], Loss: 2.6848, Perplexity: 14.65572\n",
      "Epoch [2/3], Step [124800/138038], Loss: 3.1421, Perplexity: 23.15293\n",
      "Epoch [2/3], Step [124900/138038], Loss: 3.8856, Perplexity: 48.6970\n",
      "Epoch [2/3], Step [125000/138038], Loss: 3.4764, Perplexity: 32.3436\n",
      "Epoch [2/3], Step [125100/138038], Loss: 2.1748, Perplexity: 8.80035\n",
      "Epoch [2/3], Step [125200/138038], Loss: 2.6057, Perplexity: 13.54070\n",
      "Epoch [2/3], Step [125300/138038], Loss: 1.9213, Perplexity: 6.830147\n",
      "Epoch [2/3], Step [125400/138038], Loss: 3.0958, Perplexity: 22.1046\n",
      "Epoch [2/3], Step [125500/138038], Loss: 2.1981, Perplexity: 9.00756\n",
      "Epoch [2/3], Step [125600/138038], Loss: 2.7032, Perplexity: 14.92761\n",
      "Epoch [2/3], Step [125700/138038], Loss: 2.1692, Perplexity: 8.751234\n",
      "Epoch [2/3], Step [125800/138038], Loss: 1.9874, Perplexity: 7.29638\n",
      "Epoch [2/3], Step [125900/138038], Loss: 2.3333, Perplexity: 10.3118\n",
      "Epoch [2/3], Step [126000/138038], Loss: 2.8610, Perplexity: 17.4788\n",
      "Epoch [2/3], Step [126100/138038], Loss: 3.1047, Perplexity: 22.3022\n",
      "Epoch [2/3], Step [126200/138038], Loss: 2.0011, Perplexity: 7.39707\n",
      "Epoch [2/3], Step [126300/138038], Loss: 3.4523, Perplexity: 31.5722\n",
      "Epoch [2/3], Step [126400/138038], Loss: 2.8159, Perplexity: 16.7085\n",
      "Epoch [2/3], Step [126500/138038], Loss: 1.9962, Perplexity: 7.36106\n",
      "Epoch [2/3], Step [126600/138038], Loss: 3.1425, Perplexity: 23.1620\n",
      "Epoch [2/3], Step [126700/138038], Loss: 3.0110, Perplexity: 20.30811\n",
      "Epoch [2/3], Step [126800/138038], Loss: 3.3200, Perplexity: 27.6597\n",
      "Epoch [2/3], Step [126900/138038], Loss: 2.5733, Perplexity: 13.10840\n",
      "Epoch [2/3], Step [127000/138038], Loss: 1.8167, Perplexity: 6.15164\n",
      "Epoch [2/3], Step [127100/138038], Loss: 2.1481, Perplexity: 8.56844\n",
      "Epoch [2/3], Step [127200/138038], Loss: 2.1406, Perplexity: 8.504264\n",
      "Epoch [2/3], Step [127300/138038], Loss: 2.4830, Perplexity: 11.97728\n",
      "Epoch [2/3], Step [127400/138038], Loss: 3.2891, Perplexity: 26.81884\n",
      "Epoch [2/3], Step [127500/138038], Loss: 1.9832, Perplexity: 7.265701\n",
      "Epoch [2/3], Step [127600/138038], Loss: 2.9893, Perplexity: 19.8717\n",
      "Epoch [2/3], Step [127700/138038], Loss: 2.0539, Perplexity: 7.798664\n",
      "Epoch [2/3], Step [127800/138038], Loss: 2.5297, Perplexity: 12.5501\n",
      "Epoch [2/3], Step [127900/138038], Loss: 3.3658, Perplexity: 28.95676\n",
      "Epoch [2/3], Step [128000/138038], Loss: 2.3525, Perplexity: 10.51239\n",
      "Epoch [2/3], Step [128100/138038], Loss: 2.3618, Perplexity: 10.6095\n",
      "Epoch [2/3], Step [128200/138038], Loss: 3.8496, Perplexity: 46.97639\n",
      "Epoch [2/3], Step [128300/138038], Loss: 3.2366, Perplexity: 25.4475\n",
      "Epoch [2/3], Step [128400/138038], Loss: 2.8142, Perplexity: 16.67949\n",
      "Epoch [2/3], Step [128500/138038], Loss: 2.4320, Perplexity: 11.38119\n",
      "Epoch [2/3], Step [128600/138038], Loss: 2.6101, Perplexity: 13.6006\n",
      "Epoch [2/3], Step [128700/138038], Loss: 2.5239, Perplexity: 12.47726\n",
      "Epoch [2/3], Step [128800/138038], Loss: 1.4314, Perplexity: 4.184676\n",
      "Epoch [2/3], Step [128900/138038], Loss: 4.0413, Perplexity: 56.9009\n",
      "Epoch [2/3], Step [129000/138038], Loss: 2.1967, Perplexity: 8.99507\n",
      "Epoch [2/3], Step [129100/138038], Loss: 3.0056, Perplexity: 20.1988\n",
      "Epoch [2/3], Step [129200/138038], Loss: 2.9079, Perplexity: 18.3180\n",
      "Epoch [2/3], Step [129300/138038], Loss: 3.1074, Perplexity: 22.3624\n",
      "Epoch [2/3], Step [129400/138038], Loss: 3.3813, Perplexity: 29.4095\n",
      "Epoch [2/3], Step [129500/138038], Loss: 3.7091, Perplexity: 40.8187\n",
      "Epoch [2/3], Step [129600/138038], Loss: 1.6541, Perplexity: 5.22835\n",
      "Epoch [2/3], Step [129700/138038], Loss: 2.5801, Perplexity: 13.19804\n",
      "Epoch [2/3], Step [129800/138038], Loss: 3.3307, Perplexity: 27.9589\n",
      "Epoch [2/3], Step [129900/138038], Loss: 3.5185, Perplexity: 33.7339\n",
      "Epoch [2/3], Step [130000/138038], Loss: 3.1449, Perplexity: 23.2167\n",
      "Epoch [2/3], Step [130100/138038], Loss: 2.0607, Perplexity: 7.851454\n",
      "Epoch [2/3], Step [130200/138038], Loss: 2.2342, Perplexity: 9.33914\n",
      "Epoch [2/3], Step [130300/138038], Loss: 3.5033, Perplexity: 33.2240\n",
      "Epoch [2/3], Step [130400/138038], Loss: 2.8982, Perplexity: 18.1412\n",
      "Epoch [2/3], Step [130500/138038], Loss: 2.2088, Perplexity: 9.10464\n",
      "Epoch [2/3], Step [130600/138038], Loss: 2.5234, Perplexity: 12.47110\n",
      "Epoch [2/3], Step [130700/138038], Loss: 2.7752, Perplexity: 16.04244\n",
      "Epoch [2/3], Step [130800/138038], Loss: 2.2643, Perplexity: 9.62463\n",
      "Epoch [2/3], Step [130900/138038], Loss: 2.3955, Perplexity: 10.97339\n",
      "Epoch [2/3], Step [131000/138038], Loss: 2.1906, Perplexity: 8.940463\n",
      "Epoch [2/3], Step [131100/138038], Loss: 1.9849, Perplexity: 7.27826\n",
      "Epoch [2/3], Step [131200/138038], Loss: 2.3572, Perplexity: 10.5615\n",
      "Epoch [2/3], Step [131300/138038], Loss: 2.1107, Perplexity: 8.25391\n",
      "Epoch [2/3], Step [131400/138038], Loss: 3.6882, Perplexity: 39.9732\n",
      "Epoch [2/3], Step [131500/138038], Loss: 3.3431, Perplexity: 28.30810\n",
      "Epoch [2/3], Step [131600/138038], Loss: 3.0392, Perplexity: 20.8895\n",
      "Epoch [2/3], Step [131700/138038], Loss: 3.5682, Perplexity: 35.45162\n",
      "Epoch [2/3], Step [131800/138038], Loss: 2.4756, Perplexity: 11.8891\n",
      "Epoch [2/3], Step [131900/138038], Loss: 2.9003, Perplexity: 18.18047\n",
      "Epoch [2/3], Step [132000/138038], Loss: 2.7570, Perplexity: 15.7527\n",
      "Epoch [2/3], Step [132100/138038], Loss: 2.3320, Perplexity: 10.29845\n",
      "Epoch [2/3], Step [132200/138038], Loss: 3.1538, Perplexity: 23.4260\n",
      "Epoch [2/3], Step [132300/138038], Loss: 2.2012, Perplexity: 9.03581\n",
      "Epoch [2/3], Step [132400/138038], Loss: 3.2453, Perplexity: 25.66976\n",
      "Epoch [2/3], Step [132500/138038], Loss: 2.6922, Perplexity: 14.7642\n",
      "Epoch [2/3], Step [132600/138038], Loss: 3.4687, Perplexity: 32.0963\n",
      "Epoch [2/3], Step [132700/138038], Loss: 3.6513, Perplexity: 38.5249\n",
      "Epoch [2/3], Step [132800/138038], Loss: 2.1843, Perplexity: 8.884796\n",
      "Epoch [2/3], Step [132900/138038], Loss: 1.9665, Perplexity: 7.14595\n",
      "Epoch [2/3], Step [133000/138038], Loss: 1.9070, Perplexity: 6.73307\n",
      "Epoch [2/3], Step [133100/138038], Loss: 2.8502, Perplexity: 17.2905\n",
      "Epoch [2/3], Step [133200/138038], Loss: 2.3056, Perplexity: 10.0307\n",
      "Epoch [2/3], Step [133300/138038], Loss: 2.7410, Perplexity: 15.50266\n",
      "Epoch [2/3], Step [133400/138038], Loss: 2.2734, Perplexity: 9.712432\n",
      "Epoch [2/3], Step [133500/138038], Loss: 2.7905, Perplexity: 16.2898\n",
      "Epoch [2/3], Step [133600/138038], Loss: 2.5436, Perplexity: 12.7252\n",
      "Epoch [2/3], Step [133700/138038], Loss: 3.7173, Perplexity: 41.15194\n",
      "Epoch [2/3], Step [133800/138038], Loss: 2.8444, Perplexity: 17.1910\n",
      "Epoch [2/3], Step [133900/138038], Loss: 2.2249, Perplexity: 9.252305\n",
      "Epoch [2/3], Step [134000/138038], Loss: 2.2688, Perplexity: 9.66769\n",
      "Epoch [2/3], Step [134100/138038], Loss: 2.8292, Perplexity: 16.9316\n",
      "Epoch [2/3], Step [134200/138038], Loss: 3.8412, Perplexity: 46.5814\n",
      "Epoch [2/3], Step [134300/138038], Loss: 2.1722, Perplexity: 8.77774\n",
      "Epoch [2/3], Step [134400/138038], Loss: 2.4661, Perplexity: 11.7766\n",
      "Epoch [2/3], Step [134500/138038], Loss: 3.3187, Perplexity: 27.6253\n",
      "Epoch [2/3], Step [134600/138038], Loss: 3.8463, Perplexity: 46.8182\n",
      "Epoch [2/3], Step [134700/138038], Loss: 1.9394, Perplexity: 6.954614\n",
      "Epoch [2/3], Step [134800/138038], Loss: 2.7266, Perplexity: 15.28161\n",
      "Epoch [2/3], Step [134900/138038], Loss: 2.3198, Perplexity: 10.17380\n",
      "Epoch [2/3], Step [135000/138038], Loss: 2.7546, Perplexity: 15.7145\n",
      "Epoch [2/3], Step [135100/138038], Loss: 2.3055, Perplexity: 10.0295\n",
      "Epoch [2/3], Step [135200/138038], Loss: 3.0187, Perplexity: 20.46572\n",
      "Epoch [2/3], Step [135300/138038], Loss: 3.0948, Perplexity: 22.0829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [135400/138038], Loss: 2.1582, Perplexity: 8.65547\n",
      "Epoch [2/3], Step [135500/138038], Loss: 2.8764, Perplexity: 17.7501\n",
      "Epoch [2/3], Step [135600/138038], Loss: 3.9737, Perplexity: 53.1825\n",
      "Epoch [2/3], Step [135700/138038], Loss: 2.8165, Perplexity: 16.7190\n",
      "Epoch [2/3], Step [135800/138038], Loss: 2.3304, Perplexity: 10.2819\n",
      "Epoch [2/3], Step [135900/138038], Loss: 2.8802, Perplexity: 17.81726\n",
      "Epoch [2/3], Step [136000/138038], Loss: 2.0911, Perplexity: 8.093529\n",
      "Epoch [2/3], Step [136100/138038], Loss: 3.1803, Perplexity: 24.0541\n",
      "Epoch [2/3], Step [136200/138038], Loss: 1.7726, Perplexity: 5.88600\n",
      "Epoch [2/3], Step [136300/138038], Loss: 2.6118, Perplexity: 13.6241\n",
      "Epoch [2/3], Step [136400/138038], Loss: 3.2434, Perplexity: 25.6208\n",
      "Epoch [2/3], Step [136500/138038], Loss: 1.9202, Perplexity: 6.82261\n",
      "Epoch [2/3], Step [136600/138038], Loss: 2.5304, Perplexity: 12.5583\n",
      "Epoch [2/3], Step [136700/138038], Loss: 3.7364, Perplexity: 41.94651\n",
      "Epoch [2/3], Step [136800/138038], Loss: 3.2042, Perplexity: 24.6348\n",
      "Epoch [2/3], Step [136900/138038], Loss: 2.4235, Perplexity: 11.2857\n",
      "Epoch [2/3], Step [137000/138038], Loss: 3.8249, Perplexity: 45.8293\n",
      "Epoch [2/3], Step [137100/138038], Loss: 2.8769, Perplexity: 17.75845\n",
      "Epoch [2/3], Step [137200/138038], Loss: 2.5085, Perplexity: 12.2870\n",
      "Epoch [2/3], Step [137300/138038], Loss: 3.0870, Perplexity: 21.9108\n",
      "Epoch [2/3], Step [137400/138038], Loss: 2.8171, Perplexity: 16.7291\n",
      "Epoch [2/3], Step [137500/138038], Loss: 2.5649, Perplexity: 12.99915\n",
      "Epoch [2/3], Step [137600/138038], Loss: 1.9080, Perplexity: 6.73950\n",
      "Epoch [2/3], Step [137700/138038], Loss: 2.7724, Perplexity: 15.9965\n",
      "Epoch [2/3], Step [137800/138038], Loss: 3.2890, Perplexity: 26.8161\n",
      "Epoch [2/3], Step [137900/138038], Loss: 2.6116, Perplexity: 13.62111\n",
      "Epoch [2/3], Step [138000/138038], Loss: 1.7178, Perplexity: 5.572438\n",
      "Epoch [3/3], Step [100/138038], Loss: 3.2895, Perplexity: 26.8303317\n",
      "Epoch [3/3], Step [200/138038], Loss: 2.5506, Perplexity: 12.8145\n",
      "Epoch [3/3], Step [300/138038], Loss: 3.2430, Perplexity: 25.6114\n",
      "Epoch [3/3], Step [400/138038], Loss: 2.0019, Perplexity: 7.40343\n",
      "Epoch [3/3], Step [500/138038], Loss: 2.0970, Perplexity: 8.14164\n",
      "Epoch [3/3], Step [600/138038], Loss: 2.4331, Perplexity: 11.3940\n",
      "Epoch [3/3], Step [700/138038], Loss: 2.1182, Perplexity: 8.31624\n",
      "Epoch [3/3], Step [800/138038], Loss: 2.7769, Perplexity: 16.0696\n",
      "Epoch [3/3], Step [900/138038], Loss: 3.2481, Perplexity: 25.74099\n",
      "Epoch [3/3], Step [1000/138038], Loss: 3.1221, Perplexity: 22.6946\n",
      "Epoch [3/3], Step [1100/138038], Loss: 1.9685, Perplexity: 7.16012\n",
      "Epoch [3/3], Step [1200/138038], Loss: 2.0147, Perplexity: 7.49842\n",
      "Epoch [3/3], Step [1300/138038], Loss: 2.5238, Perplexity: 12.47615\n",
      "Epoch [3/3], Step [1400/138038], Loss: 3.0267, Perplexity: 20.6289\n",
      "Epoch [3/3], Step [1500/138038], Loss: 1.8060, Perplexity: 6.08625\n",
      "Epoch [3/3], Step [1600/138038], Loss: 5.0360, Perplexity: 153.8501\n",
      "Epoch [3/3], Step [1700/138038], Loss: 2.5007, Perplexity: 12.1915\n",
      "Epoch [3/3], Step [1800/138038], Loss: 2.5981, Perplexity: 13.4380\n",
      "Epoch [3/3], Step [1900/138038], Loss: 2.2212, Perplexity: 9.218678\n",
      "Epoch [3/3], Step [2000/138038], Loss: 1.8515, Perplexity: 6.36933\n",
      "Epoch [3/3], Step [2100/138038], Loss: 2.6850, Perplexity: 14.6586\n",
      "Epoch [3/3], Step [2200/138038], Loss: 2.8715, Perplexity: 17.6628\n",
      "Epoch [3/3], Step [2300/138038], Loss: 2.4039, Perplexity: 11.0666\n",
      "Epoch [3/3], Step [2400/138038], Loss: 3.3657, Perplexity: 28.9546\n",
      "Epoch [3/3], Step [2500/138038], Loss: 2.6254, Perplexity: 13.81041\n",
      "Epoch [3/3], Step [2600/138038], Loss: 2.7618, Perplexity: 15.82912\n",
      "Epoch [3/3], Step [2700/138038], Loss: 3.8303, Perplexity: 46.0774\n",
      "Epoch [3/3], Step [2800/138038], Loss: 2.3300, Perplexity: 10.2783\n",
      "Epoch [3/3], Step [2900/138038], Loss: 2.4098, Perplexity: 11.13150\n",
      "Epoch [3/3], Step [3000/138038], Loss: 3.6198, Perplexity: 37.3298\n",
      "Epoch [3/3], Step [3100/138038], Loss: 2.3160, Perplexity: 10.1352\n",
      "Epoch [3/3], Step [3200/138038], Loss: 2.4538, Perplexity: 11.6323\n",
      "Epoch [3/3], Step [3300/138038], Loss: 2.8380, Perplexity: 17.0808\n",
      "Epoch [3/3], Step [3400/138038], Loss: 2.8006, Perplexity: 16.4544\n",
      "Epoch [3/3], Step [3500/138038], Loss: 2.8107, Perplexity: 16.6216\n",
      "Epoch [3/3], Step [3600/138038], Loss: 1.8857, Perplexity: 6.59134\n",
      "Epoch [3/3], Step [3700/138038], Loss: 2.0391, Perplexity: 7.68401\n",
      "Epoch [3/3], Step [3800/138038], Loss: 2.6428, Perplexity: 14.0518\n",
      "Epoch [3/3], Step [3900/138038], Loss: 1.3976, Perplexity: 4.045376\n",
      "Epoch [3/3], Step [4000/138038], Loss: 2.1446, Perplexity: 8.53903\n",
      "Epoch [3/3], Step [4100/138038], Loss: 2.1769, Perplexity: 8.81911\n",
      "Epoch [3/3], Step [4200/138038], Loss: 3.1111, Perplexity: 22.4454\n",
      "Epoch [3/3], Step [4300/138038], Loss: 3.5089, Perplexity: 33.4115\n",
      "Epoch [3/3], Step [4400/138038], Loss: 2.8369, Perplexity: 17.0632\n",
      "Epoch [3/3], Step [4500/138038], Loss: 2.8868, Perplexity: 17.93558\n",
      "Epoch [3/3], Step [4600/138038], Loss: 3.2323, Perplexity: 25.33833\n",
      "Epoch [3/3], Step [4700/138038], Loss: 1.9504, Perplexity: 7.031440\n",
      "Epoch [3/3], Step [4800/138038], Loss: 1.9550, Perplexity: 7.064039\n",
      "Epoch [3/3], Step [4900/138038], Loss: 2.3011, Perplexity: 9.98482\n",
      "Epoch [3/3], Step [5000/138038], Loss: 1.9578, Perplexity: 7.083916\n",
      "Epoch [3/3], Step [5100/138038], Loss: 4.3065, Perplexity: 74.1816\n",
      "Epoch [3/3], Step [5200/138038], Loss: 1.4973, Perplexity: 4.46960\n",
      "Epoch [3/3], Step [5300/138038], Loss: 3.4206, Perplexity: 30.5882\n",
      "Epoch [3/3], Step [5400/138038], Loss: 1.9405, Perplexity: 6.962353\n",
      "Epoch [3/3], Step [5500/138038], Loss: 2.5947, Perplexity: 13.3927\n",
      "Epoch [3/3], Step [5600/138038], Loss: 3.2001, Perplexity: 24.5355\n",
      "Epoch [3/3], Step [5700/138038], Loss: 2.6043, Perplexity: 13.5211\n",
      "Epoch [3/3], Step [5800/138038], Loss: 2.1438, Perplexity: 8.531784\n",
      "Epoch [3/3], Step [5900/138038], Loss: 2.1761, Perplexity: 8.812222\n",
      "Epoch [3/3], Step [6000/138038], Loss: 1.8209, Perplexity: 6.17729\n",
      "Epoch [3/3], Step [6100/138038], Loss: 2.8864, Perplexity: 17.9287\n",
      "Epoch [3/3], Step [6200/138038], Loss: 1.7701, Perplexity: 5.87127\n",
      "Epoch [3/3], Step [6300/138038], Loss: 2.8155, Perplexity: 16.7014\n",
      "Epoch [3/3], Step [6400/138038], Loss: 2.8405, Perplexity: 17.12421\n",
      "Epoch [3/3], Step [6500/138038], Loss: 3.1276, Perplexity: 22.8183\n",
      "Epoch [3/3], Step [6600/138038], Loss: 2.5901, Perplexity: 13.3312\n",
      "Epoch [3/3], Step [6700/138038], Loss: 2.2920, Perplexity: 9.894465\n",
      "Epoch [3/3], Step [6800/138038], Loss: 2.7829, Perplexity: 16.16569\n",
      "Epoch [3/3], Step [6900/138038], Loss: 2.1171, Perplexity: 8.30714\n",
      "Epoch [3/3], Step [7000/138038], Loss: 1.9466, Perplexity: 7.00495\n",
      "Epoch [3/3], Step [7100/138038], Loss: 2.0145, Perplexity: 7.497240\n",
      "Epoch [3/3], Step [7200/138038], Loss: 2.8095, Perplexity: 16.6016\n",
      "Epoch [3/3], Step [7300/138038], Loss: 2.2995, Perplexity: 9.96926\n",
      "Epoch [3/3], Step [7400/138038], Loss: 3.7960, Perplexity: 44.5243\n",
      "Epoch [3/3], Step [7500/138038], Loss: 1.9241, Perplexity: 6.849019\n",
      "Epoch [3/3], Step [7600/138038], Loss: 3.3508, Perplexity: 28.5268\n",
      "Epoch [3/3], Step [7700/138038], Loss: 1.9359, Perplexity: 6.930074\n",
      "Epoch [3/3], Step [7800/138038], Loss: 1.9102, Perplexity: 6.75421\n",
      "Epoch [3/3], Step [7900/138038], Loss: 2.5503, Perplexity: 12.81047\n",
      "Epoch [3/3], Step [8000/138038], Loss: 3.1022, Perplexity: 22.24723\n",
      "Epoch [3/3], Step [8100/138038], Loss: 2.1226, Perplexity: 8.35291\n",
      "Epoch [3/3], Step [8200/138038], Loss: 3.8853, Perplexity: 48.6826\n",
      "Epoch [3/3], Step [8300/138038], Loss: 3.4475, Perplexity: 31.42162\n",
      "Epoch [3/3], Step [8400/138038], Loss: 4.5259, Perplexity: 92.38299\n",
      "Epoch [3/3], Step [8500/138038], Loss: 2.7224, Perplexity: 15.2170\n",
      "Epoch [3/3], Step [8600/138038], Loss: 2.6979, Perplexity: 14.8492\n",
      "Epoch [3/3], Step [8700/138038], Loss: 1.7104, Perplexity: 5.53118\n",
      "Epoch [3/3], Step [8800/138038], Loss: 1.8331, Perplexity: 6.25310\n",
      "Epoch [3/3], Step [8900/138038], Loss: 3.0882, Perplexity: 21.9369\n",
      "Epoch [3/3], Step [9000/138038], Loss: 2.3018, Perplexity: 9.992428\n",
      "Epoch [3/3], Step [9100/138038], Loss: 2.4605, Perplexity: 11.71092\n",
      "Epoch [3/3], Step [9200/138038], Loss: 2.6110, Perplexity: 13.6131\n",
      "Epoch [3/3], Step [9300/138038], Loss: 2.8233, Perplexity: 16.8322\n",
      "Epoch [3/3], Step [9400/138038], Loss: 2.1193, Perplexity: 8.32517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [9500/138038], Loss: 3.9114, Perplexity: 49.97063\n",
      "Epoch [3/3], Step [9600/138038], Loss: 2.7545, Perplexity: 15.71365\n",
      "Epoch [3/3], Step [9700/138038], Loss: 2.2787, Perplexity: 9.763938\n",
      "Epoch [3/3], Step [9800/138038], Loss: 3.0043, Perplexity: 20.17291\n",
      "Epoch [3/3], Step [9900/138038], Loss: 3.2923, Perplexity: 26.90398\n",
      "Epoch [3/3], Step [10000/138038], Loss: 3.5028, Perplexity: 33.2092\n",
      "Epoch [3/3], Step [10100/138038], Loss: 2.4739, Perplexity: 11.8685\n",
      "Epoch [3/3], Step [10200/138038], Loss: 2.7108, Perplexity: 15.0410\n",
      "Epoch [3/3], Step [10300/138038], Loss: 2.2540, Perplexity: 9.52555\n",
      "Epoch [3/3], Step [10400/138038], Loss: 3.2598, Perplexity: 26.0450\n",
      "Epoch [3/3], Step [10500/138038], Loss: 4.2553, Perplexity: 70.48053\n",
      "Epoch [3/3], Step [10600/138038], Loss: 2.5671, Perplexity: 13.0274\n",
      "Epoch [3/3], Step [10700/138038], Loss: 2.5793, Perplexity: 13.1879\n",
      "Epoch [3/3], Step [10800/138038], Loss: 2.0010, Perplexity: 7.39648\n",
      "Epoch [3/3], Step [10900/138038], Loss: 1.9669, Perplexity: 7.14817\n",
      "Epoch [3/3], Step [11000/138038], Loss: 2.8135, Perplexity: 16.66843\n",
      "Epoch [3/3], Step [11100/138038], Loss: 2.6450, Perplexity: 14.08356\n",
      "Epoch [3/3], Step [11200/138038], Loss: 2.6799, Perplexity: 14.5843\n",
      "Epoch [3/3], Step [11300/138038], Loss: 1.9888, Perplexity: 7.30642\n",
      "Epoch [3/3], Step [11400/138038], Loss: 2.6534, Perplexity: 14.20230\n",
      "Epoch [3/3], Step [11500/138038], Loss: 2.8483, Perplexity: 17.2586\n",
      "Epoch [3/3], Step [11600/138038], Loss: 2.9596, Perplexity: 19.2904\n",
      "Epoch [3/3], Step [11700/138038], Loss: 2.1694, Perplexity: 8.75291\n",
      "Epoch [3/3], Step [11800/138038], Loss: 2.6853, Perplexity: 14.6620\n",
      "Epoch [3/3], Step [11900/138038], Loss: 2.5483, Perplexity: 12.7851\n",
      "Epoch [3/3], Step [12000/138038], Loss: 2.3608, Perplexity: 10.5997\n",
      "Epoch [3/3], Step [12100/138038], Loss: 1.5966, Perplexity: 4.93617\n",
      "Epoch [3/3], Step [12200/138038], Loss: 1.9518, Perplexity: 7.04142\n",
      "Epoch [3/3], Step [12300/138038], Loss: 2.2914, Perplexity: 9.889094\n",
      "Epoch [3/3], Step [12400/138038], Loss: 2.5021, Perplexity: 12.2084\n",
      "Epoch [3/3], Step [12500/138038], Loss: 2.3380, Perplexity: 10.36078\n",
      "Epoch [3/3], Step [12600/138038], Loss: 2.4791, Perplexity: 11.9305\n",
      "Epoch [3/3], Step [12700/138038], Loss: 2.4159, Perplexity: 11.2002\n",
      "Epoch [3/3], Step [12800/138038], Loss: 3.0032, Perplexity: 20.14934\n",
      "Epoch [3/3], Step [12900/138038], Loss: 2.6110, Perplexity: 13.6127\n",
      "Epoch [3/3], Step [13000/138038], Loss: 2.6215, Perplexity: 13.7566\n",
      "Epoch [3/3], Step [13100/138038], Loss: 2.5524, Perplexity: 12.83764\n",
      "Epoch [3/3], Step [13200/138038], Loss: 2.8525, Perplexity: 17.3308\n",
      "Epoch [3/3], Step [13300/138038], Loss: 2.8184, Perplexity: 16.75089\n",
      "Epoch [3/3], Step [13400/138038], Loss: 3.4351, Perplexity: 31.0343\n",
      "Epoch [3/3], Step [13500/138038], Loss: 2.6963, Perplexity: 14.8254\n",
      "Epoch [3/3], Step [13600/138038], Loss: 2.0612, Perplexity: 7.85527\n",
      "Epoch [3/3], Step [13700/138038], Loss: 3.0707, Perplexity: 21.5579\n",
      "Epoch [3/3], Step [13800/138038], Loss: 2.3767, Perplexity: 10.7697\n",
      "Epoch [3/3], Step [13900/138038], Loss: 2.7443, Perplexity: 15.5541\n",
      "Epoch [3/3], Step [14000/138038], Loss: 1.9612, Perplexity: 7.10812\n",
      "Epoch [3/3], Step [14100/138038], Loss: 2.1161, Perplexity: 8.29843\n",
      "Epoch [3/3], Step [14200/138038], Loss: 2.3276, Perplexity: 10.2534\n",
      "Epoch [3/3], Step [14300/138038], Loss: 3.4436, Perplexity: 31.3003\n",
      "Epoch [3/3], Step [14400/138038], Loss: 1.9021, Perplexity: 6.69993\n",
      "Epoch [3/3], Step [14500/138038], Loss: 3.0446, Perplexity: 21.0013\n",
      "Epoch [3/3], Step [14600/138038], Loss: 2.2208, Perplexity: 9.21515\n",
      "Epoch [3/3], Step [14700/138038], Loss: 3.0718, Perplexity: 21.5804\n",
      "Epoch [3/3], Step [14800/138038], Loss: 2.2213, Perplexity: 9.219019\n",
      "Epoch [3/3], Step [14900/138038], Loss: 3.9232, Perplexity: 50.5620\n",
      "Epoch [3/3], Step [15000/138038], Loss: 2.9981, Perplexity: 20.0481\n",
      "Epoch [3/3], Step [15100/138038], Loss: 2.9209, Perplexity: 18.5585\n",
      "Epoch [3/3], Step [15200/138038], Loss: 2.7066, Perplexity: 14.9787\n",
      "Epoch [3/3], Step [15300/138038], Loss: 2.7710, Perplexity: 15.9742\n",
      "Epoch [3/3], Step [15400/138038], Loss: 2.6168, Perplexity: 13.6912\n",
      "Epoch [3/3], Step [15500/138038], Loss: 2.1915, Perplexity: 8.94855\n",
      "Epoch [3/3], Step [15600/138038], Loss: 3.2719, Perplexity: 26.3601\n",
      "Epoch [3/3], Step [15700/138038], Loss: 2.6322, Perplexity: 13.90390\n",
      "Epoch [3/3], Step [15800/138038], Loss: 2.3064, Perplexity: 10.0383\n",
      "Epoch [3/3], Step [15900/138038], Loss: 3.4269, Perplexity: 30.7815\n",
      "Epoch [3/3], Step [16000/138038], Loss: 3.5764, Perplexity: 35.7449\n",
      "Epoch [3/3], Step [16100/138038], Loss: 2.4192, Perplexity: 11.23654\n",
      "Epoch [3/3], Step [16200/138038], Loss: 2.4786, Perplexity: 11.92452\n",
      "Epoch [3/3], Step [16300/138038], Loss: 2.5193, Perplexity: 12.4194\n",
      "Epoch [3/3], Step [16400/138038], Loss: 2.9144, Perplexity: 18.43759\n",
      "Epoch [3/3], Step [16500/138038], Loss: 2.4702, Perplexity: 11.82446\n",
      "Epoch [3/3], Step [16600/138038], Loss: 3.2920, Perplexity: 26.89649\n",
      "Epoch [3/3], Step [16700/138038], Loss: 2.5005, Perplexity: 12.1888\n",
      "Epoch [3/3], Step [16800/138038], Loss: 1.8694, Perplexity: 6.48419\n",
      "Epoch [3/3], Step [16900/138038], Loss: 1.6998, Perplexity: 5.47296\n",
      "Epoch [3/3], Step [17000/138038], Loss: 2.1816, Perplexity: 8.86069\n",
      "Epoch [3/3], Step [17100/138038], Loss: 2.2729, Perplexity: 9.70799\n",
      "Epoch [3/3], Step [17200/138038], Loss: 2.7123, Perplexity: 15.0638\n",
      "Epoch [3/3], Step [17300/138038], Loss: 1.9437, Perplexity: 6.984496\n",
      "Epoch [3/3], Step [17400/138038], Loss: 3.0886, Perplexity: 21.9463\n",
      "Epoch [3/3], Step [17500/138038], Loss: 3.3628, Perplexity: 28.8711\n",
      "Epoch [3/3], Step [17600/138038], Loss: 2.2550, Perplexity: 9.535109\n",
      "Epoch [3/3], Step [17700/138038], Loss: 2.2450, Perplexity: 9.44043\n",
      "Epoch [3/3], Step [17800/138038], Loss: 2.1665, Perplexity: 8.72816\n",
      "Epoch [3/3], Step [17900/138038], Loss: 2.5594, Perplexity: 12.9279\n",
      "Epoch [3/3], Step [18000/138038], Loss: 1.7791, Perplexity: 5.924594\n",
      "Epoch [3/3], Step [18100/138038], Loss: 1.5741, Perplexity: 4.82669\n",
      "Epoch [3/3], Step [18200/138038], Loss: 2.8059, Perplexity: 16.54203\n",
      "Epoch [3/3], Step [18300/138038], Loss: 3.1942, Perplexity: 24.3911\n",
      "Epoch [3/3], Step [18400/138038], Loss: 2.2324, Perplexity: 9.321859\n",
      "Epoch [3/3], Step [18500/138038], Loss: 2.1593, Perplexity: 8.66492\n",
      "Epoch [3/3], Step [18600/138038], Loss: 2.2696, Perplexity: 9.67512\n",
      "Epoch [3/3], Step [18700/138038], Loss: 3.3930, Perplexity: 29.7552\n",
      "Epoch [3/3], Step [18800/138038], Loss: 3.4782, Perplexity: 32.40016\n",
      "Epoch [3/3], Step [18900/138038], Loss: 1.8693, Perplexity: 6.483726\n",
      "Epoch [3/3], Step [19000/138038], Loss: 3.4232, Perplexity: 30.66827\n",
      "Epoch [3/3], Step [19100/138038], Loss: 1.7554, Perplexity: 5.785551\n",
      "Epoch [3/3], Step [19200/138038], Loss: 4.4725, Perplexity: 87.5751\n",
      "Epoch [3/3], Step [19300/138038], Loss: 4.1140, Perplexity: 61.1933\n",
      "Epoch [3/3], Step [19400/138038], Loss: 2.2955, Perplexity: 9.929525\n",
      "Epoch [3/3], Step [19500/138038], Loss: 2.4318, Perplexity: 11.3790\n",
      "Epoch [3/3], Step [19600/138038], Loss: 1.9653, Perplexity: 7.13736\n",
      "Epoch [3/3], Step [19700/138038], Loss: 2.5004, Perplexity: 12.1879\n",
      "Epoch [3/3], Step [19800/138038], Loss: 2.7604, Perplexity: 15.8065\n",
      "Epoch [3/3], Step [19900/138038], Loss: 2.9411, Perplexity: 18.93766\n",
      "Epoch [3/3], Step [20000/138038], Loss: 2.9042, Perplexity: 18.2508\n",
      "Epoch [3/3], Step [20100/138038], Loss: 2.4024, Perplexity: 11.05014\n",
      "Epoch [3/3], Step [20200/138038], Loss: 2.4877, Perplexity: 12.0340\n",
      "Epoch [3/3], Step [20300/138038], Loss: 2.3984, Perplexity: 11.00547\n",
      "Epoch [3/3], Step [20400/138038], Loss: 2.2693, Perplexity: 9.67255\n",
      "Epoch [3/3], Step [20500/138038], Loss: 2.0671, Perplexity: 7.90193\n",
      "Epoch [3/3], Step [20600/138038], Loss: 2.7776, Perplexity: 16.0797\n",
      "Epoch [3/3], Step [20700/138038], Loss: 2.2656, Perplexity: 9.63694\n",
      "Epoch [3/3], Step [20800/138038], Loss: 2.6896, Perplexity: 14.72604\n",
      "Epoch [3/3], Step [20900/138038], Loss: 2.1483, Perplexity: 8.570381\n",
      "Epoch [3/3], Step [21000/138038], Loss: 2.9394, Perplexity: 18.9044\n",
      "Epoch [3/3], Step [21100/138038], Loss: 2.6147, Perplexity: 13.6631\n",
      "Epoch [3/3], Step [21200/138038], Loss: 3.7275, Perplexity: 41.57303\n",
      "Epoch [3/3], Step [21300/138038], Loss: 3.4081, Perplexity: 30.2073\n",
      "Epoch [3/3], Step [21400/138038], Loss: 3.3143, Perplexity: 27.50422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [21500/138038], Loss: 2.7843, Perplexity: 16.1880\n",
      "Epoch [3/3], Step [21600/138038], Loss: 1.6459, Perplexity: 5.185802\n",
      "Epoch [3/3], Step [21700/138038], Loss: 1.5069, Perplexity: 4.51286\n",
      "Epoch [3/3], Step [21800/138038], Loss: 4.5578, Perplexity: 95.3731\n",
      "Epoch [3/3], Step [21900/138038], Loss: 3.2098, Perplexity: 24.77374\n",
      "Epoch [3/3], Step [22000/138038], Loss: 3.0901, Perplexity: 21.98040\n",
      "Epoch [3/3], Step [22100/138038], Loss: 2.2300, Perplexity: 9.30017\n",
      "Epoch [3/3], Step [22200/138038], Loss: 2.8574, Perplexity: 17.41626\n",
      "Epoch [3/3], Step [22300/138038], Loss: 3.3614, Perplexity: 28.8290\n",
      "Epoch [3/3], Step [22400/138038], Loss: 2.7502, Perplexity: 15.6454\n",
      "Epoch [3/3], Step [22500/138038], Loss: 3.1710, Perplexity: 23.8307\n",
      "Epoch [3/3], Step [22600/138038], Loss: 2.5554, Perplexity: 12.8761\n",
      "Epoch [3/3], Step [22700/138038], Loss: 3.1968, Perplexity: 24.4543\n",
      "Epoch [3/3], Step [22800/138038], Loss: 1.9687, Perplexity: 7.161253\n",
      "Epoch [3/3], Step [22900/138038], Loss: 2.6449, Perplexity: 14.0826\n",
      "Epoch [3/3], Step [23000/138038], Loss: 2.0907, Perplexity: 8.09094\n",
      "Epoch [3/3], Step [23100/138038], Loss: 2.2171, Perplexity: 9.18099\n",
      "Epoch [3/3], Step [23200/138038], Loss: 2.4538, Perplexity: 11.6319\n",
      "Epoch [3/3], Step [23300/138038], Loss: 3.0366, Perplexity: 20.83370\n",
      "Epoch [3/3], Step [23400/138038], Loss: 2.8495, Perplexity: 17.2784\n",
      "Epoch [3/3], Step [23500/138038], Loss: 2.8451, Perplexity: 17.20325\n",
      "Epoch [3/3], Step [23600/138038], Loss: 2.5444, Perplexity: 12.7353\n",
      "Epoch [3/3], Step [23700/138038], Loss: 3.9075, Perplexity: 49.77377\n",
      "Epoch [3/3], Step [23800/138038], Loss: 2.1109, Perplexity: 8.25544\n",
      "Epoch [3/3], Step [23900/138038], Loss: 4.2647, Perplexity: 71.1462\n",
      "Epoch [3/3], Step [24000/138038], Loss: 3.5936, Perplexity: 36.3632\n",
      "Epoch [3/3], Step [24100/138038], Loss: 2.2731, Perplexity: 9.709999\n",
      "Epoch [3/3], Step [24200/138038], Loss: 3.1737, Perplexity: 23.89538\n",
      "Epoch [3/3], Step [24300/138038], Loss: 1.9702, Perplexity: 7.17243\n",
      "Epoch [3/3], Step [24400/138038], Loss: 2.9619, Perplexity: 19.3351\n",
      "Epoch [3/3], Step [24500/138038], Loss: 2.5867, Perplexity: 13.28607\n",
      "Epoch [3/3], Step [24600/138038], Loss: 2.5414, Perplexity: 12.6980\n",
      "Epoch [3/3], Step [24700/138038], Loss: 2.9838, Perplexity: 19.7622\n",
      "Epoch [3/3], Step [24800/138038], Loss: 2.1883, Perplexity: 8.920039\n",
      "Epoch [3/3], Step [24900/138038], Loss: 2.6638, Perplexity: 14.3512\n",
      "Epoch [3/3], Step [25000/138038], Loss: 2.8623, Perplexity: 17.5013\n",
      "Epoch [3/3], Step [25100/138038], Loss: 2.5459, Perplexity: 12.7542\n",
      "Epoch [3/3], Step [25200/138038], Loss: 1.4515, Perplexity: 4.26950\n",
      "Epoch [3/3], Step [25300/138038], Loss: 2.2598, Perplexity: 9.58128\n",
      "Epoch [3/3], Step [25400/138038], Loss: 2.6439, Perplexity: 14.0676\n",
      "Epoch [3/3], Step [25500/138038], Loss: 3.0257, Perplexity: 20.60942\n",
      "Epoch [3/3], Step [25600/138038], Loss: 2.7883, Perplexity: 16.2528\n",
      "Epoch [3/3], Step [25700/138038], Loss: 2.7396, Perplexity: 15.4812\n",
      "Epoch [3/3], Step [25800/138038], Loss: 3.0509, Perplexity: 21.13497\n",
      "Epoch [3/3], Step [25900/138038], Loss: 2.8635, Perplexity: 17.5225\n",
      "Epoch [3/3], Step [26000/138038], Loss: 2.6204, Perplexity: 13.74181\n",
      "Epoch [3/3], Step [26100/138038], Loss: 2.1317, Perplexity: 8.42882\n",
      "Epoch [3/3], Step [26200/138038], Loss: 2.6845, Perplexity: 14.65083\n",
      "Epoch [3/3], Step [26300/138038], Loss: 2.9792, Perplexity: 19.6714\n",
      "Epoch [3/3], Step [26400/138038], Loss: 2.7662, Perplexity: 15.89812\n",
      "Epoch [3/3], Step [26500/138038], Loss: 3.0644, Perplexity: 21.42075\n",
      "Epoch [3/3], Step [26600/138038], Loss: 1.8444, Perplexity: 6.32446\n",
      "Epoch [3/3], Step [26700/138038], Loss: 2.1498, Perplexity: 8.58328\n",
      "Epoch [3/3], Step [26800/138038], Loss: 2.5833, Perplexity: 13.24081\n",
      "Epoch [3/3], Step [26900/138038], Loss: 2.6046, Perplexity: 13.5261\n",
      "Epoch [3/3], Step [27000/138038], Loss: 2.4056, Perplexity: 11.08553\n",
      "Epoch [3/3], Step [27100/138038], Loss: 2.5005, Perplexity: 12.1888\n",
      "Epoch [3/3], Step [27200/138038], Loss: 2.5821, Perplexity: 13.2251\n",
      "Epoch [3/3], Step [27300/138038], Loss: 1.9869, Perplexity: 7.29323\n",
      "Epoch [3/3], Step [27400/138038], Loss: 2.4044, Perplexity: 11.07219\n",
      "Epoch [3/3], Step [27500/138038], Loss: 2.9167, Perplexity: 18.4808\n",
      "Epoch [3/3], Step [27600/138038], Loss: 2.4812, Perplexity: 11.9550\n",
      "Epoch [3/3], Step [27700/138038], Loss: 2.4470, Perplexity: 11.5533\n",
      "Epoch [3/3], Step [27800/138038], Loss: 1.5729, Perplexity: 4.820458\n",
      "Epoch [3/3], Step [27900/138038], Loss: 2.7844, Perplexity: 16.19031\n",
      "Epoch [3/3], Step [28000/138038], Loss: 3.3206, Perplexity: 27.6783\n",
      "Epoch [3/3], Step [28100/138038], Loss: 3.2489, Perplexity: 25.76117\n",
      "Epoch [3/3], Step [28200/138038], Loss: 3.2656, Perplexity: 26.1969\n",
      "Epoch [3/3], Step [28300/138038], Loss: 1.8455, Perplexity: 6.33154\n",
      "Epoch [3/3], Step [28400/138038], Loss: 1.6558, Perplexity: 5.23728\n",
      "Epoch [3/3], Step [28500/138038], Loss: 2.9407, Perplexity: 18.92926\n",
      "Epoch [3/3], Step [28600/138038], Loss: 2.7857, Perplexity: 16.2106\n",
      "Epoch [3/3], Step [28700/138038], Loss: 3.2838, Perplexity: 26.6779\n",
      "Epoch [3/3], Step [28800/138038], Loss: 2.7131, Perplexity: 15.0760\n",
      "Epoch [3/3], Step [28900/138038], Loss: 3.1254, Perplexity: 22.7691\n",
      "Epoch [3/3], Step [29000/138038], Loss: 2.8631, Perplexity: 17.5165\n",
      "Epoch [3/3], Step [29100/138038], Loss: 2.3853, Perplexity: 10.86261\n",
      "Epoch [3/3], Step [29200/138038], Loss: 4.0243, Perplexity: 55.94039\n",
      "Epoch [3/3], Step [29300/138038], Loss: 2.9777, Perplexity: 19.64354\n",
      "Epoch [3/3], Step [29400/138038], Loss: 3.2918, Perplexity: 26.89105\n",
      "Epoch [3/3], Step [29500/138038], Loss: 2.4397, Perplexity: 11.46954\n",
      "Epoch [3/3], Step [29600/138038], Loss: 2.5952, Perplexity: 13.3992\n",
      "Epoch [3/3], Step [29700/138038], Loss: 2.9392, Perplexity: 18.9003\n",
      "Epoch [3/3], Step [29800/138038], Loss: 3.9165, Perplexity: 50.2249\n",
      "Epoch [3/3], Step [29900/138038], Loss: 2.2223, Perplexity: 9.22885\n",
      "Epoch [3/3], Step [30000/138038], Loss: 2.7782, Perplexity: 16.0898\n",
      "Epoch [3/3], Step [30100/138038], Loss: 3.2193, Perplexity: 25.0106\n",
      "Epoch [3/3], Step [30200/138038], Loss: 1.8949, Perplexity: 6.65163\n",
      "Epoch [3/3], Step [30300/138038], Loss: 2.6515, Perplexity: 14.1753\n",
      "Epoch [3/3], Step [30400/138038], Loss: 2.6660, Perplexity: 14.38229\n",
      "Epoch [3/3], Step [30500/138038], Loss: 3.4856, Perplexity: 32.6424\n",
      "Epoch [3/3], Step [30600/138038], Loss: 2.6361, Perplexity: 13.9584\n",
      "Epoch [3/3], Step [30700/138038], Loss: 3.3302, Perplexity: 27.9445\n",
      "Epoch [3/3], Step [30800/138038], Loss: 3.2850, Perplexity: 26.70915\n",
      "Epoch [3/3], Step [30900/138038], Loss: 2.9480, Perplexity: 19.0673\n",
      "Epoch [3/3], Step [31000/138038], Loss: 2.8042, Perplexity: 16.5141\n",
      "Epoch [3/3], Step [31100/138038], Loss: 3.4419, Perplexity: 31.2472\n",
      "Epoch [3/3], Step [31200/138038], Loss: 2.6757, Perplexity: 14.5224\n",
      "Epoch [3/3], Step [31300/138038], Loss: 2.8637, Perplexity: 17.52553\n",
      "Epoch [3/3], Step [31400/138038], Loss: 2.4421, Perplexity: 11.4966\n",
      "Epoch [3/3], Step [31500/138038], Loss: 2.1875, Perplexity: 8.912989\n",
      "Epoch [3/3], Step [31600/138038], Loss: 2.3714, Perplexity: 10.7124\n",
      "Epoch [3/3], Step [31700/138038], Loss: 2.4206, Perplexity: 11.2527\n",
      "Epoch [3/3], Step [31800/138038], Loss: 4.2926, Perplexity: 73.1536\n",
      "Epoch [3/3], Step [31900/138038], Loss: 2.1602, Perplexity: 8.672691\n",
      "Epoch [3/3], Step [32000/138038], Loss: 3.4883, Perplexity: 32.7295\n",
      "Epoch [3/3], Step [32100/138038], Loss: 2.5185, Perplexity: 12.4104\n",
      "Epoch [3/3], Step [32200/138038], Loss: 1.9050, Perplexity: 6.71958\n",
      "Epoch [3/3], Step [32300/138038], Loss: 3.0406, Perplexity: 20.9180\n",
      "Epoch [3/3], Step [32400/138038], Loss: 2.5936, Perplexity: 13.3779\n",
      "Epoch [3/3], Step [32500/138038], Loss: 2.4571, Perplexity: 11.6710\n",
      "Epoch [3/3], Step [32600/138038], Loss: 2.2256, Perplexity: 9.25899\n",
      "Epoch [3/3], Step [32700/138038], Loss: 3.3442, Perplexity: 28.3387\n",
      "Epoch [3/3], Step [32800/138038], Loss: 2.7422, Perplexity: 15.5207\n",
      "Epoch [3/3], Step [32900/138038], Loss: 2.8455, Perplexity: 17.2102\n",
      "Epoch [3/3], Step [33000/138038], Loss: 2.8121, Perplexity: 16.6444\n",
      "Epoch [3/3], Step [33100/138038], Loss: 3.9064, Perplexity: 49.7199\n",
      "Epoch [3/3], Step [33200/138038], Loss: 1.7288, Perplexity: 5.63390\n",
      "Epoch [3/3], Step [33300/138038], Loss: 3.2881, Perplexity: 26.79285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [33400/138038], Loss: 3.4567, Perplexity: 31.7135\n",
      "Epoch [3/3], Step [33500/138038], Loss: 2.2954, Perplexity: 9.928322\n",
      "Epoch [3/3], Step [33600/138038], Loss: 2.3904, Perplexity: 10.91842\n",
      "Epoch [3/3], Step [33700/138038], Loss: 2.5187, Perplexity: 12.41236\n",
      "Epoch [3/3], Step [33800/138038], Loss: 1.8104, Perplexity: 6.113083\n",
      "Epoch [3/3], Step [33900/138038], Loss: 1.9865, Perplexity: 7.28980\n",
      "Epoch [3/3], Step [34000/138038], Loss: 2.6739, Perplexity: 14.4957\n",
      "Epoch [3/3], Step [34100/138038], Loss: 2.4761, Perplexity: 11.8950\n",
      "Epoch [3/3], Step [34200/138038], Loss: 1.6291, Perplexity: 5.09933\n",
      "Epoch [3/3], Step [34300/138038], Loss: 2.9785, Perplexity: 19.65765\n",
      "Epoch [3/3], Step [34400/138038], Loss: 2.3341, Perplexity: 10.3206\n",
      "Epoch [3/3], Step [34500/138038], Loss: 2.4147, Perplexity: 11.1860\n",
      "Epoch [3/3], Step [34600/138038], Loss: 2.0488, Perplexity: 7.758885\n",
      "Epoch [3/3], Step [34700/138038], Loss: 2.2037, Perplexity: 9.05802\n",
      "Epoch [3/3], Step [34800/138038], Loss: 2.3355, Perplexity: 10.33491\n",
      "Epoch [3/3], Step [34900/138038], Loss: 2.8211, Perplexity: 16.79507\n",
      "Epoch [3/3], Step [35000/138038], Loss: 1.8161, Perplexity: 6.14750\n",
      "Epoch [3/3], Step [35100/138038], Loss: 2.3132, Perplexity: 10.1072\n",
      "Epoch [3/3], Step [35200/138038], Loss: 3.4990, Perplexity: 33.08073\n",
      "Epoch [3/3], Step [35300/138038], Loss: 2.6220, Perplexity: 13.76322\n",
      "Epoch [3/3], Step [35400/138038], Loss: 2.3751, Perplexity: 10.75203\n",
      "Epoch [3/3], Step [35500/138038], Loss: 2.5316, Perplexity: 12.5739\n",
      "Epoch [3/3], Step [35600/138038], Loss: 2.4042, Perplexity: 11.0695\n",
      "Epoch [3/3], Step [35700/138038], Loss: 2.0266, Perplexity: 7.588339\n",
      "Epoch [3/3], Step [35800/138038], Loss: 2.1610, Perplexity: 8.679982\n",
      "Epoch [3/3], Step [35900/138038], Loss: 2.7452, Perplexity: 15.5684\n",
      "Epoch [3/3], Step [36000/138038], Loss: 2.2201, Perplexity: 9.20794\n",
      "Epoch [3/3], Step [36100/138038], Loss: 3.6603, Perplexity: 38.87147\n",
      "Epoch [3/3], Step [36200/138038], Loss: 2.2035, Perplexity: 9.05680\n",
      "Epoch [3/3], Step [36300/138038], Loss: 2.9604, Perplexity: 19.30496\n",
      "Epoch [3/3], Step [36400/138038], Loss: 3.0225, Perplexity: 20.5416\n",
      "Epoch [3/3], Step [36500/138038], Loss: 2.3407, Perplexity: 10.3881\n",
      "Epoch [3/3], Step [36600/138038], Loss: 2.5793, Perplexity: 13.1875\n",
      "Epoch [3/3], Step [36700/138038], Loss: 2.8608, Perplexity: 17.4747\n",
      "Epoch [3/3], Step [36800/138038], Loss: 2.7290, Perplexity: 15.3181\n",
      "Epoch [3/3], Step [36900/138038], Loss: 2.7793, Perplexity: 16.1070\n",
      "Epoch [3/3], Step [37000/138038], Loss: 2.8367, Perplexity: 17.0591\n",
      "Epoch [3/3], Step [37100/138038], Loss: 2.6149, Perplexity: 13.6658\n",
      "Epoch [3/3], Step [37200/138038], Loss: 1.6269, Perplexity: 5.08799\n",
      "Epoch [3/3], Step [37300/138038], Loss: 1.7238, Perplexity: 5.605719\n",
      "Epoch [3/3], Step [37400/138038], Loss: 3.0304, Perplexity: 20.7060\n",
      "Epoch [3/3], Step [37500/138038], Loss: 2.5509, Perplexity: 12.8192\n",
      "Epoch [3/3], Step [37600/138038], Loss: 3.0074, Perplexity: 20.2347\n",
      "Epoch [3/3], Step [37700/138038], Loss: 2.6371, Perplexity: 13.9721\n",
      "Epoch [3/3], Step [37800/138038], Loss: 2.1226, Perplexity: 8.35279\n",
      "Epoch [3/3], Step [37900/138038], Loss: 2.4109, Perplexity: 11.1442\n",
      "Epoch [3/3], Step [38000/138038], Loss: 3.5134, Perplexity: 33.5622\n",
      "Epoch [3/3], Step [38100/138038], Loss: 3.3427, Perplexity: 28.2949\n",
      "Epoch [3/3], Step [38200/138038], Loss: 3.5240, Perplexity: 33.9187\n",
      "Epoch [3/3], Step [38300/138038], Loss: 1.8608, Perplexity: 6.429064\n",
      "Epoch [3/3], Step [38400/138038], Loss: 2.6810, Perplexity: 14.60017\n",
      "Epoch [3/3], Step [38500/138038], Loss: 2.1589, Perplexity: 8.66133\n",
      "Epoch [3/3], Step [38600/138038], Loss: 2.2991, Perplexity: 9.96527\n",
      "Epoch [3/3], Step [38700/138038], Loss: 2.5475, Perplexity: 12.7755\n",
      "Epoch [3/3], Step [38800/138038], Loss: 2.7173, Perplexity: 15.1393\n",
      "Epoch [3/3], Step [38900/138038], Loss: 1.9817, Perplexity: 7.25521\n",
      "Epoch [3/3], Step [39000/138038], Loss: 3.2551, Perplexity: 25.9219\n",
      "Epoch [3/3], Step [39100/138038], Loss: 2.9944, Perplexity: 19.97379\n",
      "Epoch [3/3], Step [39200/138038], Loss: 3.0239, Perplexity: 20.57113\n",
      "Epoch [3/3], Step [39300/138038], Loss: 2.3585, Perplexity: 10.57464\n",
      "Epoch [3/3], Step [39400/138038], Loss: 3.1276, Perplexity: 22.8195\n",
      "Epoch [3/3], Step [39500/138038], Loss: 3.5916, Perplexity: 36.2907\n",
      "Epoch [3/3], Step [39600/138038], Loss: 1.8811, Perplexity: 6.560812\n",
      "Epoch [3/3], Step [39700/138038], Loss: 2.5976, Perplexity: 13.43082\n",
      "Epoch [3/3], Step [39800/138038], Loss: 2.2907, Perplexity: 9.88222\n",
      "Epoch [3/3], Step [39900/138038], Loss: 2.4087, Perplexity: 11.1196\n",
      "Epoch [3/3], Step [40000/138038], Loss: 2.4680, Perplexity: 11.7993\n",
      "Epoch [3/3], Step [40100/138038], Loss: 3.2005, Perplexity: 24.5448\n",
      "Epoch [3/3], Step [40200/138038], Loss: 2.0861, Perplexity: 8.053539\n",
      "Epoch [3/3], Step [40300/138038], Loss: 2.7368, Perplexity: 15.4375\n",
      "Epoch [3/3], Step [40400/138038], Loss: 2.8945, Perplexity: 18.07442\n",
      "Epoch [3/3], Step [40500/138038], Loss: 2.6341, Perplexity: 13.9311\n",
      "Epoch [3/3], Step [40600/138038], Loss: 3.6164, Perplexity: 37.2023\n",
      "Epoch [3/3], Step [40700/138038], Loss: 2.8008, Perplexity: 16.45758\n",
      "Epoch [3/3], Step [40800/138038], Loss: 2.9539, Perplexity: 19.18053\n",
      "Epoch [3/3], Step [40900/138038], Loss: 1.9132, Perplexity: 6.774703\n",
      "Epoch [3/3], Step [41000/138038], Loss: 2.2861, Perplexity: 9.83675\n",
      "Epoch [3/3], Step [41100/138038], Loss: 3.7708, Perplexity: 43.41574\n",
      "Epoch [3/3], Step [41200/138038], Loss: 4.0769, Perplexity: 58.9641\n",
      "Epoch [3/3], Step [41300/138038], Loss: 1.8972, Perplexity: 6.66700\n",
      "Epoch [3/3], Step [41400/138038], Loss: 2.7778, Perplexity: 16.08383\n",
      "Epoch [3/3], Step [41500/138038], Loss: 2.5302, Perplexity: 12.5565\n",
      "Epoch [3/3], Step [41600/138038], Loss: 4.6770, Perplexity: 107.4454\n",
      "Epoch [3/3], Step [41700/138038], Loss: 2.6536, Perplexity: 14.20491\n",
      "Epoch [3/3], Step [41800/138038], Loss: 2.9820, Perplexity: 19.72660\n",
      "Epoch [3/3], Step [41900/138038], Loss: 3.3785, Perplexity: 29.3275\n",
      "Epoch [3/3], Step [42000/138038], Loss: 2.9104, Perplexity: 18.36478\n",
      "Epoch [3/3], Step [42100/138038], Loss: 2.5593, Perplexity: 12.9266\n",
      "Epoch [3/3], Step [42200/138038], Loss: 2.8306, Perplexity: 16.9550\n",
      "Epoch [3/3], Step [42300/138038], Loss: 2.6521, Perplexity: 14.18320\n",
      "Epoch [3/3], Step [42400/138038], Loss: 2.6001, Perplexity: 13.4648\n",
      "Epoch [3/3], Step [42500/138038], Loss: 3.3544, Perplexity: 28.6278\n",
      "Epoch [3/3], Step [42600/138038], Loss: 1.6891, Perplexity: 5.414612\n",
      "Epoch [3/3], Step [42700/138038], Loss: 2.8141, Perplexity: 16.6785\n",
      "Epoch [3/3], Step [42800/138038], Loss: 2.9702, Perplexity: 19.4960\n",
      "Epoch [3/3], Step [42900/138038], Loss: 3.0351, Perplexity: 20.80213\n",
      "Epoch [3/3], Step [43000/138038], Loss: 3.2617, Perplexity: 26.09301\n",
      "Epoch [3/3], Step [43100/138038], Loss: 2.4044, Perplexity: 11.0713\n",
      "Epoch [3/3], Step [43200/138038], Loss: 1.7702, Perplexity: 5.87237\n",
      "Epoch [3/3], Step [43300/138038], Loss: 3.0512, Perplexity: 21.1406\n",
      "Epoch [3/3], Step [43400/138038], Loss: 2.7847, Perplexity: 16.1946\n",
      "Epoch [3/3], Step [43500/138038], Loss: 2.1983, Perplexity: 9.00973\n",
      "Epoch [3/3], Step [43600/138038], Loss: 2.5372, Perplexity: 12.6448\n",
      "Epoch [3/3], Step [43700/138038], Loss: 1.8219, Perplexity: 6.18333\n",
      "Epoch [3/3], Step [43800/138038], Loss: 3.3595, Perplexity: 28.7759\n",
      "Epoch [3/3], Step [43900/138038], Loss: 2.1159, Perplexity: 8.297176\n",
      "Epoch [3/3], Step [44000/138038], Loss: 2.3509, Perplexity: 10.49475\n",
      "Epoch [3/3], Step [44100/138038], Loss: 1.8736, Perplexity: 6.51165\n",
      "Epoch [3/3], Step [44200/138038], Loss: 1.9200, Perplexity: 6.82083\n",
      "Epoch [3/3], Step [44300/138038], Loss: 2.2614, Perplexity: 9.596284\n",
      "Epoch [3/3], Step [44400/138038], Loss: 2.5713, Perplexity: 13.0830\n",
      "Epoch [3/3], Step [44500/138038], Loss: 1.7785, Perplexity: 5.92082\n",
      "Epoch [3/3], Step [44600/138038], Loss: 2.7915, Perplexity: 16.30588\n",
      "Epoch [3/3], Step [44700/138038], Loss: 4.4486, Perplexity: 85.50760\n",
      "Epoch [3/3], Step [44800/138038], Loss: 2.4176, Perplexity: 11.21940\n",
      "Epoch [3/3], Step [44900/138038], Loss: 1.4601, Perplexity: 4.30625\n",
      "Epoch [3/3], Step [45000/138038], Loss: 3.0627, Perplexity: 21.3843\n",
      "Epoch [3/3], Step [45100/138038], Loss: 3.3399, Perplexity: 28.2153\n",
      "Epoch [3/3], Step [45200/138038], Loss: 2.8143, Perplexity: 16.68135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [45300/138038], Loss: 2.0043, Perplexity: 7.42116\n",
      "Epoch [3/3], Step [45400/138038], Loss: 3.5921, Perplexity: 36.31177\n",
      "Epoch [3/3], Step [45500/138038], Loss: 3.3396, Perplexity: 28.20767\n",
      "Epoch [3/3], Step [45600/138038], Loss: 2.1929, Perplexity: 8.96082\n",
      "Epoch [3/3], Step [45700/138038], Loss: 2.2574, Perplexity: 9.55803\n",
      "Epoch [3/3], Step [45800/138038], Loss: 2.4090, Perplexity: 11.1228\n",
      "Epoch [3/3], Step [45900/138038], Loss: 2.7093, Perplexity: 15.0194\n",
      "Epoch [3/3], Step [46000/138038], Loss: 3.6146, Perplexity: 37.1370\n",
      "Epoch [3/3], Step [46100/138038], Loss: 2.8565, Perplexity: 17.3997\n",
      "Epoch [3/3], Step [46200/138038], Loss: 3.5038, Perplexity: 33.24042\n",
      "Epoch [3/3], Step [46300/138038], Loss: 2.1837, Perplexity: 8.87885\n",
      "Epoch [3/3], Step [46400/138038], Loss: 3.4790, Perplexity: 32.4281\n",
      "Epoch [3/3], Step [46500/138038], Loss: 2.8349, Perplexity: 17.0294\n",
      "Epoch [3/3], Step [46600/138038], Loss: 2.8984, Perplexity: 18.1456\n",
      "Epoch [3/3], Step [46700/138038], Loss: 1.6440, Perplexity: 5.175653\n",
      "Epoch [3/3], Step [46800/138038], Loss: 2.2896, Perplexity: 9.87147\n",
      "Epoch [3/3], Step [46900/138038], Loss: 1.9057, Perplexity: 6.72449\n",
      "Epoch [3/3], Step [47000/138038], Loss: 2.7996, Perplexity: 16.4379\n",
      "Epoch [3/3], Step [47100/138038], Loss: 3.6175, Perplexity: 37.24518\n",
      "Epoch [3/3], Step [47200/138038], Loss: 2.7622, Perplexity: 15.8339\n",
      "Epoch [3/3], Step [47300/138038], Loss: 3.3157, Perplexity: 27.54152\n",
      "Epoch [3/3], Step [47400/138038], Loss: 1.8869, Perplexity: 6.598801\n",
      "Epoch [3/3], Step [47500/138038], Loss: 3.3022, Perplexity: 27.1733\n",
      "Epoch [3/3], Step [47600/138038], Loss: 3.1498, Perplexity: 23.3324\n",
      "Epoch [3/3], Step [47700/138038], Loss: 3.8767, Perplexity: 48.26328\n",
      "Epoch [3/3], Step [47800/138038], Loss: 2.9286, Perplexity: 18.7010\n",
      "Epoch [3/3], Step [47900/138038], Loss: 2.7397, Perplexity: 15.48206\n",
      "Epoch [3/3], Step [48000/138038], Loss: 3.6045, Perplexity: 36.7622\n",
      "Epoch [3/3], Step [48100/138038], Loss: 1.5859, Perplexity: 4.883837\n",
      "Epoch [3/3], Step [48200/138038], Loss: 3.0516, Perplexity: 21.14914\n",
      "Epoch [3/3], Step [48300/138038], Loss: 2.5636, Perplexity: 12.98253\n",
      "Epoch [3/3], Step [48400/138038], Loss: 2.9448, Perplexity: 19.0077\n",
      "Epoch [3/3], Step [48500/138038], Loss: 2.7315, Perplexity: 15.3560\n",
      "Epoch [3/3], Step [48600/138038], Loss: 3.2821, Perplexity: 26.6310\n",
      "Epoch [3/3], Step [48700/138038], Loss: 3.0623, Perplexity: 21.37583\n",
      "Epoch [3/3], Step [48800/138038], Loss: 3.1022, Perplexity: 22.2469\n",
      "Epoch [3/3], Step [48900/138038], Loss: 3.0452, Perplexity: 21.01414\n",
      "Epoch [3/3], Step [49000/138038], Loss: 4.2630, Perplexity: 71.02202\n",
      "Epoch [3/3], Step [49100/138038], Loss: 2.3232, Perplexity: 10.20796\n",
      "Epoch [3/3], Step [49200/138038], Loss: 2.5143, Perplexity: 12.3581\n",
      "Epoch [3/3], Step [49300/138038], Loss: 2.6079, Perplexity: 13.5702\n",
      "Epoch [3/3], Step [49400/138038], Loss: 1.9329, Perplexity: 6.90945\n",
      "Epoch [3/3], Step [49500/138038], Loss: 1.9034, Perplexity: 6.70842\n",
      "Epoch [3/3], Step [49600/138038], Loss: 1.6760, Perplexity: 5.34439\n",
      "Epoch [3/3], Step [49700/138038], Loss: 3.2013, Perplexity: 24.56439\n",
      "Epoch [3/3], Step [49800/138038], Loss: 1.5844, Perplexity: 4.876590\n",
      "Epoch [3/3], Step [49900/138038], Loss: 3.1313, Perplexity: 22.9045\n",
      "Epoch [3/3], Step [50000/138038], Loss: 1.8840, Perplexity: 6.57951\n",
      "Epoch [3/3], Step [50100/138038], Loss: 2.8458, Perplexity: 17.21573\n",
      "Epoch [3/3], Step [50200/138038], Loss: 2.1715, Perplexity: 8.77136\n",
      "Epoch [3/3], Step [50300/138038], Loss: 2.5220, Perplexity: 12.45330\n",
      "Epoch [3/3], Step [50400/138038], Loss: 2.5723, Perplexity: 13.0962\n",
      "Epoch [3/3], Step [50500/138038], Loss: 2.5240, Perplexity: 12.4781\n",
      "Epoch [3/3], Step [50600/138038], Loss: 2.1385, Perplexity: 8.48690\n",
      "Epoch [3/3], Step [50700/138038], Loss: 2.5529, Perplexity: 12.8442\n",
      "Epoch [3/3], Step [50800/138038], Loss: 2.5511, Perplexity: 12.8217\n",
      "Epoch [3/3], Step [50900/138038], Loss: 2.3205, Perplexity: 10.18036\n",
      "Epoch [3/3], Step [51000/138038], Loss: 2.7250, Perplexity: 15.2559\n",
      "Epoch [3/3], Step [51100/138038], Loss: 2.5111, Perplexity: 12.3184\n",
      "Epoch [3/3], Step [51200/138038], Loss: 2.1812, Perplexity: 8.85687\n",
      "Epoch [3/3], Step [51300/138038], Loss: 3.0376, Perplexity: 20.8560\n",
      "Epoch [3/3], Step [51400/138038], Loss: 2.1734, Perplexity: 8.78858\n",
      "Epoch [3/3], Step [51500/138038], Loss: 3.1923, Perplexity: 24.3439\n",
      "Epoch [3/3], Step [51600/138038], Loss: 2.9229, Perplexity: 18.5944\n",
      "Epoch [3/3], Step [51700/138038], Loss: 3.5056, Perplexity: 33.30103\n",
      "Epoch [3/3], Step [51800/138038], Loss: 4.1217, Perplexity: 61.6622\n",
      "Epoch [3/3], Step [51900/138038], Loss: 3.7467, Perplexity: 42.37963\n",
      "Epoch [3/3], Step [52000/138038], Loss: 3.3706, Perplexity: 29.0968\n",
      "Epoch [3/3], Step [52100/138038], Loss: 3.4775, Perplexity: 32.3783\n",
      "Epoch [3/3], Step [52200/138038], Loss: 1.7923, Perplexity: 6.00325\n",
      "Epoch [3/3], Step [52300/138038], Loss: 2.6499, Perplexity: 14.1533\n",
      "Epoch [3/3], Step [52400/138038], Loss: 4.3880, Perplexity: 80.4795\n",
      "Epoch [3/3], Step [52500/138038], Loss: 2.0747, Perplexity: 7.962564\n",
      "Epoch [3/3], Step [52600/138038], Loss: 2.8591, Perplexity: 17.44599\n",
      "Epoch [3/3], Step [52700/138038], Loss: 1.8420, Perplexity: 6.30916\n",
      "Epoch [3/3], Step [52800/138038], Loss: 2.5616, Perplexity: 12.9559\n",
      "Epoch [3/3], Step [52900/138038], Loss: 3.6917, Perplexity: 40.1128\n",
      "Epoch [3/3], Step [53000/138038], Loss: 2.6952, Perplexity: 14.8089\n",
      "Epoch [3/3], Step [53100/138038], Loss: 3.0517, Perplexity: 21.1505\n",
      "Epoch [3/3], Step [53200/138038], Loss: 2.3475, Perplexity: 10.4595\n",
      "Epoch [3/3], Step [53300/138038], Loss: 1.5712, Perplexity: 4.812402\n",
      "Epoch [3/3], Step [53400/138038], Loss: 2.3056, Perplexity: 10.0305\n",
      "Epoch [3/3], Step [53500/138038], Loss: 2.3715, Perplexity: 10.71321\n",
      "Epoch [3/3], Step [53600/138038], Loss: 2.2744, Perplexity: 9.722545\n",
      "Epoch [3/3], Step [53700/138038], Loss: 2.1067, Perplexity: 8.221407\n",
      "Epoch [3/3], Step [53800/138038], Loss: 3.3688, Perplexity: 29.0445\n",
      "Epoch [3/3], Step [53900/138038], Loss: 2.2742, Perplexity: 9.71998\n",
      "Epoch [3/3], Step [54000/138038], Loss: 2.3811, Perplexity: 10.8167\n",
      "Epoch [3/3], Step [54100/138038], Loss: 3.8294, Perplexity: 46.0329\n",
      "Epoch [3/3], Step [54200/138038], Loss: 3.0498, Perplexity: 21.1113\n",
      "Epoch [3/3], Step [54300/138038], Loss: 2.8294, Perplexity: 16.9352\n",
      "Epoch [3/3], Step [54400/138038], Loss: 3.1507, Perplexity: 23.3515\n",
      "Epoch [3/3], Step [54500/138038], Loss: 3.4194, Perplexity: 30.55214\n",
      "Epoch [3/3], Step [54600/138038], Loss: 2.3211, Perplexity: 10.1869\n",
      "Epoch [3/3], Step [54700/138038], Loss: 1.7349, Perplexity: 5.668411\n",
      "Epoch [3/3], Step [54800/138038], Loss: 2.0286, Perplexity: 7.60377\n",
      "Epoch [3/3], Step [54900/138038], Loss: 1.9350, Perplexity: 6.92377\n",
      "Epoch [3/3], Step [55000/138038], Loss: 2.9267, Perplexity: 18.66602\n",
      "Epoch [3/3], Step [55100/138038], Loss: 2.8550, Perplexity: 17.3743\n",
      "Epoch [3/3], Step [55200/138038], Loss: 2.9152, Perplexity: 18.45311\n",
      "Epoch [3/3], Step [55300/138038], Loss: 2.2614, Perplexity: 9.59690\n",
      "Epoch [3/3], Step [55400/138038], Loss: 4.4166, Perplexity: 82.81604\n",
      "Epoch [3/3], Step [55500/138038], Loss: 2.8604, Perplexity: 17.4680\n",
      "Epoch [3/3], Step [55600/138038], Loss: 3.8444, Perplexity: 46.7329\n",
      "Epoch [3/3], Step [55700/138038], Loss: 1.8091, Perplexity: 6.104776\n",
      "Epoch [3/3], Step [55800/138038], Loss: 3.4577, Perplexity: 31.7430\n",
      "Epoch [3/3], Step [55900/138038], Loss: 2.1157, Perplexity: 8.29535\n",
      "Epoch [3/3], Step [56000/138038], Loss: 3.1681, Perplexity: 23.7614\n",
      "Epoch [3/3], Step [56100/138038], Loss: 3.4612, Perplexity: 31.8557\n",
      "Epoch [3/3], Step [56200/138038], Loss: 4.4132, Perplexity: 82.5339\n",
      "Epoch [3/3], Step [56300/138038], Loss: 3.2502, Perplexity: 25.7961\n",
      "Epoch [3/3], Step [56400/138038], Loss: 3.7589, Perplexity: 42.90252\n",
      "Epoch [3/3], Step [56500/138038], Loss: 2.4925, Perplexity: 12.0919\n",
      "Epoch [3/3], Step [56600/138038], Loss: 1.9171, Perplexity: 6.80097\n",
      "Epoch [3/3], Step [56700/138038], Loss: 1.6717, Perplexity: 5.321228\n",
      "Epoch [3/3], Step [56800/138038], Loss: 3.8191, Perplexity: 45.56379\n",
      "Epoch [3/3], Step [56900/138038], Loss: 2.1603, Perplexity: 8.67383\n",
      "Epoch [3/3], Step [57000/138038], Loss: 2.6681, Perplexity: 14.4120\n",
      "Epoch [3/3], Step [57100/138038], Loss: 2.7079, Perplexity: 14.9973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [57200/138038], Loss: 2.6931, Perplexity: 14.77790\n",
      "Epoch [3/3], Step [57300/138038], Loss: 2.4147, Perplexity: 11.1868\n",
      "Epoch [3/3], Step [57400/138038], Loss: 3.7963, Perplexity: 44.53475\n",
      "Epoch [3/3], Step [57500/138038], Loss: 1.8945, Perplexity: 6.649050\n",
      "Epoch [3/3], Step [57600/138038], Loss: 1.7506, Perplexity: 5.758338\n",
      "Epoch [3/3], Step [57700/138038], Loss: 3.9555, Perplexity: 52.22129\n",
      "Epoch [3/3], Step [57800/138038], Loss: 2.4012, Perplexity: 11.0369\n",
      "Epoch [3/3], Step [57900/138038], Loss: 2.4166, Perplexity: 11.2078\n",
      "Epoch [3/3], Step [58000/138038], Loss: 2.7511, Perplexity: 15.6600\n",
      "Epoch [3/3], Step [58100/138038], Loss: 3.0152, Perplexity: 20.39299\n",
      "Epoch [3/3], Step [58200/138038], Loss: 2.0535, Perplexity: 7.795295\n",
      "Epoch [3/3], Step [58300/138038], Loss: 1.7628, Perplexity: 5.82895\n",
      "Epoch [3/3], Step [58400/138038], Loss: 1.9892, Perplexity: 7.30947\n",
      "Epoch [3/3], Step [58500/138038], Loss: 1.8210, Perplexity: 6.17811\n",
      "Epoch [3/3], Step [58600/138038], Loss: 2.7175, Perplexity: 15.14204\n",
      "Epoch [3/3], Step [58700/138038], Loss: 2.2890, Perplexity: 9.86497\n",
      "Epoch [3/3], Step [58800/138038], Loss: 1.8332, Perplexity: 6.253824\n",
      "Epoch [3/3], Step [58900/138038], Loss: 1.8244, Perplexity: 6.19937\n",
      "Epoch [3/3], Step [59000/138038], Loss: 2.1403, Perplexity: 8.50219\n",
      "Epoch [3/3], Step [59100/138038], Loss: 3.8128, Perplexity: 45.27540\n",
      "Epoch [3/3], Step [59200/138038], Loss: 3.2324, Perplexity: 25.33984\n",
      "Epoch [3/3], Step [59300/138038], Loss: 2.3332, Perplexity: 10.3109\n",
      "Epoch [3/3], Step [59400/138038], Loss: 2.2486, Perplexity: 9.47446\n",
      "Epoch [3/3], Step [59500/138038], Loss: 2.5717, Perplexity: 13.0878\n",
      "Epoch [3/3], Step [59600/138038], Loss: 2.8594, Perplexity: 17.4519\n",
      "Epoch [3/3], Step [59700/138038], Loss: 2.3063, Perplexity: 10.0373\n",
      "Epoch [3/3], Step [59800/138038], Loss: 2.1686, Perplexity: 8.74617\n",
      "Epoch [3/3], Step [59900/138038], Loss: 2.4627, Perplexity: 11.7361\n",
      "Epoch [3/3], Step [60000/138038], Loss: 2.5280, Perplexity: 12.5290\n",
      "Epoch [3/3], Step [60100/138038], Loss: 2.5124, Perplexity: 12.3343\n",
      "Epoch [3/3], Step [60200/138038], Loss: 2.7884, Perplexity: 16.2547\n",
      "Epoch [3/3], Step [60300/138038], Loss: 2.2280, Perplexity: 9.28158\n",
      "Epoch [3/3], Step [60400/138038], Loss: 2.0282, Perplexity: 7.60020\n",
      "Epoch [3/3], Step [60500/138038], Loss: 2.3795, Perplexity: 10.7999\n",
      "Epoch [3/3], Step [60600/138038], Loss: 1.2693, Perplexity: 3.55824\n",
      "Epoch [3/3], Step [60700/138038], Loss: 2.2634, Perplexity: 9.61622\n",
      "Epoch [3/3], Step [60800/138038], Loss: 2.3249, Perplexity: 10.22520\n",
      "Epoch [3/3], Step [60900/138038], Loss: 2.6914, Perplexity: 14.7516\n",
      "Epoch [3/3], Step [61000/138038], Loss: 2.5445, Perplexity: 12.7373\n",
      "Epoch [3/3], Step [61100/138038], Loss: 3.0860, Perplexity: 21.89032\n",
      "Epoch [3/3], Step [61200/138038], Loss: 1.8151, Perplexity: 6.14203\n",
      "Epoch [3/3], Step [61300/138038], Loss: 2.1634, Perplexity: 8.700550\n",
      "Epoch [3/3], Step [61400/138038], Loss: 2.0389, Perplexity: 7.681992\n",
      "Epoch [3/3], Step [61500/138038], Loss: 3.0655, Perplexity: 21.4452\n",
      "Epoch [3/3], Step [61600/138038], Loss: 1.6787, Perplexity: 5.358528\n",
      "Epoch [3/3], Step [61700/138038], Loss: 1.2579, Perplexity: 3.51796\n",
      "Epoch [3/3], Step [61800/138038], Loss: 2.3841, Perplexity: 10.8494\n",
      "Epoch [3/3], Step [61900/138038], Loss: 4.0603, Perplexity: 57.9941\n",
      "Epoch [3/3], Step [62000/138038], Loss: 2.6585, Perplexity: 14.27488\n",
      "Epoch [3/3], Step [62100/138038], Loss: 3.6499, Perplexity: 38.4726\n",
      "Epoch [3/3], Step [62200/138038], Loss: 3.5308, Perplexity: 34.1519\n",
      "Epoch [3/3], Step [62300/138038], Loss: 2.0758, Perplexity: 7.970904\n",
      "Epoch [3/3], Step [62400/138038], Loss: 3.5127, Perplexity: 33.5400\n",
      "Epoch [3/3], Step [62500/138038], Loss: 2.5509, Perplexity: 12.8185\n",
      "Epoch [3/3], Step [62600/138038], Loss: 2.8616, Perplexity: 17.4902\n",
      "Epoch [3/3], Step [62700/138038], Loss: 2.1429, Perplexity: 8.52440\n",
      "Epoch [3/3], Step [62800/138038], Loss: 1.8246, Perplexity: 6.20016\n",
      "Epoch [3/3], Step [62900/138038], Loss: 2.7204, Perplexity: 15.1867\n",
      "Epoch [3/3], Step [63000/138038], Loss: 1.9451, Perplexity: 6.99404\n",
      "Epoch [3/3], Step [63100/138038], Loss: 2.7703, Perplexity: 15.9634\n",
      "Epoch [3/3], Step [63200/138038], Loss: 2.4446, Perplexity: 11.5260\n",
      "Epoch [3/3], Step [63300/138038], Loss: 3.1442, Perplexity: 23.2016\n",
      "Epoch [3/3], Step [63400/138038], Loss: 2.4518, Perplexity: 11.6097\n",
      "Epoch [3/3], Step [63500/138038], Loss: 3.2218, Perplexity: 25.0726\n",
      "Epoch [3/3], Step [63600/138038], Loss: 2.3230, Perplexity: 10.2063\n",
      "Epoch [3/3], Step [63700/138038], Loss: 2.5163, Perplexity: 12.38271\n",
      "Epoch [3/3], Step [63800/138038], Loss: 2.2080, Perplexity: 9.09738\n",
      "Epoch [3/3], Step [63900/138038], Loss: 3.5126, Perplexity: 33.5350\n",
      "Epoch [3/3], Step [64000/138038], Loss: 2.6272, Perplexity: 13.8344\n",
      "Epoch [3/3], Step [64100/138038], Loss: 2.6727, Perplexity: 14.4784\n",
      "Epoch [3/3], Step [64200/138038], Loss: 1.7788, Perplexity: 5.92250\n",
      "Epoch [3/3], Step [64300/138038], Loss: 3.2935, Perplexity: 26.9362\n",
      "Epoch [3/3], Step [64400/138038], Loss: 2.5599, Perplexity: 12.93460\n",
      "Epoch [3/3], Step [64500/138038], Loss: 2.1230, Perplexity: 8.35651\n",
      "Epoch [3/3], Step [64600/138038], Loss: 3.6769, Perplexity: 39.5256\n",
      "Epoch [3/3], Step [64700/138038], Loss: 3.1934, Perplexity: 24.37189\n",
      "Epoch [3/3], Step [64800/138038], Loss: 1.7354, Perplexity: 5.67105\n",
      "Epoch [3/3], Step [64900/138038], Loss: 3.6662, Perplexity: 39.1038\n",
      "Epoch [3/3], Step [65000/138038], Loss: 2.0455, Perplexity: 7.73334\n",
      "Epoch [3/3], Step [65100/138038], Loss: 1.7870, Perplexity: 5.97171\n",
      "Epoch [3/3], Step [65200/138038], Loss: 2.7911, Perplexity: 16.2988\n",
      "Epoch [3/3], Step [65300/138038], Loss: 1.8633, Perplexity: 6.44516\n",
      "Epoch [3/3], Step [65400/138038], Loss: 3.1856, Perplexity: 24.1814\n",
      "Epoch [3/3], Step [65500/138038], Loss: 2.4715, Perplexity: 11.8403\n",
      "Epoch [3/3], Step [65600/138038], Loss: 1.8076, Perplexity: 6.095579\n",
      "Epoch [3/3], Step [65700/138038], Loss: 3.2014, Perplexity: 24.5664\n",
      "Epoch [3/3], Step [65800/138038], Loss: 3.3096, Perplexity: 27.3743\n",
      "Epoch [3/3], Step [65900/138038], Loss: 2.7304, Perplexity: 15.33861\n",
      "Epoch [3/3], Step [66000/138038], Loss: 2.6671, Perplexity: 14.3983\n",
      "Epoch [3/3], Step [66100/138038], Loss: 2.9292, Perplexity: 18.7129\n",
      "Epoch [3/3], Step [66200/138038], Loss: 2.1256, Perplexity: 8.37773\n",
      "Epoch [3/3], Step [66300/138038], Loss: 2.6730, Perplexity: 14.48309\n",
      "Epoch [3/3], Step [66400/138038], Loss: 3.7371, Perplexity: 41.9754\n",
      "Epoch [3/3], Step [66500/138038], Loss: 2.7883, Perplexity: 16.2531\n",
      "Epoch [3/3], Step [66600/138038], Loss: 2.8306, Perplexity: 16.95643\n",
      "Epoch [3/3], Step [66700/138038], Loss: 2.7156, Perplexity: 15.11406\n",
      "Epoch [3/3], Step [66800/138038], Loss: 2.1687, Perplexity: 8.74732\n",
      "Epoch [3/3], Step [66900/138038], Loss: 2.0253, Perplexity: 7.57854\n",
      "Epoch [3/3], Step [67000/138038], Loss: 3.1621, Perplexity: 23.62129\n",
      "Epoch [3/3], Step [67100/138038], Loss: 2.7486, Perplexity: 15.6207\n",
      "Epoch [3/3], Step [67200/138038], Loss: 1.8267, Perplexity: 6.21346\n",
      "Epoch [3/3], Step [67300/138038], Loss: 2.9412, Perplexity: 18.9378\n",
      "Epoch [3/3], Step [67400/138038], Loss: 2.2372, Perplexity: 9.36699\n",
      "Epoch [3/3], Step [67500/138038], Loss: 2.0987, Perplexity: 8.155905\n",
      "Epoch [3/3], Step [67600/138038], Loss: 2.4272, Perplexity: 11.32737\n",
      "Epoch [3/3], Step [67700/138038], Loss: 2.3236, Perplexity: 10.2120\n",
      "Epoch [3/3], Step [67800/138038], Loss: 2.5681, Perplexity: 13.0416\n",
      "Epoch [3/3], Step [67900/138038], Loss: 2.1142, Perplexity: 8.28324\n",
      "Epoch [3/3], Step [68000/138038], Loss: 2.3461, Perplexity: 10.4453\n",
      "Epoch [3/3], Step [68100/138038], Loss: 3.1580, Perplexity: 23.52428\n",
      "Epoch [3/3], Step [68200/138038], Loss: 2.1052, Perplexity: 8.20867\n",
      "Epoch [3/3], Step [68300/138038], Loss: 1.6357, Perplexity: 5.13299\n",
      "Epoch [3/3], Step [68400/138038], Loss: 2.3712, Perplexity: 10.71021\n",
      "Epoch [3/3], Step [68500/138038], Loss: 1.7779, Perplexity: 5.91776\n",
      "Epoch [3/3], Step [68600/138038], Loss: 3.0793, Perplexity: 21.7431\n",
      "Epoch [3/3], Step [68700/138038], Loss: 2.4878, Perplexity: 12.0348\n",
      "Epoch [3/3], Step [68800/138038], Loss: 2.4717, Perplexity: 11.84271\n",
      "Epoch [3/3], Step [68900/138038], Loss: 1.1839, Perplexity: 3.26697\n",
      "Epoch [3/3], Step [69000/138038], Loss: 2.8826, Perplexity: 17.8604\n",
      "Epoch [3/3], Step [69100/138038], Loss: 2.5384, Perplexity: 12.65988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [69200/138038], Loss: 2.2241, Perplexity: 9.24526\n",
      "Epoch [3/3], Step [69300/138038], Loss: 3.0635, Perplexity: 21.4021\n",
      "Epoch [3/3], Step [69400/138038], Loss: 2.9057, Perplexity: 18.2789\n",
      "Epoch [3/3], Step [69500/138038], Loss: 2.2903, Perplexity: 9.87813\n",
      "Epoch [3/3], Step [69600/138038], Loss: 2.6831, Perplexity: 14.63072\n",
      "Epoch [3/3], Step [69700/138038], Loss: 2.0903, Perplexity: 8.087134\n",
      "Epoch [3/3], Step [69800/138038], Loss: 2.1101, Perplexity: 8.24919\n",
      "Epoch [3/3], Step [69900/138038], Loss: 2.7217, Perplexity: 15.2067\n",
      "Epoch [3/3], Step [70000/138038], Loss: 1.2688, Perplexity: 3.556570\n",
      "Epoch [3/3], Step [70100/138038], Loss: 2.7572, Perplexity: 15.75611\n",
      "Epoch [3/3], Step [70200/138038], Loss: 1.9148, Perplexity: 6.78556\n",
      "Epoch [3/3], Step [70300/138038], Loss: 2.6391, Perplexity: 14.00029\n",
      "Epoch [3/3], Step [70400/138038], Loss: 3.8407, Perplexity: 46.5598\n",
      "Epoch [3/3], Step [70500/138038], Loss: 3.2088, Perplexity: 24.74927\n",
      "Epoch [3/3], Step [70600/138038], Loss: 1.6396, Perplexity: 5.15331\n",
      "Epoch [3/3], Step [70700/138038], Loss: 1.9775, Perplexity: 7.224333\n",
      "Epoch [3/3], Step [70800/138038], Loss: 2.0394, Perplexity: 7.686321\n",
      "Epoch [3/3], Step [70900/138038], Loss: 2.1842, Perplexity: 8.88378\n",
      "Epoch [3/3], Step [71000/138038], Loss: 2.8470, Perplexity: 17.2366\n",
      "Epoch [3/3], Step [71100/138038], Loss: 2.6685, Perplexity: 14.4178\n",
      "Epoch [3/3], Step [71200/138038], Loss: 1.8417, Perplexity: 6.307112\n",
      "Epoch [3/3], Step [71300/138038], Loss: 2.2653, Perplexity: 9.63419\n",
      "Epoch [3/3], Step [71400/138038], Loss: 2.3913, Perplexity: 10.9274\n",
      "Epoch [3/3], Step [71500/138038], Loss: 3.5912, Perplexity: 36.2768\n",
      "Epoch [3/3], Step [71600/138038], Loss: 3.2562, Perplexity: 25.9500\n",
      "Epoch [3/3], Step [71700/138038], Loss: 2.5164, Perplexity: 12.38391\n",
      "Epoch [3/3], Step [71800/138038], Loss: 2.2248, Perplexity: 9.25148\n",
      "Epoch [3/3], Step [71900/138038], Loss: 2.7799, Perplexity: 16.1182\n",
      "Epoch [3/3], Step [72000/138038], Loss: 2.2535, Perplexity: 9.52143\n",
      "Epoch [3/3], Step [72100/138038], Loss: 4.2341, Perplexity: 69.00206\n",
      "Epoch [3/3], Step [72200/138038], Loss: 2.0283, Perplexity: 7.601519\n",
      "Epoch [3/3], Step [72300/138038], Loss: 2.4363, Perplexity: 11.43083\n",
      "Epoch [3/3], Step [72400/138038], Loss: 2.3543, Perplexity: 10.5307\n",
      "Epoch [3/3], Step [72500/138038], Loss: 2.3055, Perplexity: 10.0291\n",
      "Epoch [3/3], Step [72600/138038], Loss: 2.8714, Perplexity: 17.6620\n",
      "Epoch [3/3], Step [72700/138038], Loss: 3.5185, Perplexity: 33.73516\n",
      "Epoch [3/3], Step [72800/138038], Loss: 2.4035, Perplexity: 11.0624\n",
      "Epoch [3/3], Step [72900/138038], Loss: 2.1749, Perplexity: 8.80141\n",
      "Epoch [3/3], Step [73000/138038], Loss: 2.4713, Perplexity: 11.8383\n",
      "Epoch [3/3], Step [73100/138038], Loss: 3.0032, Perplexity: 20.1504\n",
      "Epoch [3/3], Step [73200/138038], Loss: 2.9999, Perplexity: 20.08317\n",
      "Epoch [3/3], Step [73300/138038], Loss: 2.0795, Perplexity: 8.000405\n",
      "Epoch [3/3], Step [73400/138038], Loss: 2.6244, Perplexity: 13.7963\n",
      "Epoch [3/3], Step [73500/138038], Loss: 2.1410, Perplexity: 8.50809\n",
      "Epoch [3/3], Step [73600/138038], Loss: 3.6687, Perplexity: 39.1990\n",
      "Epoch [3/3], Step [73700/138038], Loss: 2.9149, Perplexity: 18.4467\n",
      "Epoch [3/3], Step [73800/138038], Loss: 2.8914, Perplexity: 18.01856\n",
      "Epoch [3/3], Step [73900/138038], Loss: 2.7419, Perplexity: 15.5166\n",
      "Epoch [3/3], Step [74000/138038], Loss: 2.5551, Perplexity: 12.8730\n",
      "Epoch [3/3], Step [74100/138038], Loss: 2.6181, Perplexity: 13.71025\n",
      "Epoch [3/3], Step [74200/138038], Loss: 4.1461, Perplexity: 63.1870\n",
      "Epoch [3/3], Step [74300/138038], Loss: 5.0713, Perplexity: 159.3783\n",
      "Epoch [3/3], Step [74400/138038], Loss: 1.9797, Perplexity: 7.240510\n",
      "Epoch [3/3], Step [74500/138038], Loss: 2.6682, Perplexity: 14.4136\n",
      "Epoch [3/3], Step [74600/138038], Loss: 2.7498, Perplexity: 15.6389\n",
      "Epoch [3/3], Step [74700/138038], Loss: 2.3968, Perplexity: 10.98751\n",
      "Epoch [3/3], Step [74800/138038], Loss: 2.2708, Perplexity: 9.687684\n",
      "Epoch [3/3], Step [74900/138038], Loss: 2.6386, Perplexity: 13.9935\n",
      "Epoch [3/3], Step [75000/138038], Loss: 2.8754, Perplexity: 17.7324\n",
      "Epoch [3/3], Step [75100/138038], Loss: 2.8461, Perplexity: 17.2197\n",
      "Epoch [3/3], Step [75200/138038], Loss: 3.0250, Perplexity: 20.5934\n",
      "Epoch [3/3], Step [75300/138038], Loss: 2.5153, Perplexity: 12.37078\n",
      "Epoch [3/3], Step [75400/138038], Loss: 1.8839, Perplexity: 6.57912\n",
      "Epoch [3/3], Step [75500/138038], Loss: 2.8711, Perplexity: 17.6568\n",
      "Epoch [3/3], Step [75600/138038], Loss: 2.0956, Perplexity: 8.13031\n",
      "Epoch [3/3], Step [75700/138038], Loss: 3.8397, Perplexity: 46.5109\n",
      "Epoch [3/3], Step [75800/138038], Loss: 2.0887, Perplexity: 8.07408\n",
      "Epoch [3/3], Step [75900/138038], Loss: 3.5641, Perplexity: 35.3063\n",
      "Epoch [3/3], Step [76000/138038], Loss: 3.6451, Perplexity: 38.28790\n",
      "Epoch [3/3], Step [76100/138038], Loss: 2.5665, Perplexity: 13.0201\n",
      "Epoch [3/3], Step [76200/138038], Loss: 2.0744, Perplexity: 7.95940\n",
      "Epoch [3/3], Step [76300/138038], Loss: 2.1080, Perplexity: 8.23171\n",
      "Epoch [3/3], Step [76400/138038], Loss: 2.8982, Perplexity: 18.1418\n",
      "Epoch [3/3], Step [76500/138038], Loss: 2.7187, Perplexity: 15.1606\n",
      "Epoch [3/3], Step [76600/138038], Loss: 2.4817, Perplexity: 11.96174\n",
      "Epoch [3/3], Step [76700/138038], Loss: 2.0715, Perplexity: 7.93637\n",
      "Epoch [3/3], Step [76800/138038], Loss: 2.5449, Perplexity: 12.74234\n",
      "Epoch [3/3], Step [76900/138038], Loss: 1.9730, Perplexity: 7.19209\n",
      "Epoch [3/3], Step [77000/138038], Loss: 2.9040, Perplexity: 18.2471\n",
      "Epoch [3/3], Step [77100/138038], Loss: 3.2249, Perplexity: 25.1509\n",
      "Epoch [3/3], Step [77200/138038], Loss: 2.4535, Perplexity: 11.62944\n",
      "Epoch [3/3], Step [77300/138038], Loss: 4.0155, Perplexity: 55.4529\n",
      "Epoch [3/3], Step [77400/138038], Loss: 3.0168, Perplexity: 20.4251\n",
      "Epoch [3/3], Step [77500/138038], Loss: 1.9830, Perplexity: 7.26487\n",
      "Epoch [3/3], Step [77600/138038], Loss: 3.5707, Perplexity: 35.54244\n",
      "Epoch [3/3], Step [77700/138038], Loss: 2.1021, Perplexity: 8.18328\n",
      "Epoch [3/3], Step [77800/138038], Loss: 2.6316, Perplexity: 13.89662\n",
      "Epoch [3/3], Step [77900/138038], Loss: 2.0353, Perplexity: 7.65437\n",
      "Epoch [3/3], Step [78000/138038], Loss: 2.0621, Perplexity: 7.86249\n",
      "Epoch [3/3], Step [78100/138038], Loss: 1.6277, Perplexity: 5.09223\n",
      "Epoch [3/3], Step [78200/138038], Loss: 1.3594, Perplexity: 3.89406\n",
      "Epoch [3/3], Step [78300/138038], Loss: 1.6603, Perplexity: 5.26115\n",
      "Epoch [3/3], Step [78400/138038], Loss: 2.2901, Perplexity: 9.87601\n",
      "Epoch [3/3], Step [78500/138038], Loss: 1.3860, Perplexity: 3.998978\n",
      "Epoch [3/3], Step [78600/138038], Loss: 3.3527, Perplexity: 28.5793\n",
      "Epoch [3/3], Step [78700/138038], Loss: 2.0387, Perplexity: 7.68049\n",
      "Epoch [3/3], Step [78800/138038], Loss: 2.7033, Perplexity: 14.92903\n",
      "Epoch [3/3], Step [78900/138038], Loss: 2.6454, Perplexity: 14.0893\n",
      "Epoch [3/3], Step [79000/138038], Loss: 2.8543, Perplexity: 17.3622\n",
      "Epoch [3/3], Step [79100/138038], Loss: 2.8749, Perplexity: 17.7234\n",
      "Epoch [3/3], Step [79200/138038], Loss: 3.2019, Perplexity: 24.58014\n",
      "Epoch [3/3], Step [79300/138038], Loss: 2.0802, Perplexity: 8.00634\n",
      "Epoch [3/3], Step [79400/138038], Loss: 3.1066, Perplexity: 22.3439\n",
      "Epoch [3/3], Step [79500/138038], Loss: 2.4461, Perplexity: 11.5435\n",
      "Epoch [3/3], Step [79600/138038], Loss: 3.8554, Perplexity: 47.2469\n",
      "Epoch [3/3], Step [79700/138038], Loss: 3.3078, Perplexity: 27.3250\n",
      "Epoch [3/3], Step [79800/138038], Loss: 2.2514, Perplexity: 9.50078\n",
      "Epoch [3/3], Step [79900/138038], Loss: 2.9506, Perplexity: 19.1175\n",
      "Epoch [3/3], Step [80000/138038], Loss: 2.6542, Perplexity: 14.2131\n",
      "Epoch [3/3], Step [80100/138038], Loss: 3.6876, Perplexity: 39.9499\n",
      "Epoch [3/3], Step [80200/138038], Loss: 2.3235, Perplexity: 10.21174\n",
      "Epoch [3/3], Step [80300/138038], Loss: 3.1676, Perplexity: 23.7503\n",
      "Epoch [3/3], Step [80400/138038], Loss: 3.0877, Perplexity: 21.92674\n",
      "Epoch [3/3], Step [80500/138038], Loss: 2.2110, Perplexity: 9.125012\n",
      "Epoch [3/3], Step [80600/138038], Loss: 1.9797, Perplexity: 7.240280\n",
      "Epoch [3/3], Step [80700/138038], Loss: 2.0605, Perplexity: 7.850161\n",
      "Epoch [3/3], Step [80800/138038], Loss: 2.6004, Perplexity: 13.46859\n",
      "Epoch [3/3], Step [80900/138038], Loss: 3.6121, Perplexity: 37.0437\n",
      "Epoch [3/3], Step [81000/138038], Loss: 2.3042, Perplexity: 10.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [81100/138038], Loss: 3.0732, Perplexity: 21.6102\n",
      "Epoch [3/3], Step [81200/138038], Loss: 2.5904, Perplexity: 13.3355\n",
      "Epoch [3/3], Step [81300/138038], Loss: 3.1670, Perplexity: 23.73654\n",
      "Epoch [3/3], Step [81400/138038], Loss: 2.5190, Perplexity: 12.4164\n",
      "Epoch [3/3], Step [81500/138038], Loss: 3.3525, Perplexity: 28.5744\n",
      "Epoch [3/3], Step [81600/138038], Loss: 2.7432, Perplexity: 15.5363\n",
      "Epoch [3/3], Step [81700/138038], Loss: 2.0731, Perplexity: 7.949456\n",
      "Epoch [3/3], Step [81800/138038], Loss: 2.4630, Perplexity: 11.74015\n",
      "Epoch [3/3], Step [81900/138038], Loss: 2.4359, Perplexity: 11.4264\n",
      "Epoch [3/3], Step [82000/138038], Loss: 1.9464, Perplexity: 7.00370\n",
      "Epoch [3/3], Step [82100/138038], Loss: 2.2742, Perplexity: 9.720668\n",
      "Epoch [3/3], Step [82200/138038], Loss: 2.2739, Perplexity: 9.717397\n",
      "Epoch [3/3], Step [82300/138038], Loss: 3.2490, Perplexity: 25.7657\n",
      "Epoch [3/3], Step [82400/138038], Loss: 2.0384, Perplexity: 7.67804\n",
      "Epoch [3/3], Step [82500/138038], Loss: 2.5065, Perplexity: 12.2624\n",
      "Epoch [3/3], Step [82600/138038], Loss: 3.2480, Perplexity: 25.7389\n",
      "Epoch [3/3], Step [82700/138038], Loss: 2.7208, Perplexity: 15.1923\n",
      "Epoch [3/3], Step [82800/138038], Loss: 1.8067, Perplexity: 6.09043\n",
      "Epoch [3/3], Step [82900/138038], Loss: 2.6046, Perplexity: 13.5264\n",
      "Epoch [3/3], Step [83000/138038], Loss: 2.3101, Perplexity: 10.07529\n",
      "Epoch [3/3], Step [83100/138038], Loss: 2.2660, Perplexity: 9.64125\n",
      "Epoch [3/3], Step [83200/138038], Loss: 2.6226, Perplexity: 13.77131\n",
      "Epoch [3/3], Step [83300/138038], Loss: 2.9776, Perplexity: 19.64057\n",
      "Epoch [3/3], Step [83400/138038], Loss: 1.9574, Perplexity: 7.080603\n",
      "Epoch [3/3], Step [83500/138038], Loss: 2.8442, Perplexity: 17.1872\n",
      "Epoch [3/3], Step [83600/138038], Loss: 3.5220, Perplexity: 33.85138\n",
      "Epoch [3/3], Step [83700/138038], Loss: 3.6972, Perplexity: 40.3333\n",
      "Epoch [3/3], Step [83800/138038], Loss: 2.0807, Perplexity: 8.010295\n",
      "Epoch [3/3], Step [83900/138038], Loss: 2.1355, Perplexity: 8.46163\n",
      "Epoch [3/3], Step [84000/138038], Loss: 2.8908, Perplexity: 18.0072\n",
      "Epoch [3/3], Step [84100/138038], Loss: 2.0998, Perplexity: 8.16463\n",
      "Epoch [3/3], Step [84200/138038], Loss: 2.6464, Perplexity: 14.1025\n",
      "Epoch [3/3], Step [84300/138038], Loss: 3.6232, Perplexity: 37.4583\n",
      "Epoch [3/3], Step [84400/138038], Loss: 2.9599, Perplexity: 19.2961\n",
      "Epoch [3/3], Step [84500/138038], Loss: 1.9717, Perplexity: 7.18304\n",
      "Epoch [3/3], Step [84600/138038], Loss: 4.1707, Perplexity: 64.7627\n",
      "Epoch [3/3], Step [84700/138038], Loss: 3.0354, Perplexity: 20.8096\n",
      "Epoch [3/3], Step [84800/138038], Loss: 3.5219, Perplexity: 33.84901\n",
      "Epoch [3/3], Step [84900/138038], Loss: 1.6627, Perplexity: 5.273825\n",
      "Epoch [3/3], Step [85000/138038], Loss: 2.6124, Perplexity: 13.6319\n",
      "Epoch [3/3], Step [85100/138038], Loss: 2.2960, Perplexity: 9.93435\n",
      "Epoch [3/3], Step [85200/138038], Loss: 3.7614, Perplexity: 43.0094\n",
      "Epoch [3/3], Step [85300/138038], Loss: 2.8914, Perplexity: 18.01930\n",
      "Epoch [3/3], Step [85400/138038], Loss: 2.5968, Perplexity: 13.4207\n",
      "Epoch [3/3], Step [85500/138038], Loss: 2.3110, Perplexity: 10.0849\n",
      "Epoch [3/3], Step [85600/138038], Loss: 1.7547, Perplexity: 5.78175\n",
      "Epoch [3/3], Step [85700/138038], Loss: 3.3910, Perplexity: 29.69568\n",
      "Epoch [3/3], Step [85800/138038], Loss: 2.8329, Perplexity: 16.9951\n",
      "Epoch [3/3], Step [85900/138038], Loss: 3.2907, Perplexity: 26.8616\n",
      "Epoch [3/3], Step [86000/138038], Loss: 2.4481, Perplexity: 11.5661\n",
      "Epoch [3/3], Step [86100/138038], Loss: 2.1385, Perplexity: 8.48630\n",
      "Epoch [3/3], Step [86200/138038], Loss: 2.1513, Perplexity: 8.59561\n",
      "Epoch [3/3], Step [86300/138038], Loss: 2.9304, Perplexity: 18.7343\n",
      "Epoch [3/3], Step [86400/138038], Loss: 3.5518, Perplexity: 34.8752\n",
      "Epoch [3/3], Step [86500/138038], Loss: 2.5893, Perplexity: 13.3202\n",
      "Epoch [3/3], Step [86600/138038], Loss: 3.6648, Perplexity: 39.04808\n",
      "Epoch [3/3], Step [86700/138038], Loss: 3.2304, Perplexity: 25.2908\n",
      "Epoch [3/3], Step [86800/138038], Loss: 2.1791, Perplexity: 8.83828\n",
      "Epoch [3/3], Step [86900/138038], Loss: 3.7551, Perplexity: 42.73836\n",
      "Epoch [3/3], Step [87000/138038], Loss: 3.1446, Perplexity: 23.2098\n",
      "Epoch [3/3], Step [87100/138038], Loss: 2.8539, Perplexity: 17.3548\n",
      "Epoch [3/3], Step [87200/138038], Loss: 1.8247, Perplexity: 6.200967\n",
      "Epoch [3/3], Step [87300/138038], Loss: 2.8398, Perplexity: 17.1119\n",
      "Epoch [3/3], Step [87400/138038], Loss: 2.8350, Perplexity: 17.0305\n",
      "Epoch [3/3], Step [87500/138038], Loss: 2.1890, Perplexity: 8.926088\n",
      "Epoch [3/3], Step [87600/138038], Loss: 3.0720, Perplexity: 21.5852\n",
      "Epoch [3/3], Step [87700/138038], Loss: 1.7361, Perplexity: 5.67540\n",
      "Epoch [3/3], Step [87800/138038], Loss: 3.3298, Perplexity: 27.9326\n",
      "Epoch [3/3], Step [87900/138038], Loss: 3.3867, Perplexity: 29.5682\n",
      "Epoch [3/3], Step [88000/138038], Loss: 2.4351, Perplexity: 11.4168\n",
      "Epoch [3/3], Step [88100/138038], Loss: 2.0951, Perplexity: 8.12634\n",
      "Epoch [3/3], Step [88200/138038], Loss: 4.4281, Perplexity: 83.7721\n",
      "Epoch [3/3], Step [88300/138038], Loss: 2.9524, Perplexity: 19.15178\n",
      "Epoch [3/3], Step [88400/138038], Loss: 2.2276, Perplexity: 9.27726\n",
      "Epoch [3/3], Step [88500/138038], Loss: 3.2590, Perplexity: 26.02435\n",
      "Epoch [3/3], Step [88600/138038], Loss: 2.4478, Perplexity: 11.5623\n",
      "Epoch [3/3], Step [88700/138038], Loss: 1.9841, Perplexity: 7.27249\n",
      "Epoch [3/3], Step [88800/138038], Loss: 5.1953, Perplexity: 180.4173\n",
      "Epoch [3/3], Step [88900/138038], Loss: 1.9088, Perplexity: 6.744931\n",
      "Epoch [3/3], Step [89000/138038], Loss: 2.8755, Perplexity: 17.7352\n",
      "Epoch [3/3], Step [89100/138038], Loss: 3.3539, Perplexity: 28.6144\n",
      "Epoch [3/3], Step [89200/138038], Loss: 2.4380, Perplexity: 11.44982\n",
      "Epoch [3/3], Step [89300/138038], Loss: 2.5782, Perplexity: 13.1730\n",
      "Epoch [3/3], Step [89400/138038], Loss: 2.5958, Perplexity: 13.4073\n",
      "Epoch [3/3], Step [89500/138038], Loss: 2.9038, Perplexity: 18.2430\n",
      "Epoch [3/3], Step [89600/138038], Loss: 2.3272, Perplexity: 10.2487\n",
      "Epoch [3/3], Step [89700/138038], Loss: 2.7266, Perplexity: 15.2812\n",
      "Epoch [3/3], Step [89800/138038], Loss: 3.5378, Perplexity: 34.3922\n",
      "Epoch [3/3], Step [89900/138038], Loss: 2.6186, Perplexity: 13.7161\n",
      "Epoch [3/3], Step [90000/138038], Loss: 2.3758, Perplexity: 10.7595\n",
      "Epoch [3/3], Step [90100/138038], Loss: 2.7258, Perplexity: 15.2684\n",
      "Epoch [3/3], Step [90200/138038], Loss: 1.9136, Perplexity: 6.77747\n",
      "Epoch [3/3], Step [90300/138038], Loss: 2.6686, Perplexity: 14.4194\n",
      "Epoch [3/3], Step [90400/138038], Loss: 2.8738, Perplexity: 17.7046\n",
      "Epoch [3/3], Step [90500/138038], Loss: 4.8273, Perplexity: 124.8736\n",
      "Epoch [3/3], Step [90600/138038], Loss: 3.7780, Perplexity: 43.72945\n",
      "Epoch [3/3], Step [90700/138038], Loss: 2.7884, Perplexity: 16.25479\n",
      "Epoch [3/3], Step [90800/138038], Loss: 3.5094, Perplexity: 33.42752\n",
      "Epoch [3/3], Step [90900/138038], Loss: 1.6789, Perplexity: 5.35970\n",
      "Epoch [3/3], Step [91000/138038], Loss: 3.1053, Perplexity: 22.31635\n",
      "Epoch [3/3], Step [91100/138038], Loss: 2.5605, Perplexity: 12.94272\n",
      "Epoch [3/3], Step [91200/138038], Loss: 2.3826, Perplexity: 10.83295\n",
      "Epoch [3/3], Step [91300/138038], Loss: 2.6716, Perplexity: 14.46338\n",
      "Epoch [3/3], Step [91400/138038], Loss: 3.0365, Perplexity: 20.8316\n",
      "Epoch [3/3], Step [91500/138038], Loss: 2.0467, Perplexity: 7.742398\n",
      "Epoch [3/3], Step [91600/138038], Loss: 2.1196, Perplexity: 8.32758\n",
      "Epoch [3/3], Step [91700/138038], Loss: 2.9718, Perplexity: 19.5270\n",
      "Epoch [3/3], Step [91800/138038], Loss: 2.1565, Perplexity: 8.640670\n",
      "Epoch [3/3], Step [91900/138038], Loss: 3.3492, Perplexity: 28.47893\n",
      "Epoch [3/3], Step [92000/138038], Loss: 2.7654, Perplexity: 15.8851\n",
      "Epoch [3/3], Step [92100/138038], Loss: 3.1488, Perplexity: 23.3083\n",
      "Epoch [3/3], Step [92200/138038], Loss: 2.5510, Perplexity: 12.82032\n",
      "Epoch [3/3], Step [92300/138038], Loss: 2.7317, Perplexity: 15.3582\n",
      "Epoch [3/3], Step [92400/138038], Loss: 1.5934, Perplexity: 4.920318\n",
      "Epoch [3/3], Step [92500/138038], Loss: 2.3880, Perplexity: 10.8919\n",
      "Epoch [3/3], Step [92600/138038], Loss: 3.0188, Perplexity: 20.4668\n",
      "Epoch [3/3], Step [92700/138038], Loss: 2.7581, Perplexity: 15.7700\n",
      "Epoch [3/3], Step [92800/138038], Loss: 2.8824, Perplexity: 17.8576\n",
      "Epoch [3/3], Step [92900/138038], Loss: 3.6934, Perplexity: 40.18223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [93000/138038], Loss: 2.4403, Perplexity: 11.47619\n",
      "Epoch [3/3], Step [93100/138038], Loss: 2.5799, Perplexity: 13.1962\n",
      "Epoch [3/3], Step [93200/138038], Loss: 2.4352, Perplexity: 11.4184\n",
      "Epoch [3/3], Step [93300/138038], Loss: 2.1192, Perplexity: 8.32467\n",
      "Epoch [3/3], Step [93400/138038], Loss: 2.4789, Perplexity: 11.9277\n",
      "Epoch [3/3], Step [93500/138038], Loss: 3.2511, Perplexity: 25.8175\n",
      "Epoch [3/3], Step [93600/138038], Loss: 2.2174, Perplexity: 9.18374\n",
      "Epoch [3/3], Step [93700/138038], Loss: 2.4181, Perplexity: 11.22443\n",
      "Epoch [3/3], Step [93800/138038], Loss: 4.1432, Perplexity: 63.00141\n",
      "Epoch [3/3], Step [93900/138038], Loss: 1.7923, Perplexity: 6.003254\n",
      "Epoch [3/3], Step [94000/138038], Loss: 2.0700, Perplexity: 7.924554\n",
      "Epoch [3/3], Step [94100/138038], Loss: 2.7989, Perplexity: 16.4267\n",
      "Epoch [3/3], Step [94200/138038], Loss: 2.7216, Perplexity: 15.2039\n",
      "Epoch [3/3], Step [94300/138038], Loss: 1.8444, Perplexity: 6.324674\n",
      "Epoch [3/3], Step [94400/138038], Loss: 2.6938, Perplexity: 14.7884\n",
      "Epoch [3/3], Step [94500/138038], Loss: 2.6819, Perplexity: 14.61280\n",
      "Epoch [3/3], Step [94600/138038], Loss: 4.0571, Perplexity: 57.8040\n",
      "Epoch [3/3], Step [94700/138038], Loss: 3.2829, Perplexity: 26.6541\n",
      "Epoch [3/3], Step [94800/138038], Loss: 2.5670, Perplexity: 13.0272\n",
      "Epoch [3/3], Step [94900/138038], Loss: 2.8688, Perplexity: 17.6165\n",
      "Epoch [3/3], Step [95000/138038], Loss: 3.3573, Perplexity: 28.71241\n",
      "Epoch [3/3], Step [95100/138038], Loss: 3.9814, Perplexity: 53.59471\n",
      "Epoch [3/3], Step [95200/138038], Loss: 3.3374, Perplexity: 28.1467\n",
      "Epoch [3/3], Step [95300/138038], Loss: 3.0314, Perplexity: 20.72613\n",
      "Epoch [3/3], Step [95400/138038], Loss: 2.6001, Perplexity: 13.4647\n",
      "Epoch [3/3], Step [95500/138038], Loss: 3.2151, Perplexity: 24.9063\n",
      "Epoch [3/3], Step [95600/138038], Loss: 3.5887, Perplexity: 36.1879\n",
      "Epoch [3/3], Step [95700/138038], Loss: 2.2162, Perplexity: 9.17210\n",
      "Epoch [3/3], Step [95800/138038], Loss: 2.6759, Perplexity: 14.52520\n",
      "Epoch [3/3], Step [95900/138038], Loss: 2.8079, Perplexity: 16.5744\n",
      "Epoch [3/3], Step [96000/138038], Loss: 3.0592, Perplexity: 21.31141\n",
      "Epoch [3/3], Step [96100/138038], Loss: 2.5167, Perplexity: 12.3878\n",
      "Epoch [3/3], Step [96200/138038], Loss: 2.4134, Perplexity: 11.1724\n",
      "Epoch [3/3], Step [96300/138038], Loss: 2.9915, Perplexity: 19.9147\n",
      "Epoch [3/3], Step [96400/138038], Loss: 2.3520, Perplexity: 10.5066\n",
      "Epoch [3/3], Step [96500/138038], Loss: 2.1594, Perplexity: 8.66552\n",
      "Epoch [3/3], Step [96600/138038], Loss: 2.6969, Perplexity: 14.83310\n",
      "Epoch [3/3], Step [96700/138038], Loss: 2.7581, Perplexity: 15.76999\n",
      "Epoch [3/3], Step [96800/138038], Loss: 2.7749, Perplexity: 16.0376\n",
      "Epoch [3/3], Step [96900/138038], Loss: 2.6813, Perplexity: 14.60439\n",
      "Epoch [3/3], Step [97000/138038], Loss: 1.7999, Perplexity: 6.04899\n",
      "Epoch [3/3], Step [97100/138038], Loss: 2.9815, Perplexity: 19.7172\n",
      "Epoch [3/3], Step [97200/138038], Loss: 1.7532, Perplexity: 5.772967\n",
      "Epoch [3/3], Step [97300/138038], Loss: 2.7092, Perplexity: 15.0174\n",
      "Epoch [3/3], Step [97400/138038], Loss: 2.6839, Perplexity: 14.6422\n",
      "Epoch [3/3], Step [97500/138038], Loss: 3.3476, Perplexity: 28.4355\n",
      "Epoch [3/3], Step [97600/138038], Loss: 2.4157, Perplexity: 11.19759\n",
      "Epoch [3/3], Step [97700/138038], Loss: 3.7626, Perplexity: 43.0600\n",
      "Epoch [3/3], Step [97800/138038], Loss: 2.5885, Perplexity: 13.3092\n",
      "Epoch [3/3], Step [97900/138038], Loss: 2.7745, Perplexity: 16.03123\n",
      "Epoch [3/3], Step [98000/138038], Loss: 2.3195, Perplexity: 10.1711\n",
      "Epoch [3/3], Step [98100/138038], Loss: 2.2470, Perplexity: 9.459078\n",
      "Epoch [3/3], Step [98200/138038], Loss: 2.5432, Perplexity: 12.72013\n",
      "Epoch [3/3], Step [98300/138038], Loss: 2.2776, Perplexity: 9.75340\n",
      "Epoch [3/3], Step [98400/138038], Loss: 2.0265, Perplexity: 7.587513\n",
      "Epoch [3/3], Step [98500/138038], Loss: 2.1497, Perplexity: 8.582626\n",
      "Epoch [3/3], Step [98600/138038], Loss: 2.2172, Perplexity: 9.18156\n",
      "Epoch [3/3], Step [98700/138038], Loss: 2.9705, Perplexity: 19.50150\n",
      "Epoch [3/3], Step [98800/138038], Loss: 2.7259, Perplexity: 15.2705\n",
      "Epoch [3/3], Step [98900/138038], Loss: 2.2182, Perplexity: 9.190630\n",
      "Epoch [3/3], Step [99000/138038], Loss: 2.5008, Perplexity: 12.19256\n",
      "Epoch [3/3], Step [99100/138038], Loss: 2.0121, Perplexity: 7.479107\n",
      "Epoch [3/3], Step [99200/138038], Loss: 2.3655, Perplexity: 10.64982\n",
      "Epoch [3/3], Step [99300/138038], Loss: 3.7192, Perplexity: 41.2295\n",
      "Epoch [3/3], Step [99400/138038], Loss: 3.5922, Perplexity: 36.3122\n",
      "Epoch [3/3], Step [99500/138038], Loss: 2.3167, Perplexity: 10.14218\n",
      "Epoch [3/3], Step [99600/138038], Loss: 3.0573, Perplexity: 21.2692\n",
      "Epoch [3/3], Step [99700/138038], Loss: 1.8267, Perplexity: 6.21330\n",
      "Epoch [3/3], Step [99800/138038], Loss: 2.6565, Perplexity: 14.2468\n",
      "Epoch [3/3], Step [99900/138038], Loss: 2.5701, Perplexity: 13.06765\n",
      "Epoch [3/3], Step [100000/138038], Loss: 2.7525, Perplexity: 15.6811\n",
      "Epoch [3/3], Step [100100/138038], Loss: 2.0205, Perplexity: 7.54209\n",
      "Epoch [3/3], Step [100200/138038], Loss: 2.5221, Perplexity: 12.4553\n",
      "Epoch [3/3], Step [100300/138038], Loss: 3.2638, Perplexity: 26.1479\n",
      "Epoch [3/3], Step [100400/138038], Loss: 3.3531, Perplexity: 28.59256\n",
      "Epoch [3/3], Step [100500/138038], Loss: 2.2404, Perplexity: 9.39699\n",
      "Epoch [3/3], Step [100600/138038], Loss: 2.6656, Perplexity: 14.3762\n",
      "Epoch [3/3], Step [100700/138038], Loss: 2.4401, Perplexity: 11.4740\n",
      "Epoch [3/3], Step [100800/138038], Loss: 2.7978, Perplexity: 16.4089\n",
      "Epoch [3/3], Step [100900/138038], Loss: 2.5811, Perplexity: 13.21169\n",
      "Epoch [3/3], Step [101000/138038], Loss: 1.2431, Perplexity: 3.46651\n",
      "Epoch [3/3], Step [101100/138038], Loss: 2.4741, Perplexity: 11.8714\n",
      "Epoch [3/3], Step [101200/138038], Loss: 2.8589, Perplexity: 17.4418\n",
      "Epoch [3/3], Step [101300/138038], Loss: 2.5232, Perplexity: 12.4688\n",
      "Epoch [3/3], Step [101400/138038], Loss: 3.0644, Perplexity: 21.4209\n",
      "Epoch [3/3], Step [101500/138038], Loss: 3.5069, Perplexity: 33.34555\n",
      "Epoch [3/3], Step [101600/138038], Loss: 2.8607, Perplexity: 17.47442\n",
      "Epoch [3/3], Step [101700/138038], Loss: 2.1577, Perplexity: 8.65129\n",
      "Epoch [3/3], Step [101800/138038], Loss: 2.1676, Perplexity: 8.73753\n",
      "Epoch [3/3], Step [101900/138038], Loss: 2.1082, Perplexity: 8.233489\n",
      "Epoch [3/3], Step [102000/138038], Loss: 3.5221, Perplexity: 33.8541\n",
      "Epoch [3/3], Step [102100/138038], Loss: 3.1017, Perplexity: 22.2356\n",
      "Epoch [3/3], Step [102200/138038], Loss: 1.8553, Perplexity: 6.393856\n",
      "Epoch [3/3], Step [102300/138038], Loss: 1.9096, Perplexity: 6.75029\n",
      "Epoch [3/3], Step [102400/138038], Loss: 2.0310, Perplexity: 7.62207\n",
      "Epoch [3/3], Step [102500/138038], Loss: 2.3431, Perplexity: 10.4135\n",
      "Epoch [3/3], Step [102600/138038], Loss: 1.5918, Perplexity: 4.91285\n",
      "Epoch [3/3], Step [102700/138038], Loss: 2.5086, Perplexity: 12.28725\n",
      "Epoch [3/3], Step [102800/138038], Loss: 3.0595, Perplexity: 21.3177\n",
      "Epoch [3/3], Step [102900/138038], Loss: 2.3494, Perplexity: 10.4789\n",
      "Epoch [3/3], Step [103000/138038], Loss: 3.2579, Perplexity: 25.9947\n",
      "Epoch [3/3], Step [103100/138038], Loss: 3.5850, Perplexity: 36.0526\n",
      "Epoch [3/3], Step [103200/138038], Loss: 3.6741, Perplexity: 39.4142\n",
      "Epoch [3/3], Step [103300/138038], Loss: 3.2102, Perplexity: 24.7836\n",
      "Epoch [3/3], Step [103400/138038], Loss: 2.6237, Perplexity: 13.7870\n",
      "Epoch [3/3], Step [103500/138038], Loss: 2.8387, Perplexity: 17.0931\n",
      "Epoch [3/3], Step [103600/138038], Loss: 2.8657, Perplexity: 17.56143\n",
      "Epoch [3/3], Step [103700/138038], Loss: 1.6926, Perplexity: 5.43364\n",
      "Epoch [3/3], Step [103800/138038], Loss: 2.7355, Perplexity: 15.41718\n",
      "Epoch [3/3], Step [103900/138038], Loss: 1.7195, Perplexity: 5.58165\n",
      "Epoch [3/3], Step [104000/138038], Loss: 2.3770, Perplexity: 10.7726\n",
      "Epoch [3/3], Step [104100/138038], Loss: 2.6820, Perplexity: 14.6140\n",
      "Epoch [3/3], Step [104200/138038], Loss: 2.4521, Perplexity: 11.6121\n",
      "Epoch [3/3], Step [104300/138038], Loss: 2.5539, Perplexity: 12.8572\n",
      "Epoch [3/3], Step [104400/138038], Loss: 3.3796, Perplexity: 29.3585\n",
      "Epoch [3/3], Step [104500/138038], Loss: 2.5703, Perplexity: 13.07018\n",
      "Epoch [3/3], Step [104600/138038], Loss: 2.3291, Perplexity: 10.26916\n",
      "Epoch [3/3], Step [104700/138038], Loss: 2.3248, Perplexity: 10.2245\n",
      "Epoch [3/3], Step [104800/138038], Loss: 2.6767, Perplexity: 14.53682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [104900/138038], Loss: 2.4330, Perplexity: 11.3932\n",
      "Epoch [3/3], Step [105000/138038], Loss: 2.9408, Perplexity: 18.9306\n",
      "Epoch [3/3], Step [105100/138038], Loss: 2.3916, Perplexity: 10.9309\n",
      "Epoch [3/3], Step [105200/138038], Loss: 2.2373, Perplexity: 9.367648\n",
      "Epoch [3/3], Step [105300/138038], Loss: 1.9657, Perplexity: 7.13998\n",
      "Epoch [3/3], Step [105400/138038], Loss: 1.7546, Perplexity: 5.781278\n",
      "Epoch [3/3], Step [105500/138038], Loss: 3.8104, Perplexity: 45.1678\n",
      "Epoch [3/3], Step [105600/138038], Loss: 2.5223, Perplexity: 12.4571\n",
      "Epoch [3/3], Step [105700/138038], Loss: 1.9476, Perplexity: 7.012298\n",
      "Epoch [3/3], Step [105800/138038], Loss: 2.3949, Perplexity: 10.9671\n",
      "Epoch [3/3], Step [105900/138038], Loss: 3.0712, Perplexity: 21.5668\n",
      "Epoch [3/3], Step [106000/138038], Loss: 2.2068, Perplexity: 9.08642\n",
      "Epoch [3/3], Step [106100/138038], Loss: 1.3863, Perplexity: 4.00023\n",
      "Epoch [3/3], Step [106200/138038], Loss: 2.1029, Perplexity: 8.18959\n",
      "Epoch [3/3], Step [106300/138038], Loss: 2.7056, Perplexity: 14.9639\n",
      "Epoch [3/3], Step [106400/138038], Loss: 1.3150, Perplexity: 3.724686\n",
      "Epoch [3/3], Step [106500/138038], Loss: 2.0894, Perplexity: 8.08024\n",
      "Epoch [3/3], Step [106600/138038], Loss: 3.6011, Perplexity: 36.63930\n",
      "Epoch [3/3], Step [106700/138038], Loss: 3.1839, Perplexity: 24.13964\n",
      "Epoch [3/3], Step [106800/138038], Loss: 3.4856, Perplexity: 32.6412\n",
      "Epoch [3/3], Step [106900/138038], Loss: 2.1108, Perplexity: 8.254816\n",
      "Epoch [3/3], Step [107000/138038], Loss: 2.3272, Perplexity: 10.24889\n",
      "Epoch [3/3], Step [107100/138038], Loss: 3.2607, Perplexity: 26.06720\n",
      "Epoch [3/3], Step [107200/138038], Loss: 2.2716, Perplexity: 9.69528\n",
      "Epoch [3/3], Step [107300/138038], Loss: 3.6629, Perplexity: 38.9723\n",
      "Epoch [3/3], Step [107400/138038], Loss: 2.1695, Perplexity: 8.75354\n",
      "Epoch [3/3], Step [107500/138038], Loss: 3.0443, Perplexity: 20.9948\n",
      "Epoch [3/3], Step [107600/138038], Loss: 2.8862, Perplexity: 17.9258\n",
      "Epoch [3/3], Step [107700/138038], Loss: 2.3894, Perplexity: 10.9067\n",
      "Epoch [3/3], Step [107800/138038], Loss: 2.3299, Perplexity: 10.27747\n",
      "Epoch [3/3], Step [107900/138038], Loss: 4.7568, Perplexity: 116.3693\n",
      "Epoch [3/3], Step [108000/138038], Loss: 3.0229, Perplexity: 20.5514\n",
      "Epoch [3/3], Step [108100/138038], Loss: 2.2007, Perplexity: 9.03106\n",
      "Epoch [3/3], Step [108200/138038], Loss: 3.2279, Perplexity: 25.2278\n",
      "Epoch [3/3], Step [108300/138038], Loss: 1.9586, Perplexity: 7.08917\n",
      "Epoch [3/3], Step [108400/138038], Loss: 3.0871, Perplexity: 21.9130\n",
      "Epoch [3/3], Step [108500/138038], Loss: 2.4040, Perplexity: 11.0674\n",
      "Epoch [3/3], Step [108600/138038], Loss: 2.0032, Perplexity: 7.41268\n",
      "Epoch [3/3], Step [108700/138038], Loss: 3.3335, Perplexity: 28.0374\n",
      "Epoch [3/3], Step [108800/138038], Loss: 4.1071, Perplexity: 60.7678\n",
      "Epoch [3/3], Step [108900/138038], Loss: 2.8257, Perplexity: 16.87290\n",
      "Epoch [3/3], Step [109000/138038], Loss: 3.1008, Perplexity: 22.2166\n",
      "Epoch [3/3], Step [109100/138038], Loss: 3.4325, Perplexity: 30.95449\n",
      "Epoch [3/3], Step [109200/138038], Loss: 1.8816, Perplexity: 6.564311\n",
      "Epoch [3/3], Step [109300/138038], Loss: 1.8665, Perplexity: 6.465468\n",
      "Epoch [3/3], Step [109400/138038], Loss: 2.2031, Perplexity: 9.052943\n",
      "Epoch [3/3], Step [109500/138038], Loss: 2.3997, Perplexity: 11.0195\n",
      "Epoch [3/3], Step [109600/138038], Loss: 1.4448, Perplexity: 4.24092\n",
      "Epoch [3/3], Step [109700/138038], Loss: 2.2494, Perplexity: 9.48177\n",
      "Epoch [3/3], Step [109800/138038], Loss: 2.7785, Perplexity: 16.0947\n",
      "Epoch [3/3], Step [109900/138038], Loss: 2.1170, Perplexity: 8.30653\n",
      "Epoch [3/3], Step [110000/138038], Loss: 2.6986, Perplexity: 14.85843\n",
      "Epoch [3/3], Step [110100/138038], Loss: 2.3134, Perplexity: 10.10865\n",
      "Epoch [3/3], Step [110200/138038], Loss: 3.3069, Perplexity: 27.2997\n",
      "Epoch [3/3], Step [110300/138038], Loss: 3.2173, Perplexity: 24.95955\n",
      "Epoch [3/3], Step [110400/138038], Loss: 1.3698, Perplexity: 3.93475\n",
      "Epoch [3/3], Step [110500/138038], Loss: 2.3441, Perplexity: 10.4239\n",
      "Epoch [3/3], Step [110600/138038], Loss: 1.7581, Perplexity: 5.801268\n",
      "Epoch [3/3], Step [110700/138038], Loss: 2.4546, Perplexity: 11.6422\n",
      "Epoch [3/3], Step [110800/138038], Loss: 2.4836, Perplexity: 11.98428\n",
      "Epoch [3/3], Step [110900/138038], Loss: 3.9610, Perplexity: 52.5087\n",
      "Epoch [3/3], Step [111000/138038], Loss: 3.5199, Perplexity: 33.7809\n",
      "Epoch [3/3], Step [111100/138038], Loss: 2.3832, Perplexity: 10.83993\n",
      "Epoch [3/3], Step [111200/138038], Loss: 1.8717, Perplexity: 6.49961\n",
      "Epoch [3/3], Step [111300/138038], Loss: 2.4686, Perplexity: 11.8060\n",
      "Epoch [3/3], Step [111400/138038], Loss: 1.6315, Perplexity: 5.11161\n",
      "Epoch [3/3], Step [111500/138038], Loss: 3.1560, Perplexity: 23.47655\n",
      "Epoch [3/3], Step [111600/138038], Loss: 2.4700, Perplexity: 11.82244\n",
      "Epoch [3/3], Step [111700/138038], Loss: 2.3336, Perplexity: 10.3155\n",
      "Epoch [3/3], Step [111800/138038], Loss: 2.8620, Perplexity: 17.4964\n",
      "Epoch [3/3], Step [111900/138038], Loss: 2.6090, Perplexity: 13.58482\n",
      "Epoch [3/3], Step [112000/138038], Loss: 2.8051, Perplexity: 16.5286\n",
      "Epoch [3/3], Step [112100/138038], Loss: 2.1175, Perplexity: 8.31026\n",
      "Epoch [3/3], Step [112200/138038], Loss: 1.9759, Perplexity: 7.212932\n",
      "Epoch [3/3], Step [112300/138038], Loss: 4.0784, Perplexity: 59.0537\n",
      "Epoch [3/3], Step [112400/138038], Loss: 2.8166, Perplexity: 16.7206\n",
      "Epoch [3/3], Step [112500/138038], Loss: 1.5596, Perplexity: 4.75715\n",
      "Epoch [3/3], Step [112600/138038], Loss: 2.0635, Perplexity: 7.87371\n",
      "Epoch [3/3], Step [112700/138038], Loss: 2.7224, Perplexity: 15.21713\n",
      "Epoch [3/3], Step [112800/138038], Loss: 2.2838, Perplexity: 9.81391\n",
      "Epoch [3/3], Step [112900/138038], Loss: 2.2286, Perplexity: 9.286927\n",
      "Epoch [3/3], Step [113000/138038], Loss: 3.1275, Perplexity: 22.8167\n",
      "Epoch [3/3], Step [113100/138038], Loss: 2.4632, Perplexity: 11.7424\n",
      "Epoch [3/3], Step [113200/138038], Loss: 2.6603, Perplexity: 14.30128\n",
      "Epoch [3/3], Step [113300/138038], Loss: 2.2059, Perplexity: 9.07828\n",
      "Epoch [3/3], Step [113400/138038], Loss: 2.2517, Perplexity: 9.50365\n",
      "Epoch [3/3], Step [113500/138038], Loss: 2.2891, Perplexity: 9.86596\n",
      "Epoch [3/3], Step [113600/138038], Loss: 2.3314, Perplexity: 10.2919\n",
      "Epoch [3/3], Step [113700/138038], Loss: 2.9182, Perplexity: 18.5086\n",
      "Epoch [3/3], Step [113800/138038], Loss: 2.8300, Perplexity: 16.9447\n",
      "Epoch [3/3], Step [113900/138038], Loss: 3.8395, Perplexity: 46.5030\n",
      "Epoch [3/3], Step [114000/138038], Loss: 2.9443, Perplexity: 18.9970\n",
      "Epoch [3/3], Step [114100/138038], Loss: 2.4101, Perplexity: 11.1353\n",
      "Epoch [3/3], Step [114200/138038], Loss: 2.8138, Perplexity: 16.6724\n",
      "Epoch [3/3], Step [114300/138038], Loss: 3.1909, Perplexity: 24.3115\n",
      "Epoch [3/3], Step [114400/138038], Loss: 1.6010, Perplexity: 4.95828\n",
      "Epoch [3/3], Step [114500/138038], Loss: 1.3917, Perplexity: 4.02165\n",
      "Epoch [3/3], Step [114600/138038], Loss: 2.4234, Perplexity: 11.28388\n",
      "Epoch [3/3], Step [114700/138038], Loss: 2.8003, Perplexity: 16.4492\n",
      "Epoch [3/3], Step [114800/138038], Loss: 2.3912, Perplexity: 10.9263\n",
      "Epoch [3/3], Step [114900/138038], Loss: 1.8253, Perplexity: 6.204705\n",
      "Epoch [3/3], Step [115000/138038], Loss: 1.8049, Perplexity: 6.07966\n",
      "Epoch [3/3], Step [115100/138038], Loss: 3.7450, Perplexity: 42.3108\n",
      "Epoch [3/3], Step [115200/138038], Loss: 2.5704, Perplexity: 13.0717\n",
      "Epoch [3/3], Step [115300/138038], Loss: 3.0106, Perplexity: 20.3004\n",
      "Epoch [3/3], Step [115400/138038], Loss: 2.9290, Perplexity: 18.70915\n",
      "Epoch [3/3], Step [115500/138038], Loss: 1.9524, Perplexity: 7.04559\n",
      "Epoch [3/3], Step [115600/138038], Loss: 3.1652, Perplexity: 23.6934\n",
      "Epoch [3/3], Step [115700/138038], Loss: 2.4001, Perplexity: 11.02398\n",
      "Epoch [3/3], Step [115800/138038], Loss: 2.5852, Perplexity: 13.2666\n",
      "Epoch [3/3], Step [115900/138038], Loss: 2.5209, Perplexity: 12.4398\n",
      "Epoch [3/3], Step [116000/138038], Loss: 3.0527, Perplexity: 21.1734\n",
      "Epoch [3/3], Step [116100/138038], Loss: 2.2685, Perplexity: 9.66510\n",
      "Epoch [3/3], Step [116200/138038], Loss: 2.6491, Perplexity: 14.1408\n",
      "Epoch [3/3], Step [116300/138038], Loss: 2.4091, Perplexity: 11.12443\n",
      "Epoch [3/3], Step [116400/138038], Loss: 1.6826, Perplexity: 5.37939\n",
      "Epoch [3/3], Step [116500/138038], Loss: 3.4950, Perplexity: 32.9518\n",
      "Epoch [3/3], Step [116600/138038], Loss: 3.0045, Perplexity: 20.1767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [116700/138038], Loss: 2.0827, Perplexity: 8.02651\n",
      "Epoch [3/3], Step [116800/138038], Loss: 2.1352, Perplexity: 8.458641\n",
      "Epoch [3/3], Step [116900/138038], Loss: 1.8594, Perplexity: 6.42017\n",
      "Epoch [3/3], Step [117000/138038], Loss: 2.4984, Perplexity: 12.16276\n",
      "Epoch [3/3], Step [117100/138038], Loss: 2.8563, Perplexity: 17.3973\n",
      "Epoch [3/3], Step [117200/138038], Loss: 2.4413, Perplexity: 11.4882\n",
      "Epoch [3/3], Step [117300/138038], Loss: 2.1635, Perplexity: 8.70120\n",
      "Epoch [3/3], Step [117400/138038], Loss: 3.5980, Perplexity: 36.5235\n",
      "Epoch [3/3], Step [117500/138038], Loss: 3.8659, Perplexity: 47.7478\n",
      "Epoch [3/3], Step [117600/138038], Loss: 2.3452, Perplexity: 10.43579\n",
      "Epoch [3/3], Step [117700/138038], Loss: 2.8199, Perplexity: 16.7744\n",
      "Epoch [3/3], Step [117800/138038], Loss: 2.2740, Perplexity: 9.71820\n",
      "Epoch [3/3], Step [117900/138038], Loss: 2.3663, Perplexity: 10.6576\n",
      "Epoch [3/3], Step [118000/138038], Loss: 2.9567, Perplexity: 19.23494\n",
      "Epoch [3/3], Step [118100/138038], Loss: 2.1370, Perplexity: 8.474366\n",
      "Epoch [3/3], Step [118200/138038], Loss: 2.4399, Perplexity: 11.4721\n",
      "Epoch [3/3], Step [118300/138038], Loss: 2.7622, Perplexity: 15.8350\n",
      "Epoch [3/3], Step [118400/138038], Loss: 2.2134, Perplexity: 9.14646\n",
      "Epoch [3/3], Step [118500/138038], Loss: 3.0753, Perplexity: 21.65673\n",
      "Epoch [3/3], Step [118600/138038], Loss: 2.6861, Perplexity: 14.6737\n",
      "Epoch [3/3], Step [118700/138038], Loss: 1.6303, Perplexity: 5.105361\n",
      "Epoch [3/3], Step [118800/138038], Loss: 2.5071, Perplexity: 12.2693\n",
      "Epoch [3/3], Step [118900/138038], Loss: 3.0179, Perplexity: 20.4489\n",
      "Epoch [3/3], Step [119000/138038], Loss: 1.8200, Perplexity: 6.17199\n",
      "Epoch [3/3], Step [119100/138038], Loss: 3.6026, Perplexity: 36.6924\n",
      "Epoch [3/3], Step [119200/138038], Loss: 2.5697, Perplexity: 13.0621\n",
      "Epoch [3/3], Step [119300/138038], Loss: 2.8710, Perplexity: 17.6549\n",
      "Epoch [3/3], Step [119400/138038], Loss: 1.8408, Perplexity: 6.30170\n",
      "Epoch [3/3], Step [119500/138038], Loss: 1.8854, Perplexity: 6.58930\n",
      "Epoch [3/3], Step [119600/138038], Loss: 3.4369, Perplexity: 31.09131\n",
      "Epoch [3/3], Step [119700/138038], Loss: 2.7969, Perplexity: 16.3933\n",
      "Epoch [3/3], Step [119800/138038], Loss: 4.5458, Perplexity: 94.2394\n",
      "Epoch [3/3], Step [119900/138038], Loss: 2.4348, Perplexity: 11.4132\n",
      "Epoch [3/3], Step [120000/138038], Loss: 2.1425, Perplexity: 8.521045\n",
      "Epoch [3/3], Step [120100/138038], Loss: 2.4339, Perplexity: 11.4031\n",
      "Epoch [3/3], Step [120200/138038], Loss: 2.8103, Perplexity: 16.6145\n",
      "Epoch [3/3], Step [120300/138038], Loss: 1.9905, Perplexity: 7.31941\n",
      "Epoch [3/3], Step [120400/138038], Loss: 2.1718, Perplexity: 8.77443\n",
      "Epoch [3/3], Step [120500/138038], Loss: 1.9652, Perplexity: 7.13618\n",
      "Epoch [3/3], Step [120600/138038], Loss: 3.3377, Perplexity: 28.1547\n",
      "Epoch [3/3], Step [120700/138038], Loss: 3.0805, Perplexity: 21.76928\n",
      "Epoch [3/3], Step [120800/138038], Loss: 2.3144, Perplexity: 10.1189\n",
      "Epoch [3/3], Step [120900/138038], Loss: 2.4814, Perplexity: 11.9574\n",
      "Epoch [3/3], Step [121000/138038], Loss: 3.3003, Perplexity: 27.1207\n",
      "Epoch [3/3], Step [121100/138038], Loss: 2.7175, Perplexity: 15.1430\n",
      "Epoch [3/3], Step [121200/138038], Loss: 1.7045, Perplexity: 5.49879\n",
      "Epoch [3/3], Step [121300/138038], Loss: 3.5151, Perplexity: 33.6177\n",
      "Epoch [3/3], Step [121400/138038], Loss: 4.2033, Perplexity: 66.9080\n",
      "Epoch [3/3], Step [121500/138038], Loss: 3.3125, Perplexity: 27.4536\n",
      "Epoch [3/3], Step [121600/138038], Loss: 2.5125, Perplexity: 12.3362\n",
      "Epoch [3/3], Step [121700/138038], Loss: 2.1074, Perplexity: 8.226599\n",
      "Epoch [3/3], Step [121800/138038], Loss: 1.5755, Perplexity: 4.833081\n",
      "Epoch [3/3], Step [121900/138038], Loss: 3.1000, Perplexity: 22.1986\n",
      "Epoch [3/3], Step [122000/138038], Loss: 2.4327, Perplexity: 11.3893\n",
      "Epoch [3/3], Step [122100/138038], Loss: 2.8214, Perplexity: 16.79993\n",
      "Epoch [3/3], Step [122200/138038], Loss: 1.8432, Perplexity: 6.31707\n",
      "Epoch [3/3], Step [122300/138038], Loss: 3.5249, Perplexity: 33.94954\n",
      "Epoch [3/3], Step [122400/138038], Loss: 2.7723, Perplexity: 15.99562\n",
      "Epoch [3/3], Step [122500/138038], Loss: 4.1920, Perplexity: 66.1559\n",
      "Epoch [3/3], Step [122600/138038], Loss: 2.6122, Perplexity: 13.62863\n",
      "Epoch [3/3], Step [122700/138038], Loss: 2.7093, Perplexity: 15.01811\n",
      "Epoch [3/3], Step [122800/138038], Loss: 2.0708, Perplexity: 7.93122\n",
      "Epoch [3/3], Step [122900/138038], Loss: 2.7185, Perplexity: 15.15746\n",
      "Epoch [3/3], Step [123000/138038], Loss: 2.8536, Perplexity: 17.3501\n",
      "Epoch [3/3], Step [123100/138038], Loss: 3.8072, Perplexity: 45.0249\n",
      "Epoch [3/3], Step [123200/138038], Loss: 2.3803, Perplexity: 10.8080\n",
      "Epoch [3/3], Step [123300/138038], Loss: 2.4450, Perplexity: 11.5308\n",
      "Epoch [3/3], Step [123400/138038], Loss: 4.3123, Perplexity: 74.61105\n",
      "Epoch [3/3], Step [123500/138038], Loss: 3.0525, Perplexity: 21.1673\n",
      "Epoch [3/3], Step [123600/138038], Loss: 2.1477, Perplexity: 8.56551\n",
      "Epoch [3/3], Step [123700/138038], Loss: 2.4993, Perplexity: 12.1738\n",
      "Epoch [3/3], Step [123800/138038], Loss: 1.7372, Perplexity: 5.681364\n",
      "Epoch [3/3], Step [123900/138038], Loss: 1.7613, Perplexity: 5.820103\n",
      "Epoch [3/3], Step [124000/138038], Loss: 2.6955, Perplexity: 14.8123\n",
      "Epoch [3/3], Step [124100/138038], Loss: 2.9874, Perplexity: 19.83389\n",
      "Epoch [3/3], Step [124200/138038], Loss: 2.0265, Perplexity: 7.587523\n",
      "Epoch [3/3], Step [124300/138038], Loss: 2.8423, Perplexity: 17.15459\n",
      "Epoch [3/3], Step [124400/138038], Loss: 2.7163, Perplexity: 15.1241\n",
      "Epoch [3/3], Step [124500/138038], Loss: 2.8016, Perplexity: 16.4708\n",
      "Epoch [3/3], Step [124600/138038], Loss: 4.1902, Perplexity: 66.03561\n",
      "Epoch [3/3], Step [124700/138038], Loss: 3.1054, Perplexity: 22.3189\n",
      "Epoch [3/3], Step [124800/138038], Loss: 2.6016, Perplexity: 13.48529\n",
      "Epoch [3/3], Step [124900/138038], Loss: 3.4884, Perplexity: 32.73507\n",
      "Epoch [3/3], Step [125000/138038], Loss: 2.8336, Perplexity: 17.00701\n",
      "Epoch [3/3], Step [125100/138038], Loss: 1.7910, Perplexity: 5.99561\n",
      "Epoch [3/3], Step [125200/138038], Loss: 1.6330, Perplexity: 5.11917\n",
      "Epoch [3/3], Step [125300/138038], Loss: 2.5793, Perplexity: 13.1877\n",
      "Epoch [3/3], Step [125400/138038], Loss: 2.6741, Perplexity: 14.49884\n",
      "Epoch [3/3], Step [125500/138038], Loss: 2.5730, Perplexity: 13.10442\n",
      "Epoch [3/3], Step [125600/138038], Loss: 3.2593, Perplexity: 26.03111\n",
      "Epoch [3/3], Step [125700/138038], Loss: 2.5360, Perplexity: 12.6289\n",
      "Epoch [3/3], Step [125800/138038], Loss: 1.9977, Perplexity: 7.37196\n",
      "Epoch [3/3], Step [125900/138038], Loss: 2.6443, Perplexity: 14.0742\n",
      "Epoch [3/3], Step [126000/138038], Loss: 1.9638, Perplexity: 7.12678\n",
      "Epoch [3/3], Step [126100/138038], Loss: 2.7747, Perplexity: 16.0331\n",
      "Epoch [3/3], Step [126200/138038], Loss: 2.4574, Perplexity: 11.6746\n",
      "Epoch [3/3], Step [126300/138038], Loss: 2.9620, Perplexity: 19.3358\n",
      "Epoch [3/3], Step [126400/138038], Loss: 3.8349, Perplexity: 46.2898\n",
      "Epoch [3/3], Step [126500/138038], Loss: 3.5452, Perplexity: 34.6466\n",
      "Epoch [3/3], Step [126600/138038], Loss: 3.3810, Perplexity: 29.3988\n",
      "Epoch [3/3], Step [126700/138038], Loss: 2.1460, Perplexity: 8.55040\n",
      "Epoch [3/3], Step [126800/138038], Loss: 2.4381, Perplexity: 11.45096\n",
      "Epoch [3/3], Step [126900/138038], Loss: 2.4595, Perplexity: 11.69851\n",
      "Epoch [3/3], Step [127000/138038], Loss: 1.4419, Perplexity: 4.22884\n",
      "Epoch [3/3], Step [127100/138038], Loss: 2.2929, Perplexity: 9.903382\n",
      "Epoch [3/3], Step [127200/138038], Loss: 2.4560, Perplexity: 11.65800\n",
      "Epoch [3/3], Step [127300/138038], Loss: 1.6356, Perplexity: 5.13233\n",
      "Epoch [3/3], Step [127400/138038], Loss: 2.0966, Perplexity: 8.13864\n",
      "Epoch [3/3], Step [127500/138038], Loss: 2.7536, Perplexity: 15.6995\n",
      "Epoch [3/3], Step [127600/138038], Loss: 3.2550, Perplexity: 25.92014\n",
      "Epoch [3/3], Step [127700/138038], Loss: 2.4247, Perplexity: 11.2988\n",
      "Epoch [3/3], Step [127800/138038], Loss: 2.8098, Perplexity: 16.6059\n",
      "Epoch [3/3], Step [127900/138038], Loss: 2.5230, Perplexity: 12.46604\n",
      "Epoch [3/3], Step [128000/138038], Loss: 2.0261, Perplexity: 7.58445\n",
      "Epoch [3/3], Step [128100/138038], Loss: 1.3641, Perplexity: 3.91234\n",
      "Epoch [3/3], Step [128200/138038], Loss: 3.3377, Perplexity: 28.1548\n",
      "Epoch [3/3], Step [128300/138038], Loss: 1.9073, Perplexity: 6.73478\n",
      "Epoch [3/3], Step [128400/138038], Loss: 2.3053, Perplexity: 10.0275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [128500/138038], Loss: 1.9585, Perplexity: 7.08834\n",
      "Epoch [3/3], Step [128600/138038], Loss: 3.5326, Perplexity: 34.21129\n",
      "Epoch [3/3], Step [128700/138038], Loss: 1.7996, Perplexity: 6.04749\n",
      "Epoch [3/3], Step [128800/138038], Loss: 3.5744, Perplexity: 35.6750\n",
      "Epoch [3/3], Step [128900/138038], Loss: 3.6484, Perplexity: 38.4116\n",
      "Epoch [3/3], Step [129000/138038], Loss: 2.7261, Perplexity: 15.2739\n",
      "Epoch [3/3], Step [129100/138038], Loss: 2.2293, Perplexity: 9.29373\n",
      "Epoch [3/3], Step [129200/138038], Loss: 2.1922, Perplexity: 8.95498\n",
      "Epoch [3/3], Step [129300/138038], Loss: 2.3897, Perplexity: 10.9103\n",
      "Epoch [3/3], Step [129400/138038], Loss: 1.9724, Perplexity: 7.18808\n",
      "Epoch [3/3], Step [129500/138038], Loss: 1.9659, Perplexity: 7.14153\n",
      "Epoch [3/3], Step [129600/138038], Loss: 3.2038, Perplexity: 24.6251\n",
      "Epoch [3/3], Step [129700/138038], Loss: 1.3629, Perplexity: 3.90775\n",
      "Epoch [3/3], Step [129800/138038], Loss: 2.5524, Perplexity: 12.8382\n",
      "Epoch [3/3], Step [129900/138038], Loss: 3.5112, Perplexity: 33.48810\n",
      "Epoch [3/3], Step [130000/138038], Loss: 1.6552, Perplexity: 5.234274\n",
      "Epoch [3/3], Step [130100/138038], Loss: 3.3747, Perplexity: 29.2154\n",
      "Epoch [3/3], Step [130200/138038], Loss: 3.5124, Perplexity: 33.5292\n",
      "Epoch [3/3], Step [130300/138038], Loss: 1.7707, Perplexity: 5.87481\n",
      "Epoch [3/3], Step [130400/138038], Loss: 2.8130, Perplexity: 16.6606\n",
      "Epoch [3/3], Step [130500/138038], Loss: 2.3480, Perplexity: 10.46463\n",
      "Epoch [3/3], Step [130600/138038], Loss: 1.9598, Perplexity: 7.09824\n",
      "Epoch [3/3], Step [130700/138038], Loss: 3.0578, Perplexity: 21.2797\n",
      "Epoch [3/3], Step [130800/138038], Loss: 2.6687, Perplexity: 14.42091\n",
      "Epoch [3/3], Step [130900/138038], Loss: 2.1990, Perplexity: 9.015797\n",
      "Epoch [3/3], Step [131000/138038], Loss: 1.8953, Perplexity: 6.654734\n",
      "Epoch [3/3], Step [131100/138038], Loss: 3.2661, Perplexity: 26.20865\n",
      "Epoch [3/3], Step [131200/138038], Loss: 2.5697, Perplexity: 13.0621\n",
      "Epoch [3/3], Step [131300/138038], Loss: 2.1887, Perplexity: 8.923640\n",
      "Epoch [3/3], Step [131400/138038], Loss: 1.7686, Perplexity: 5.862772\n",
      "Epoch [3/3], Step [131500/138038], Loss: 3.1211, Perplexity: 22.6711\n",
      "Epoch [3/3], Step [131600/138038], Loss: 2.7626, Perplexity: 15.8405\n",
      "Epoch [3/3], Step [131700/138038], Loss: 1.8945, Perplexity: 6.64893\n",
      "Epoch [3/3], Step [131800/138038], Loss: 2.4196, Perplexity: 11.2411\n",
      "Epoch [3/3], Step [131900/138038], Loss: 2.6183, Perplexity: 13.71201\n",
      "Epoch [3/3], Step [132000/138038], Loss: 2.2784, Perplexity: 9.76149\n",
      "Epoch [3/3], Step [132100/138038], Loss: 2.8568, Perplexity: 17.4051\n",
      "Epoch [3/3], Step [132200/138038], Loss: 2.7971, Perplexity: 16.39658\n",
      "Epoch [3/3], Step [132300/138038], Loss: 3.0823, Perplexity: 21.80947\n",
      "Epoch [3/3], Step [132400/138038], Loss: 3.1096, Perplexity: 22.4132\n",
      "Epoch [3/3], Step [132500/138038], Loss: 2.3967, Perplexity: 10.9872\n",
      "Epoch [3/3], Step [132600/138038], Loss: 1.7640, Perplexity: 5.835611\n",
      "Epoch [3/3], Step [132700/138038], Loss: 2.1839, Perplexity: 8.88059\n",
      "Epoch [3/3], Step [132800/138038], Loss: 2.8502, Perplexity: 17.29112\n",
      "Epoch [3/3], Step [132900/138038], Loss: 2.3632, Perplexity: 10.6254\n",
      "Epoch [3/3], Step [133000/138038], Loss: 2.9190, Perplexity: 18.52261\n",
      "Epoch [3/3], Step [133100/138038], Loss: 3.5501, Perplexity: 34.8161\n",
      "Epoch [3/3], Step [133200/138038], Loss: 2.7509, Perplexity: 15.65690\n",
      "Epoch [3/3], Step [133300/138038], Loss: 1.8979, Perplexity: 6.67221\n",
      "Epoch [3/3], Step [133400/138038], Loss: 2.3898, Perplexity: 10.9114\n",
      "Epoch [3/3], Step [133500/138038], Loss: 2.8903, Perplexity: 17.99899\n",
      "Epoch [3/3], Step [133600/138038], Loss: 3.0288, Perplexity: 20.67195\n",
      "Epoch [3/3], Step [133700/138038], Loss: 3.4150, Perplexity: 30.41680\n",
      "Epoch [3/3], Step [133800/138038], Loss: 2.5656, Perplexity: 13.00910\n",
      "Epoch [3/3], Step [133900/138038], Loss: 2.7601, Perplexity: 15.8020\n",
      "Epoch [3/3], Step [134000/138038], Loss: 3.6019, Perplexity: 36.6671\n",
      "Epoch [3/3], Step [134100/138038], Loss: 3.2325, Perplexity: 25.34400\n",
      "Epoch [3/3], Step [134200/138038], Loss: 2.5556, Perplexity: 12.87845\n",
      "Epoch [3/3], Step [134300/138038], Loss: 2.4579, Perplexity: 11.6801\n",
      "Epoch [3/3], Step [134400/138038], Loss: 2.3361, Perplexity: 10.34074\n",
      "Epoch [3/3], Step [134500/138038], Loss: 2.4887, Perplexity: 12.0453\n",
      "Epoch [3/3], Step [134600/138038], Loss: 2.4928, Perplexity: 12.09482\n",
      "Epoch [3/3], Step [134700/138038], Loss: 3.6798, Perplexity: 39.6380\n",
      "Epoch [3/3], Step [134800/138038], Loss: 2.4490, Perplexity: 11.57680\n",
      "Epoch [3/3], Step [134900/138038], Loss: 2.8219, Perplexity: 16.8095\n",
      "Epoch [3/3], Step [135000/138038], Loss: 2.9570, Perplexity: 19.2407\n",
      "Epoch [3/3], Step [135100/138038], Loss: 3.2731, Perplexity: 26.39431\n",
      "Epoch [3/3], Step [135200/138038], Loss: 2.3607, Perplexity: 10.59834\n",
      "Epoch [3/3], Step [135300/138038], Loss: 1.7462, Perplexity: 5.732859\n",
      "Epoch [3/3], Step [135400/138038], Loss: 2.4773, Perplexity: 11.9090\n",
      "Epoch [3/3], Step [135500/138038], Loss: 3.1057, Perplexity: 22.3241\n",
      "Epoch [3/3], Step [135600/138038], Loss: 3.3090, Perplexity: 27.35878\n",
      "Epoch [3/3], Step [135700/138038], Loss: 2.3114, Perplexity: 10.0885\n",
      "Epoch [3/3], Step [135800/138038], Loss: 2.9957, Perplexity: 19.9993\n",
      "Epoch [3/3], Step [135900/138038], Loss: 2.5676, Perplexity: 13.03504\n",
      "Epoch [3/3], Step [136000/138038], Loss: 2.3996, Perplexity: 11.01882\n",
      "Epoch [3/3], Step [136100/138038], Loss: 4.0012, Perplexity: 54.6660\n",
      "Epoch [3/3], Step [136200/138038], Loss: 2.4311, Perplexity: 11.3719\n",
      "Epoch [3/3], Step [136300/138038], Loss: 2.7125, Perplexity: 15.06761\n",
      "Epoch [3/3], Step [136400/138038], Loss: 2.3525, Perplexity: 10.5117\n",
      "Epoch [3/3], Step [136500/138038], Loss: 1.9767, Perplexity: 7.21861\n",
      "Epoch [3/3], Step [136600/138038], Loss: 2.3462, Perplexity: 10.44593\n",
      "Epoch [3/3], Step [136700/138038], Loss: 3.5323, Perplexity: 34.20334\n",
      "Epoch [3/3], Step [136800/138038], Loss: 2.4995, Perplexity: 12.1763\n",
      "Epoch [3/3], Step [136900/138038], Loss: 2.8383, Perplexity: 17.0868\n",
      "Epoch [3/3], Step [137000/138038], Loss: 2.5149, Perplexity: 12.36517\n",
      "Epoch [3/3], Step [137100/138038], Loss: 2.9177, Perplexity: 18.4989\n",
      "Epoch [3/3], Step [137200/138038], Loss: 2.5707, Perplexity: 13.07551\n",
      "Epoch [3/3], Step [137300/138038], Loss: 3.0373, Perplexity: 20.8490\n",
      "Epoch [3/3], Step [137400/138038], Loss: 4.1577, Perplexity: 63.9220\n",
      "Epoch [3/3], Step [137500/138038], Loss: 2.0478, Perplexity: 7.75069\n",
      "Epoch [3/3], Step [137600/138038], Loss: 2.4860, Perplexity: 12.0128\n",
      "Epoch [3/3], Step [137700/138038], Loss: 2.0948, Perplexity: 8.12399\n",
      "Epoch [3/3], Step [137800/138038], Loss: 4.0361, Perplexity: 56.6075\n",
      "Epoch [3/3], Step [137900/138038], Loss: 2.5346, Perplexity: 12.6110\n",
      "Epoch [3/3], Step [138000/138038], Loss: 2.8263, Perplexity: 16.88243\n",
      "Epoch [3/3], Step [138038/138038], Loss: 1.8229, Perplexity: 6.18976"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions[:, :-1])\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
